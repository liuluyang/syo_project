2018-09-04 15:37:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-04 15:37:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-04 15:37:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-04 15:37:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-04 15:37:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-04 15:37:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-04 15:37:30 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-04 15:37:30 [scrapy.core.engine] INFO: Spider opened
2018-09-04 15:37:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-04 15:37:30 [btc_policy] INFO: Spider opened: btc_policy
2018-09-04 15:37:30 [btc_policy] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-04 15:37:30 [btc_policy] WARNING: mysql服务连接失败
2018-09-04 15:37:35 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-04 15:37:35 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'PolicySpider' object has no attribute 'connect'
2018-09-04 15:37:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7476,
 'downloader/request_count': 16,
 'downloader/request_method_count/GET': 16,
 'downloader/response_bytes': 646564,
 'downloader/response_count': 16,
 'downloader/response_status_count/200': 16,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 4, 7, 37, 35, 148059),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 16,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'start_time': datetime.datetime(2018, 9, 4, 7, 37, 30, 684843)}
2018-09-04 15:37:35 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-05 08:57:24 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-05 08:57:24 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-05 08:57:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-05 08:57:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-05 08:57:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-05 08:57:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-05 08:57:25 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-05 08:57:25 [scrapy.core.engine] INFO: Spider opened
2018-09-05 08:57:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-05 08:57:25 [btc_policy] INFO: Spider opened: btc_policy
2018-09-05 08:57:25 [btc_policy] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-05 08:57:25 [btc_policy] WARNING: mysql服务连接失败
2018-09-05 08:57:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.8btc.com/news?cat_id=572> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\news\btc_policy.py", line 51, in parse
    if self.redis.sismember('news_urls', origin_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-09-05 08:57:30 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-05 08:57:30 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'PolicySpider' object has no attribute 'connect'
2018-09-05 08:57:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 291,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42733,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 5, 0, 57, 30, 683207),
 'log_count/ERROR': 2,
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ConnectionError': 1,
 'start_time': datetime.datetime(2018, 9, 5, 0, 57, 25, 886932)}
2018-09-05 08:57:30 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-05 08:57:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-05 08:57:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-05 08:57:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-05 08:57:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-05 08:57:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-05 08:57:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-05 08:57:42 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-05 08:57:42 [scrapy.core.engine] INFO: Spider opened
2018-09-05 08:57:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-05 08:57:42 [btc_policy] INFO: Spider opened: btc_policy
2018-09-05 08:57:42 [btc_policy] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-05 08:57:42 [btc_policy] WARNING: mysql服务连接失败
2018-09-05 08:57:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.8btc.com/news?cat_id=572> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\news\btc_policy.py", line 51, in parse
    if self.redis.sismember('news_urls', origin_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-09-05 08:57:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-05 08:57:47 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'PolicySpider' object has no attribute 'connect'
2018-09-05 08:57:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 216,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42727,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 5, 0, 57, 47, 173150),
 'log_count/ERROR': 2,
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ConnectionError': 1,
 'start_time': datetime.datetime(2018, 9, 5, 0, 57, 42, 674893)}
2018-09-05 08:57:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-05 09:02:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-05 09:02:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-05 09:02:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-05 09:02:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-05 09:02:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-05 09:02:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-05 09:02:11 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-05 09:02:11 [scrapy.core.engine] INFO: Spider opened
2018-09-05 09:02:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-05 09:02:11 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-09-05 09:02:11 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-05 09:02:11 [bsj_kuaixun] WARNING: mysql服务连接失败
2018-09-05 09:02:16 [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.bishijie.com/kuaixun/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\newsletter\bsj_kuaixun.py", line 68, in parse
    if self.redis.sismember('newsletter_urls', origin_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-09-05 09:02:16 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-05 09:02:16 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'KuaixunSpider' object has no attribute 'connect'
2018-09-05 09:02:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 304,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 41531,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 5, 1, 2, 16, 408148),
 'log_count/ERROR': 2,
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ConnectionError': 1,
 'start_time': datetime.datetime(2018, 9, 5, 1, 2, 11, 988896)}
2018-09-05 09:02:16 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-05 09:05:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-05 09:05:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-05 09:05:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-05 09:05:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-05 09:05:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-05 09:05:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-05 09:05:27 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-05 09:05:27 [scrapy.core.engine] INFO: Spider opened
2018-09-05 09:05:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-05 09:05:27 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-09-05 09:05:27 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-05 09:05:27 [bsj_kuaixun] WARNING: mysql服务连接失败
2018-09-05 09:05:30 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-05 09:05:30 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'KuaixunSpider' object has no attribute 'connect'
2018-09-05 09:05:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 34270,
 'downloader/request_count': 79,
 'downloader/request_method_count/GET': 79,
 'downloader/response_bytes': 968255,
 'downloader/response_count': 79,
 'downloader/response_status_count/200': 79,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 5, 1, 5, 30, 818069),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 79,
 'scheduler/dequeued': 79,
 'scheduler/dequeued/memory': 79,
 'scheduler/enqueued': 79,
 'scheduler/enqueued/memory': 79,
 'start_time': datetime.datetime(2018, 9, 5, 1, 5, 27, 674889)}
2018-09-05 09:05:30 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-05 09:07:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-05 09:07:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-05 09:07:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-05 09:07:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-05 09:07:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-05 09:07:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-05 09:07:19 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-05 09:07:19 [scrapy.core.engine] INFO: Spider opened
2018-09-05 09:07:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-05 09:07:19 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-09-05 09:07:19 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-05 09:07:19 [bsj_kuaixun] WARNING: mysql服务连接失败
2018-09-05 09:07:19 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-05 09:07:19 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'KuaixunSpider' object has no attribute 'connect'
2018-09-05 09:07:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 294,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 5945,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 5, 1, 7, 19, 724298),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 5, 1, 7, 19, 449282)}
2018-09-05 09:07:19 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-05 09:07:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-05 09:07:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-05 09:07:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-05 09:07:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-05 09:07:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-05 09:07:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-05 09:07:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-05 09:07:29 [scrapy.core.engine] INFO: Spider opened
2018-09-05 09:07:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-05 09:07:29 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-09-05 09:07:29 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-05 09:07:29 [bsj_kuaixun] WARNING: mysql服务连接失败
2018-09-05 09:07:29 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-05 09:07:29 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'KuaixunSpider' object has no attribute 'connect'
2018-09-05 09:07:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 240,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15384,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 5, 1, 7, 29, 426853),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 5, 1, 7, 29, 183839)}
2018-09-05 09:07:29 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-05 09:08:24 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-05 09:08:24 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-05 09:08:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-05 09:08:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-05 09:08:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-05 09:08:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-05 09:08:25 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-05 09:08:25 [scrapy.core.engine] INFO: Spider opened
2018-09-05 09:08:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-05 09:08:25 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-09-05 09:08:25 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-05 09:08:25 [bsj_kuaixun] WARNING: mysql服务连接失败
2018-09-05 09:08:26 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-05 09:08:26 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'KuaixunSpider' object has no attribute 'connect'
2018-09-05 09:08:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 252,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 5938,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 5, 1, 8, 26, 129096),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 5, 1, 8, 25, 745074)}
2018-09-05 09:08:26 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 16:46:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 16:46:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 16:46:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 16:46:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 16:46:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 16:46:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 16:46:05 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 16:46:05 [scrapy.core.engine] INFO: Spider opened
2018-09-28 16:46:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 16:46:05 [token_address] INFO: Spider opened: token_address
2018-09-28 16:46:10 [token_address] WARNING: Redis服务连接失败
2018-09-28 16:46:10 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A6780>
2018-09-28 16:46:13 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 16:46:13 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 16:46:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 357,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46926,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 8, 46, 13, 305163),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 8, 46, 5, 994742)}
2018-09-28 16:46:13 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 16:46:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 16:46:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 16:46:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 16:46:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 16:46:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 16:46:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 16:46:27 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 16:46:27 [scrapy.core.engine] INFO: Spider opened
2018-09-28 16:46:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 16:46:27 [token_address] INFO: Spider opened: token_address
2018-09-28 16:46:31 [token_address] WARNING: Redis服务连接失败
2018-09-28 16:46:31 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B5748>
2018-09-28 16:46:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 16:46:34 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 16:46:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 305,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46917,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 8, 46, 34, 882233),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 8, 46, 27, 495002)}
2018-09-28 16:46:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 16:47:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 16:47:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 16:47:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 16:47:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 16:47:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 16:47:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 16:47:36 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 16:47:36 [scrapy.core.engine] INFO: Spider opened
2018-09-28 16:47:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 16:47:36 [token_address] INFO: Spider opened: token_address
2018-09-28 16:47:37 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 16:47:37 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A6860>
2018-09-28 16:47:39 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 16:47:39 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 16:47:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 357,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46914,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 8, 47, 39, 663004),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 8, 47, 36, 174594)}
2018-09-28 16:47:39 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 16:50:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 16:50:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 16:50:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 16:50:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 16:50:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 16:50:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 16:50:26 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 16:50:26 [scrapy.core.engine] INFO: Spider opened
2018-09-28 16:50:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 16:50:26 [token_address] INFO: Spider opened: token_address
2018-09-28 16:50:27 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 16:50:27 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A6860>
2018-09-28 16:50:30 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 16:50:30 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 16:50:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 348,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46932,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 8, 50, 30, 669061),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 8, 50, 26, 634348)}
2018-09-28 16:50:30 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 16:53:07 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 16:53:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 16:53:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 16:53:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 16:53:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 16:53:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 16:53:08 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 16:53:08 [scrapy.core.engine] INFO: Spider opened
2018-09-28 16:53:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 16:53:08 [token_address] INFO: Spider opened: token_address
2018-09-28 16:53:09 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 16:53:09 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A5898>
2018-09-28 16:53:12 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 16:53:12 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 16:53:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 357,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46904,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 8, 53, 12, 392335),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 8, 53, 8, 517528)}
2018-09-28 16:53:12 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 16:57:15 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 16:57:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 16:57:15 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 16:57:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 16:57:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 16:57:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 16:57:16 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 16:57:16 [scrapy.core.engine] INFO: Spider opened
2018-09-28 16:57:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 16:57:16 [token_address] INFO: Spider opened: token_address
2018-09-28 16:57:17 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 16:57:17 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B5898>
2018-09-28 16:57:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/tokenholderchart/0xB8c77482e45F1F44dE1745F52C74426C631bDD52> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\token_address.py", line 30, in parse
    panking = tds[0].css('::text').extract_first()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-09-28 16:57:19 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 16:57:19 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 16:57:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 324,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46962,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 8, 57, 19, 848362),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2018, 9, 28, 8, 57, 16, 165752)}
2018-09-28 16:57:19 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 16:58:54 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 16:58:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 16:58:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 16:58:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 16:58:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 16:58:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 16:58:55 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 16:58:55 [scrapy.core.engine] INFO: Spider opened
2018-09-28 16:58:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 16:58:55 [token_address] INFO: Spider opened: token_address
2018-09-28 16:58:56 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 16:58:56 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A58D0>
2018-09-28 16:58:59 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 16:58:59 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 16:58:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 347,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46985,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 8, 58, 59, 657661),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 8, 58, 55, 846051)}
2018-09-28 16:58:59 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:04:37 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:04:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:04:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:04:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:04:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:04:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:04:38 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:04:38 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:04:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:04:38 [token_address] INFO: Spider opened: token_address
2018-09-28 17:04:39 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:04:39 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A8828>
2018-09-28 17:04:42 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:04:42 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:04:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 359,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46925,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 4, 42, 367762),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 4, 38, 375750)}
2018-09-28 17:04:42 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:07:06 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:07:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:07:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:07:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:07:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:07:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:07:07 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:07:07 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:07:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:07:07 [token_address] INFO: Spider opened: token_address
2018-09-28 17:07:08 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:07:08 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B48D0>
2018-09-28 17:07:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:07:11 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:07:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 359,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46937,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 7, 11, 870303),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 7, 7, 860695)}
2018-09-28 17:07:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:09:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:09:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:09:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:09:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:09:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:09:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:09:27 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:09:27 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:09:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:09:27 [token_address] INFO: Spider opened: token_address
2018-09-28 17:09:28 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:09:28 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B58D0>
2018-09-28 17:09:31 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:09:31 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:09:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 301,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46795,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 9, 31, 460485),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 9, 27, 182073)}
2018-09-28 17:09:31 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:10:30 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:10:30 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:10:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:10:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:10:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:10:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:10:31 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:10:31 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:10:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:10:31 [token_address] INFO: Spider opened: token_address
2018-09-28 17:10:32 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:10:32 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A5898>
2018-09-28 17:10:35 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:10:35 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:10:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 357,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46591,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 10, 35, 209666),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 10, 31, 42653)}
2018-09-28 17:10:35 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:11:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:11:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:11:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:11:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:11:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:11:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:11:35 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:11:35 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:11:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:11:35 [token_address] INFO: Spider opened: token_address
2018-09-28 17:11:36 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:11:36 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A8828>
2018-09-28 17:11:39 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:11:39 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:11:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 301,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46802,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 11, 39, 757898),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 11, 35, 942689)}
2018-09-28 17:11:39 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:14:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:14:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:14:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:14:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:14:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:14:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:14:05 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:14:05 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:14:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:14:05 [token_address] INFO: Spider opened: token_address
2018-09-28 17:14:06 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:14:06 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A48D0>
2018-09-28 17:14:09 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:14:09 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:14:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 316,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46775,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 14, 9, 409981),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 14, 5, 197370)}
2018-09-28 17:14:09 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:16:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:16:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:16:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:16:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:16:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:16:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:16:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:16:29 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:16:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:16:29 [token_address] INFO: Spider opened: token_address
2018-09-28 17:16:30 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:16:30 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A48D0>
2018-09-28 17:16:33 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:16:33 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:16:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 355,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46062,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 16, 33, 211241),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 16, 29, 450833)}
2018-09-28 17:16:33 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:17:20 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:17:20 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:17:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:17:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:17:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:17:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:17:21 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:17:21 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:17:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:17:21 [token_address] INFO: Spider opened: token_address
2018-09-28 17:17:22 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:17:22 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A58D0>
2018-09-28 17:17:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:17:25 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:17:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 312,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46843,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 17, 25, 771595),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 17, 21, 921186)}
2018-09-28 17:17:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:37:12 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:37:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:37:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:37:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:37:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:37:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:37:13 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:37:13 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:37:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:37:13 [token_address] INFO: Spider opened: token_address
2018-09-28 17:37:15 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:37:15 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A5898>
2018-09-28 17:37:18 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:37:18 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:37:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 316,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46621,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 37, 18, 115511),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 37, 13, 958101)}
2018-09-28 17:37:18 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:39:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:39:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:39:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:39:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:39:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:39:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:39:51 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:39:51 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:39:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:39:51 [token_address] INFO: Spider opened: token_address
2018-09-28 17:39:52 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:39:52 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A4908>
2018-09-28 17:39:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/tokenholderchart/0xB8c77482e45F1F44dE1745F52C74426C631bDD52> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\token_address.py", line 56, in parse
    percent_100 = super(percent_list)
TypeError: super() argument 1 must be type, not list
2018-09-28 17:39:55 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:39:55 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:39:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 340,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46655,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 39, 55, 726006),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 39, 51, 914995)}
2018-09-28 17:39:55 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:40:21 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:40:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:40:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:40:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:40:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:40:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:40:22 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:40:22 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:40:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:40:22 [token_address] INFO: Spider opened: token_address
2018-09-28 17:40:23 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:40:23 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A48D0>
2018-09-28 17:40:26 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:40:26 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:40:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 304,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46657,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 40, 26, 717889),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 40, 22, 492277)}
2018-09-28 17:40:26 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:45:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:45:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:45:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:45:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:45:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:45:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:45:05 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:45:05 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:45:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:45:05 [token_address] INFO: Spider opened: token_address
2018-09-28 17:45:06 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:45:06 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B4A20>
2018-09-28 17:45:09 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:45:09 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:45:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 348,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46642,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 45, 9, 372715),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 45, 5, 673706)}
2018-09-28 17:45:09 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 17:47:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 17:47:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 17:47:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 17:47:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 17:47:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 17:47:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 17:47:26 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 17:47:26 [scrapy.core.engine] INFO: Spider opened
2018-09-28 17:47:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 17:47:26 [token_address] INFO: Spider opened: token_address
2018-09-28 17:47:27 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 17:47:27 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A4978>
2018-09-28 17:47:30 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 17:47:30 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 17:47:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 360,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46663,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 9, 47, 30, 342735),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 9, 47, 26, 290926)}
2018-09-28 17:47:30 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-28 18:00:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-28 18:00:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-28 18:00:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-28 18:00:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-28 18:00:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-28 18:00:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-28 18:00:27 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-28 18:00:27 [scrapy.core.engine] INFO: Spider opened
2018-09-28 18:00:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-28 18:00:27 [token_address] INFO: Spider opened: token_address
2018-09-28 18:00:28 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-28 18:00:28 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B2C18>
2018-09-28 18:00:32 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-28 18:00:32 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-28 18:00:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 316,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46843,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 28, 10, 0, 32, 224710),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 28, 10, 0, 27, 617900)}
2018-09-28 18:00:32 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 09:51:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 09:51:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 09:51:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 09:51:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 09:51:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 09:51:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 09:51:56 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 09:51:56 [scrapy.core.engine] INFO: Spider opened
2018-09-29 09:51:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 09:51:56 [token_address] INFO: Spider opened: token_address
2018-09-29 09:52:00 [token_address] WARNING: Redis服务连接失败
2018-09-29 09:52:00 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A4940>
2018-09-29 09:52:15 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 09:52:15 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 09:52:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 334,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 46549,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 1, 52, 15, 245141),
 'item_scraped_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 1, 51, 56, 80000)}
2018-09-29 09:52:15 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 09:54:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 09:54:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 09:54:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 09:54:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 09:54:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 09:54:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 09:54:30 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 09:54:30 [scrapy.core.engine] INFO: Spider opened
2018-09-29 09:54:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 09:54:30 [token_address] INFO: Spider opened: token_address
2018-09-29 09:54:34 [token_address] WARNING: Redis服务连接失败
2018-09-29 09:54:34 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058849B0>
2018-09-29 09:55:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 09:56:32 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xd850942ef8811f2a866692a623011bde52a462c1>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xd850942ef8811f2a866692a623011bde52a462c1 took longer than 20.0 seconds..
2018-09-29 09:56:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 09:56:43 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xd26114cd6EE289AccF82350c8d8487fedB8A0C07>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xd26114cd6EE289AccF82350c8d8487fedB8A0C07 took longer than 20.0 seconds..
2018-09-29 09:56:55 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xe41d2489571d322189246dafa5ebde1f4699f498>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xe41d2489571d322189246dafa5ebde1f4699f498 took longer than 20.0 seconds..
2018-09-29 09:57:07 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x9f8f72aa9304c8b593d555f12ef6589cc3a579a2>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x9f8f72aa9304c8b593d555f12ef6589cc3a579a2 took longer than 20.0 seconds..
2018-09-29 09:57:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x05f4a42e251f2d52b8ed15e9fedaacfcef1fad27>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x05f4a42e251f2d52b8ed15e9fedaacfcef1fad27 took longer than 20.0 seconds..
2018-09-29 09:57:29 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xb5a5f22694352c15b00323844ad545abb2b11028>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xb5a5f22694352c15b00323844ad545abb2b11028 took longer than 20.0 seconds..
2018-09-29 09:57:41 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x5ca9a71b1d01849c0a95490cc00559717fcf0d1d>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x5ca9a71b1d01849c0a95490cc00559717fcf0d1d took longer than 20.0 seconds..
2018-09-29 09:57:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 09:57:53 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xcb97e65f07da24d46bcdd078ebebd7c6e6e3d750>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xcb97e65f07da24d46bcdd078ebebd7c6e6e3d750 took longer than 20.0 seconds..
2018-09-29 09:58:04 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x0d8775f648430679a709e98d2b0cb6250d2887ef>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x0d8775f648430679a709e98d2b0cb6250d2887ef took longer than 20.0 seconds..
2018-09-29 09:58:16 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xa15c7ebe1f07caf6bff097d8a589fb8ac49ae5b3>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xa15c7ebe1f07caf6bff097d8a589fb8ac49ae5b3 took longer than 20.0 seconds..
2018-09-29 09:58:28 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xa74476443119A942dE498590Fe1f2454d7D4aC0d>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xa74476443119A942dE498590Fe1f2454d7D4aC0d took longer than 20.0 seconds..
2018-09-29 09:58:39 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x1985365e9f78359a9B6AD760e32412f4a445E862>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x1985365e9f78359a9B6AD760e32412f4a445E862 took longer than 20.0 seconds..
2018-09-29 09:58:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 09:58:50 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x6c6ee5e31d828de241282b9606c8e98ea48526e2>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x6c6ee5e31d828de241282b9606c8e98ea48526e2 took longer than 20.0 seconds..
2018-09-29 09:59:01 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x744d70fdbe2ba4cf95131626614a1763df805b9e>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x744d70fdbe2ba4cf95131626614a1763df805b9e took longer than 20.0 seconds..
2018-09-29 09:59:13 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x514910771af9ca656af840dff83e8264ecf986ca>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x514910771af9ca656af840dff83e8264ecf986ca took longer than 20.0 seconds..
2018-09-29 09:59:25 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xb7cb1c96db6b22b0d3d9536e0108d062bd488f74>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xb7cb1c96db6b22b0d3d9536e0108d062bd488f74 took longer than 20.0 seconds..
2018-09-29 09:59:36 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xd4fa1460f537bb9085d22c7bccb5dd450ef28e3a>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xd4fa1460f537bb9085d22c7bccb5dd450ef28e3a took longer than 20.0 seconds..
2018-09-29 09:59:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 09:59:37 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xfa1a856cfa3409cfa145fa4e20eb270df3eb21ab>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xfa1a856cfa3409cfa145fa4e20eb270df3eb21ab took longer than 20.0 seconds..
2018-09-29 09:59:59 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x168296bb09e24a88805cb9c33356536b980d3fc5>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x168296bb09e24a88805cb9c33356536b980d3fc5 took longer than 20.0 seconds..
2018-09-29 10:00:00 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x4CEdA7906a5Ed2179785Cd3A40A69ee8bc99C466>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x4CEdA7906a5Ed2179785Cd3A40A69ee8bc99C466 took longer than 20.0 seconds..
2018-09-29 10:00:21 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x8dd5fbce2f6a956c3022ba3663759011dd51e73e>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x8dd5fbce2f6a956c3022ba3663759011dd51e73e took longer than 20.0 seconds..
2018-09-29 10:00:22 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x08d32b0da63e2C3bcF8019c9c5d849d7a9d791e6>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x08d32b0da63e2C3bcF8019c9c5d849d7a9d791e6 took longer than 20.0 seconds..
2018-09-29 10:00:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 10:00:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x6f259637dcd74c767781e37bc6133cd6a68aa161>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x6f259637dcd74c767781e37bc6133cd6a68aa161 took longer than 20.0 seconds..
2018-09-29 10:00:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x1f573d6fb3f13d689ff844b4ce37794d79a7ff1c>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x1f573d6fb3f13d689ff844b4ce37794d79a7ff1c took longer than 20.0 seconds..
2018-09-29 10:01:07 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x9ab165d795019b6d8b3e971dda91071421305e5a>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x9ab165d795019b6d8b3e971dda91071421305e5a took longer than 20.0 seconds..
2018-09-29 10:01:07 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x419d0d8bdd9af5e606ae2232ed285aff190e711b>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x419d0d8bdd9af5e606ae2232ed285aff190e711b took longer than 20.0 seconds..
2018-09-29 10:01:07 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xbf2179859fc6d5bee9bf9158632dc51678a4100e>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xbf2179859fc6d5bee9bf9158632dc51678a4100e took longer than 20.0 seconds..
2018-09-29 10:01:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 10:01:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xe0b7927c4af23765cb51314a0e0521a9645f0e2a>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xe0b7927c4af23765cb51314a0e0521a9645f0e2a took longer than 20.0 seconds..
2018-09-29 10:01:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x618e75ac90b12c6049ba3b27f5d5f8651b0037f6>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x618e75ac90b12c6049ba3b27f5d5f8651b0037f6 took longer than 20.0 seconds..
2018-09-29 10:01:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x0f5d2fb29fb7d3cfee444a200298f468908cc942>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x0f5d2fb29fb7d3cfee444a200298f468908cc942 took longer than 20.0 seconds..
2018-09-29 10:02:05 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x3883f5e181fccaf8410fa61e12b59bad963fb645>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x3883f5e181fccaf8410fa61e12b59bad963fb645 took longer than 20.0 seconds..
2018-09-29 10:02:27 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xB97048628DB6B661D4C2aA833e95Dbe1A905B280>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xB97048628DB6B661D4C2aA833e95Dbe1A905B280 took longer than 20.0 seconds..
2018-09-29 10:02:27 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0xb63b606ac810a52cca15e44bb630fd42d8d1d83d>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0xb63b606ac810a52cca15e44bb630fd42d8d1d83d took longer than 20.0 seconds..
2018-09-29 10:02:27 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x595832f8fc6bf59c85c527fec3740a1b7a361269>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x595832f8fc6bf59c85c527fec3740a1b7a361269 took longer than 20.0 seconds..
2018-09-29 10:02:27 [scrapy.core.scraper] ERROR: Error downloading <GET https://etherscan.io/token/tokenholderchart/0x39bb259f66e1c59d5abef88375979b4d20d98022>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\core\downloader\handlers\http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://etherscan.io/token/tokenholderchart/0x39bb259f66e1c59d5abef88375979b4d20d98022 took longer than 20.0 seconds..
2018-09-29 10:02:42 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 18 pages/min), scraped 13 items (at 13 items/min)
2018-09-29 10:02:43 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 10:02:43 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 10:02:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 121,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 121,
 'downloader/request_bytes': 47365,
 'downloader/request_count': 140,
 'downloader/request_method_count/GET': 140,
 'downloader/response_bytes': 712113,
 'downloader/response_count': 19,
 'downloader/response_status_count/200': 15,
 'downloader/response_status_count/403': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 2, 2, 43, 3976),
 'item_scraped_count': 15,
 'log_count/ERROR': 35,
 'log_count/INFO': 18,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 19,
 'retry/count': 86,
 'retry/max_reached': 35,
 'retry/reason_count/twisted.internet.error.TimeoutError': 86,
 'scheduler/dequeued': 140,
 'scheduler/dequeued/memory': 140,
 'scheduler/enqueued': 140,
 'scheduler/enqueued/memory': 140,
 'start_time': datetime.datetime(2018, 9, 29, 1, 54, 30, 669303)}
2018-09-29 10:02:43 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 16:40:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 16:40:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 16:40:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 16:40:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 16:40:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 16:40:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 16:40:26 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 16:40:26 [scrapy.core.engine] INFO: Spider opened
2018-09-29 16:40:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 16:40:26 [eth_token] INFO: Spider opened: eth_token
2018-09-29 16:40:30 [eth_token] WARNING: Redis服务连接失败
2018-09-29 16:40:30 [eth_token] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A39B0>
2018-09-29 16:42:08 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 16:42:35 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 16:43:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x1985365e9f78359a9B6AD760e32412f4a445E862> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xa74476443119A942dE498590Fe1f2454d7D4aC0d> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xa15c7ebe1f07caf6bff097d8a589fb8ac49ae5b3> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xcb97e65f07da24d46bcdd078ebebd7c6e6e3d750> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x0d8775f648430679a709e98d2b0cb6250d2887ef> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x5ca9a71b1d01849c0a95490cc00559717fcf0d1d> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xb5a5f22694352c15b00323844ad545abb2b11028> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x05f4a42e251f2d52b8ed15e9fedaacfcef1fad27> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xe41d2489571d322189246dafa5ebde1f4699f498> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x9f8f72aa9304c8b593d555f12ef6589cc3a579a2> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x4CEdA7906a5Ed2179785Cd3A40A69ee8bc99C466> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x168296bb09e24a88805cb9c33356536b980d3fc5> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xfa1a856cfa3409cfa145fa4e20eb270df3eb21ab> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:26 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 16:43:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x8dd5fbce2f6a956c3022ba3663759011dd51e73e> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xb7cb1c96db6b22b0d3d9536e0108d062bd488f74> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:27 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2018-09-29 16:43:27 [scrapy.core.engine] INFO: Closing spider (shutdown)
2018-09-29 16:43:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x514910771af9ca656af840dff83e8264ecf986ca> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xd4fa1460f537bb9085d22c7bccb5dd450ef28e3a> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x744d70fdbe2ba4cf95131626614a1763df805b9e> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x6c6ee5e31d828de241282b9606c8e98ea48526e2> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xd26114cd6EE289AccF82350c8d8487fedB8A0C07> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x419d0d8bdd9af5e606ae2232ed285aff190e711b> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x08d32b0da63e2C3bcF8019c9c5d849d7a9d791e6> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xbf2179859fc6d5bee9bf9158632dc51678a4100e> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x1f573d6fb3f13d689ff844b4ce37794d79a7ff1c> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x6f259637dcd74c767781e37bc6133cd6a68aa161> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xd850942ef8811f2a866692a623011bde52a462c1> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x39bb259f66e1c59d5abef88375979b4d20d98022> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x595832f8fc6bf59c85c527fec3740a1b7a361269> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xb63b606ac810a52cca15e44bb630fd42d8d1d83d> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xf85feea2fdd81d51177f6b8f35f0e6734ce45f5f> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xe0b7927c4af23765cb51314a0e0521a9645f0e2a> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xf0ee6b27b759c9893ce4f094b49ad28fd15a23e4> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xea11755ae41d889ceec39a63e6ff75a02bc1c00d> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x12480e24eb5bec1a9d4369cab6a80cad3c0a377a> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xb91318f35bdb262e9423bc7c7c2a3a93dd93c92c> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x818fc6c2ec5986bc6e2cbf00939d90556ab12ce5> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x9992ec3cf6a55b00978cddf2b27bc6882d88d1ec> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xf629cbd94d3791c9250152bd8dfbdf380e2a3b9c> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xdd974d5c2e2928dea5f71b9825b8b646686bd200> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x89d24a6b4ccb1b6faa2625fe562bdd9a23260359> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xa4e8c3ec456107ea67d3075bf9e3df3a75823db0> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xef68e7c694f40c8202821edf525de3782458639f> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x419c4db4b9e25d6db2ad9691ccb832c8d9fda05e> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x3883f5e181fccaf8410fa61e12b59bad963fb645> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0xB97048628DB6B661D4C2aA833e95Dbe1A905B280> (referer: https://etherscan.io/tokens?p=1)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\eth_token.py", line 65, in detail
    item['created_at'] = time.strftime('%Y-%m-%d %X', time.localtime())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'EthToken does not support field: created_at'
2018-09-29 16:43:47 [eth_token] INFO: eth_token关闭mysql数据库连接
2018-09-29 16:43:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 4,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 4,
 'downloader/request_bytes': 24922,
 'downloader/request_count': 52,
 'downloader/request_method_count/GET': 52,
 'downloader/response_bytes': 605116,
 'downloader/response_count': 48,
 'downloader/response_status_count/200': 47,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2018, 9, 29, 8, 43, 47, 230052),
 'log_count/ERROR': 45,
 'log_count/INFO': 14,
 'log_count/WARNING': 1,
 'request_depth_max': 2,
 'response_received_count': 48,
 'retry/count': 4,
 'retry/reason_count/twisted.internet.error.TimeoutError': 4,
 'scheduler/dequeued': 52,
 'scheduler/dequeued/memory': 52,
 'scheduler/enqueued': 108,
 'scheduler/enqueued/memory': 108,
 'spider_exceptions/KeyError': 45,
 'start_time': datetime.datetime(2018, 9, 29, 8, 40, 26, 487591)}
2018-09-29 16:43:47 [scrapy.core.engine] INFO: Spider closed (shutdown)
2018-09-29 16:48:30 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 16:48:30 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 16:48:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 16:48:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 16:48:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 16:48:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 16:48:31 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 16:48:31 [scrapy.core.engine] INFO: Spider opened
2018-09-29 16:48:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 16:48:31 [eth_token] INFO: Spider opened: eth_token
2018-09-29 16:48:35 [eth_token] WARNING: Redis服务连接失败
2018-09-29 16:48:35 [eth_token] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A39E8>
2018-09-29 16:49:31 [scrapy.extensions.logstats] INFO: Crawled 51 pages (at 51 pages/min), scraped 42 items (at 42 items/min)
2018-09-29 16:50:31 [scrapy.extensions.logstats] INFO: Crawled 148 pages (at 97 pages/min), scraped 102 items (at 60 items/min)
2018-09-29 16:51:26 [py.warnings] WARNING: C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py:170: Warning: (1265, "Data truncated for column 'en_name' at row 1")
  result = self._query(query)

2018-09-29 16:51:31 [scrapy.extensions.logstats] INFO: Crawled 239 pages (at 91 pages/min), scraped 162 items (at 60 items/min)
2018-09-29 16:52:31 [scrapy.extensions.logstats] INFO: Crawled 333 pages (at 94 pages/min), scraped 218 items (at 56 items/min)
2018-09-29 16:53:31 [scrapy.extensions.logstats] INFO: Crawled 429 pages (at 96 pages/min), scraped 281 items (at 63 items/min)
2018-09-29 16:54:31 [scrapy.extensions.logstats] INFO: Crawled 524 pages (at 95 pages/min), scraped 340 items (at 59 items/min)
2018-09-29 16:55:31 [scrapy.extensions.logstats] INFO: Crawled 622 pages (at 98 pages/min), scraped 399 items (at 59 items/min)
2018-09-29 16:56:31 [scrapy.extensions.logstats] INFO: Crawled 715 pages (at 93 pages/min), scraped 457 items (at 58 items/min)
2018-09-29 16:57:31 [scrapy.extensions.logstats] INFO: Crawled 810 pages (at 95 pages/min), scraped 517 items (at 60 items/min)
2018-09-29 16:58:31 [scrapy.extensions.logstats] INFO: Crawled 902 pages (at 92 pages/min), scraped 573 items (at 56 items/min)
2018-09-29 16:59:31 [scrapy.extensions.logstats] INFO: Crawled 1000 pages (at 98 pages/min), scraped 633 items (at 60 items/min)
2018-09-29 16:59:36 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 16:59:36 [eth_token] INFO: eth_token关闭mysql数据库连接
2018-09-29 16:59:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 494750,
 'downloader/request_count': 1010,
 'downloader/request_method_count/GET': 1010,
 'downloader/response_bytes': 9654329,
 'downloader/response_count': 1009,
 'downloader/response_status_count/200': 944,
 'downloader/response_status_count/403': 65,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 8, 59, 36, 557801),
 'item_scraped_count': 641,
 'log_count/INFO': 21,
 'log_count/WARNING': 2,
 'request_depth_max': 16,
 'response_received_count': 1009,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 1,
 'scheduler/dequeued': 1010,
 'scheduler/dequeued/memory': 1010,
 'scheduler/enqueued': 1010,
 'scheduler/enqueued/memory': 1010,
 'start_time': datetime.datetime(2018, 9, 29, 8, 48, 31, 363935)}
2018-09-29 16:59:36 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:25:20 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:25:20 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:25:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:25:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:25:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:25:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:25:21 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:25:21 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:25:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:25:21 [token_address] INFO: Spider opened: token_address
2018-09-29 17:25:25 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:25:25 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005798B38>
2018-09-29 17:25:39 [scrapy.core.scraper] ERROR: Error processing {'data_list': [{'address': '0x606af0bd4501855914b50e2672c5926b896737ef',
                'amount': 152237800.873333,
                'market': '0xProtocol_MultiSig1',
                'panking': '1',
                'percent': 15.2238},
               {'address': '0x206376e8940e42538781cd94ef024df3c1e0fd43',
                'amount': 147085790.724108,
                'market': '0xProtocol_ExtDevPool',
                'panking': '2',
                'percent': 14.7086},
               {'address': '0xdb63d40c033d35e79cdbb21430f0fe10e9d97303',
                'amount': 124022267.53547812,
                'market': '0xProtocol_TeamVesting',
                'panking': '3',
                'percent': 12.4022},
               {'address': '0xfe9e8709d3215310075d67e3ed32a380ccf451c8',
                'amount': 89789329.26831067,
                'market': 'Binance_5',
                'panking': '4',
                'percent': 8.9789},
               {'address': '0xead6be34ce315940264519f250d8160f369fa5cd',
                'amount': 36783906.749327265,
                'market': None,
                'panking': '5',
                'percent': 3.6784},
               {'address': '0xe4abc54f5a6288b60c18b361442a151fc4911da6',
                'amount': 32733335.73682775,
                'market': 'Polychain',
                'panking': '6',
                'percent': 3.2733},
               {'address': '0xFBb1b73C4f0BDa4f67dcA266ce6Ef42f520fBB98',
                'amount': 17885124.91150169,
                'market': 'Bittrex_1',
                'panking': '7',
                'percent': 1.7885},
               {'address': '0x3f5ce5fbfe3e9af3971dd833d26ba9b5c936f0be',
                'amount': 17719358.1402717,
                'market': 'Binance_1',
                'panking': '8',
                'percent': 1.7719},
               {'address': '0x742d35cc6634c0532925a3b844bc454e4438f44e',
                'amount': 12007172.30405,
                'market': 'Bitfinex_5',
                'panking': '9',
                'percent': 1.2007},
               {'address': '0x15ab2321d7e83d00c015048b567f4f6aadc1b022',
                'amount': 6658950.6600476485,
                'market': None,
                'panking': '10',
                'percent': 0.6659},
               {'address': '0xeC9Ec27165250D96B64Ee54e366A805C2C465F3E',
                'amount': 6389523.629776333,
                'market': None,
                'panking': '11',
                'percent': 0.639},
               {'address': '0x77Ad7D9c2439566D7Fa08D30ef2e76D115C930B6',
                'amount': 5916225.583126234,
                'market': None,
                'panking': '12',
                'percent': 0.5916},
               {'address': '0x1b0776b83cc820b2c5f21f97025a80b7e4937774',
                'amount': 5180284.92571088,
                'market': None,
                'panking': '13',
                'percent': 0.518},
               {'address': '0x4aeacad51ac19e0b4cac5181b25d6a447ccd14b4',
                'amount': 4674559.68859672,
                'market': None,
                'panking': '14',
                'percent': 0.4675},
               {'address': '0x7d439999e63b75618b9c6c69d6efed0c2bc295c8',
                'amount': 4566274.2073638,
                'market': None,
                'panking': '15',
                'percent': 0.4566},
               {'address': '0x93c0D481cAa8a5dd3B0E46e2e2B06A9A29fD95B5',
                'amount': 4186902.741501708,
                'market': None,
                'panking': '16',
                'percent': 0.4187},
               {'address': '0x10c34f1ead7e3861cdc8e2210ebc0fa4e4d9a2ca',
                'amount': 3806133.404117191,
                'market': None,
                'panking': '17',
                'percent': 0.3806},
               {'address': '0xb5b9e9a2e6de28fd5d17cd19970d0fe48353a98a',
                'amount': 3786384.37320079,
                'market': None,
                'panking': '18',
                'percent': 0.3786},
               {'address': '0x574ed0f2ebd15f566268d67f36dfe9c24a56e434',
                'amount': 3200000.0,
                'market': None,
                'panking': '19',
                'percent': 0.32},
               {'address': '0xadb2b42f6bd96f5c65920b9ac88619dce4166f94',
                'amount': 3005591.650408048,
                'market': 'Huobi_7',
                'panking': '20',
                'percent': 0.3006},
               {'address': '0x2e70a775b347d198b683909981ac09e27ba22103',
                'amount': 2965996.35,
                'market': None,
                'panking': '21',
                'percent': 0.2966},
               {'address': '0xcca71809e8870afeb72c4720d0fe50d5c3230e05',
                'amount': 2722231.464927141,
                'market': None,
                'panking': '22',
                'percent': 0.2722},
               {'address': '0xcf675d8847a36d80703c26a2ede8991916293583',
                'amount': 2539083.0,
                'market': None,
                'panking': '23',
                'percent': 0.2539},
               {'address': '0xdaf01f6d502b4e7476b85bab0effd92b79f1c8cf',
                'amount': 2467232.9244067855,
                'market': None,
                'panking': '24',
                'percent': 0.2467},
               {'address': '0xa996f1a68befa5b92e451b35f5cfbf7eaad682a4',
                'amount': 2407203.574087508,
                'market': None,
                'panking': '25',
                'percent': 0.2407},
               {'address': '0xd551234ae421e3bcba99a0da6d736074f22192ff',
                'amount': 2368729.95407397,
                'market': 'Binance_2',
                'panking': '26',
                'percent': 0.2369},
               {'address': '0xdf2c8d6d25c8ee258502805d6623998d6c141e34',
                'amount': 2318710.94577282,
                'market': None,
                'panking': '27',
                'percent': 0.2319},
               {'address': '0x0681d8db095565fe8a346fa0277bffde9c0edbbf',
                'amount': 2084268.41229976,
                'market': 'Binance_4',
                'panking': '28',
                'percent': 0.2084},
               {'address': '0x564286362092d8e7936f0549571a803b203aaced',
                'amount': 1922636.40663118,
                'market': 'Binance_3',
                'panking': '29',
                'percent': 0.1923},
               {'address': '0x1c3b915292a2ffca1962038569a8bd09551165b7',
                'amount': 1887757.31657,
                'market': None,
                'panking': '30',
                'percent': 0.1888},
               {'address': '0xec9ce6e59ce95248ecef070408e7cf35bbc79989',
                'amount': 1886392.6309440618,
                'market': None,
                'panking': '31',
                'percent': 0.1886},
               {'address': '0x387fc6939b5e54b2f11793df05388f9d11942948',
                'amount': 1880000.0,
                'market': None,
                'panking': '32',
                'percent': 0.188},
               {'address': '0xd03cf9fa773da40475baead68e59c954c5ac77b6',
                'amount': 1748950.47,
                'market': None,
                'panking': '33',
                'percent': 0.1749},
               {'address': '0x5e575279bf9f4acf0a130c186861454247394c06',
                'amount': 1724597.704253114,
                'market': 'Liqui.io_2',
                'panking': '34',
                'percent': 0.1725},
               {'address': '0x22573c3fdc88adcce0ea3800331eadaac5e2dad0',
                'amount': 1368894.1516128208,
                'market': None,
                'panking': '35',
                'percent': 0.1369},
               {'address': '0x186f5b891ccce6240acd69cf5b0982d51fc14dee',
                'amount': 1343981.42795915,
                'market': None,
                'panking': '36',
                'percent': 0.1344},
               {'address': '0xfd72fac7d08f9912e5d02091417304d5a0072e2b',
                'amount': 1340256.7618042019,
                'market': None,
                'panking': '37',
                'percent': 0.134},
               {'address': '0x85b5022bc07b21d69a0c3656ad74286e137cf5dd',
                'amount': 1324568.7129191374,
                'market': None,
                'panking': '38',
                'percent': 0.1325},
               {'address': '0x6cc5f688a315f3dc28a7781717a9a798a59fda7b',
                'amount': 1320137.2102790212,
                'market': 'Okex',
                'panking': '39',
                'percent': 0.132},
               {'address': '0xba92851953a04c9b0dea5d73f66598ea04dff20d',
                'amount': 1301569.6282877715,
                'market': None,
                'panking': '40',
                'percent': 0.1302},
               {'address': '0x8d12a197cb00d4747a1fe03395095ce2a5cc6819',
                'amount': 1268977.5182237301,
                'market': 'EtherDelta_2',
                'panking': '41',
                'percent': 0.1269},
               {'address': '0x1dc9b91de003fd503f25cb5d114cf0fc68f7afe6',
                'amount': 1218577.1477253127,
                'market': None,
                'panking': '42',
                'percent': 0.1219},
               {'address': '0x784b5468ca56abbc05a12e3983fbfd9dc5ea2fed',
                'amount': 1145943.3149563163,
                'market': None,
                'panking': '43',
                'percent': 0.1146},
               {'address': '0xcac737603fd3d83cfcd213e94cdbfdd2eb60c958',
                'amount': 1137176.347390952,
                'market': None,
                'panking': '44',
                'percent': 0.1137},
               {'address': '0xf84d1bb3f378c9fc1075f6f569da27dd69ddf9c2',
                'amount': 1110564.109,
                'market': None,
                'panking': '45',
                'percent': 0.1111},
               {'address': '0x3935e417ed92c3e2ef93e2555ccae1d46b18b742',
                'amount': 1067084.3329,
                'market': None,
                'panking': '46',
                'percent': 0.1067},
               {'address': '0x2474df399c1670223325cd07d64341e940416cf3',
                'amount': 1060065.67,
                'market': None,
                'panking': '47',
                'percent': 0.106},
               {'address': '0x7e7da68a7d2bf9d586563e25ec4986828188d679',
                'amount': 1000000.0,
                'market': None,
                'panking': '48',
                'percent': 0.1},
               {'address': '0x82f61576ac8d9c403587265669c829f73b6d7ce4',
                'amount': 1000000.0,
                'market': None,
                'panking': '49',
                'percent': 0.1},
               {'address': '0xc3da0e85b898711a4e8913be33b32f22e661f758',
                'amount': 1000000.0,
                'market': None,
                'panking': '50',
                'percent': 0.1},
               {'address': '0xaf54743051f61fda465609460f081c9aa426569e',
                'amount': 1000000.0,
                'market': None,
                'panking': '51',
                'percent': 0.1},
               {'address': '0x2a31b923b5b30962628acabc50a61a777ced1f21',
                'amount': 955086.189,
                'market': None,
                'panking': '52',
                'percent': 0.0955},
               {'address': '0x1c4b70a3968436b9a0a9cf5205c787eb81bb558c',
                'amount': 940311.66846241,
                'market': 'Gate.io_3',
                'panking': '53',
                'percent': 0.094},
               {'address': '0x4e06a63cc06772a19cd341b54f8dacaa7d873d7f',
                'amount': 928798.32,
                'market': None,
                'panking': '54',
                'percent': 0.0929},
               {'address': '0x1126fcf3f1d02a3ca5044e8917d4e68dc94e3252',
                'amount': 863746.68434783,
                'market': None,
                'panking': '55',
                'percent': 0.0864},
               {'address': '0xd8ff522485c6cfe2cd7c0b7a4197762c4a581fbe',
                'amount': 835961.0,
                'market': None,
                'panking': '56',
                'percent': 0.0836},
               {'address': '0x571cb8fe59ed9da0828127d978997a0cf2fdbde6',
                'amount': 823698.4325653139,
                'market': None,
                'panking': '57',
                'percent': 0.0824},
               {'address': '0x301790b0031f00dbfbce60692f5e7a049b23a989',
                'amount': 820833.333,
                'market': None,
                'panking': '58',
                'percent': 0.0821},
               {'address': '0x4687466ac9bd165ca3bf2f6b6446822560b9fc3d',
                'amount': 800000.0,
                'market': None,
                'panking': '59',
                'percent': 0.08},
               {'address': '0x681aefa75c0f3b26bf052b2869ce3bb12d7930d2',
                'amount': 800000.0,
                'market': None,
                'panking': '60',
                'percent': 0.08},
               {'address': '0x29d372320c18315b549dce4820963776807c5d71',
                'amount': 794431.1209209844,
                'market': None,
                'panking': '61',
                'percent': 0.0794},
               {'address': '0x5bd98a1cb7b8e4ec9d8a25472f9f1976af0c85e7',
                'amount': 788366.13,
                'market': None,
                'panking': '62',
                'percent': 0.0788},
               {'address': '0xa12431d0b9db640034b0cdfceef9cce161e62be4',
                'amount': 786233.6027149108,
                'market': None,
                'panking': '63',
                'percent': 0.0786},
               {'address': '0x05455f0f8a4b42c1bf08e2c842c68645f901e6c8',
                'amount': 785075.0,
                'market': None,
                'panking': '64',
                'percent': 0.0785},
               {'address': '0x9134d45bff99a106a4d386b29e64e8d534a53102',
                'amount': 768777.4004649428,
                'market': None,
                'panking': '65',
                'percent': 0.0769},
               {'address': '0x67a9d28790077ed6407ec57377584ec301130f61',
                'amount': 764382.0,
                'market': None,
                'panking': '66',
                'percent': 0.0764},
               {'address': '0x035483926d143b394eb9fc8b974b44ebf338dfc7',
                'amount': 763796.96076708,
                'market': None,
                'panking': '67',
                'percent': 0.0764},
               {'address': '0xae4a41b52a47370c754b6d85e156fe5f659594a9',
                'amount': 746250.0,
                'market': None,
                'panking': '68',
                'percent': 0.0746},
               {'address': '0x719af1fac896acf3d5cab38b3159d05cc2645a63',
                'amount': 700000.31867108,
                'market': None,
                'panking': '69',
                'percent': 0.07},
               {'address': '0x97ca371d59bbfefdb391aa6dcbdf4455fec361f2',
                'amount': 692573.13441825,
                'market': None,
                'panking': '70',
                'percent': 0.0693},
               {'address': '0x47139cbec96ff8e2d06e0595f966827e400492c3',
                'amount': 687518.78830409,
                'market': None,
                'panking': '71',
                'percent': 0.0688},
               {'address': '0x9f68af2f8c68d9e0438fc6131356fdfff4b0019c',
                'amount': 665267.9237419164,
                'market': None,
                'panking': '72',
                'percent': 0.0665},
               {'address': '0xf34d2c5da5b1459d1f0ab5dcc3777af0cdcec75e',
                'amount': 661769.46745467,
                'market': None,
                'panking': '73',
                'percent': 0.0662},
               {'address': '0x876eabf441b2ee5b5b0554fd502a8e0600950cfa',
                'amount': 625636.9962318552,
                'market': 'Bitfinex_4',
                'panking': '74',
                'percent': 0.0626},
               {'address': '0x3fda67f7583380e67ef93072294a7fac882fd7e7',
                'amount': 607809.974673995,
                'market': 'Compound_MoneyMarket',
                'panking': '75',
                'percent': 0.0608},
               {'address': '0x54afbc918a2b49c289d76645708e4002c84da439',
                'amount': 604317.98765432,
                'market': None,
                'panking': '76',
                'percent': 0.0604},
               {'address': '0x1d4acbc9f70cadd4e6eb215731b8a20bb848d042',
                'amount': 600000.0,
                'market': None,
                'panking': '77',
                'percent': 0.06},
               {'address': '0x36ff9ca5d91619dce19478a9d44a8934e6ce1def',
                'amount': 600000.0,
                'market': None,
                'panking': '78',
                'percent': 0.06},
               {'address': '0xde210746677fd1034c158332b0db802da8f9dd2e',
                'amount': 575876.9272748983,
                'market': None,
                'panking': '79',
                'percent': 0.0576},
               {'address': '0x0489076a0d17394835af93cd62acff703b6814a9',
                'amount': 572930.3846990707,
                'market': None,
                'panking': '80',
                'percent': 0.0573},
               {'address': '0x79fe88a01bf6d2d939f06af59f6915c45588db75',
                'amount': 571953.8368875083,
                'market': None,
                'panking': '81',
                'percent': 0.0572},
               {'address': '0xbd2118740e6bec4fb1721ee9aeaee0d0611e4890',
                'amount': 566677.1671205944,
                'market': None,
                'panking': '82',
                'percent': 0.0567},
               {'address': '0x72bcfa6932feacd91cb2ea44b0731ed8ae04d0d3',
                'amount': 562678.92727715,
                'market': None,
                'panking': '83',
                'percent': 0.0563},
               {'address': '0x9d742a103bc5a6d5f309810358bcd8ea0b0992bc',
                'amount': 559211.8678,
                'market': None,
                'panking': '84',
                'percent': 0.0559},
               {'address': '0xa3ff11816c083f2844e2493f98ed9d4584ffd028',
                'amount': 555108.4678,
                'market': None,
                'panking': '85',
                'percent': 0.0555},
               {'address': '0xb4d997342691bccc504caddfd72aaf70390dd763',
                'amount': 538840.97505316,
                'market': None,
                'panking': '86',
                'percent': 0.0539},
               {'address': '0xffe38ba0210772ce0e37bbcf4ef4e98721360135',
                'amount': 528640.63255468,
                'market': None,
                'panking': '87',
                'percent': 0.0529},
               {'address': '0x8e279e54b04327adf57117c19bc3950d7109407c',
                'amount': 509777.206,
                'market': None,
                'panking': '88',
                'percent': 0.051},
               {'address': '0x23fcf6b8385a8b4836d35aa9f85ccad75cfcd5d1',
                'amount': 500901.1976,
                'market': None,
                'panking': '89',
                'percent': 0.0501},
               {'address': '0xc20b38875ad70e96a114c227552ea3cdc65e67cc',
                'amount': 500000.52638708,
                'market': None,
                'panking': '90',
                'percent': 0.05},
               {'address': '0x7b9ed7c54dd182e056141ece63c20a681c9d1b84',
                'amount': 500000.0,
                'market': None,
                'panking': '91',
                'percent': 0.05},
               {'address': '0x508bc5bd31f566eee5f7ac38038f4b69d200daaf',
                'amount': 490196.07843137,
                'market': None,
                'panking': '92',
                'percent': 0.049},
               {'address': '0x660c2620416bc7724f35f8852dc70e6413b15867',
                'amount': 490196.07843137,
                'market': None,
                'panking': '93',
                'percent': 0.049},
               {'address': '0x2fcdef09932b7087e9470934c55b69ed3b13a041',
                'amount': 478025.96811689,
                'market': None,
                'panking': '94',
                'percent': 0.0478},
               {'address': '0xb73fd104ab7c6eb2fd4037d826b08814cc433217',
                'amount': 476832.63162055,
                'market': None,
                'panking': '95',
                'percent': 0.0477},
               {'address': '0xa82a8af2be904a51bf99218728b421b86708c5e5',
                'amount': 470184.3726,
                'market': None,
                'panking': '96',
                'percent': 0.047},
               {'address': '0xeb7e8db095550d54035866aa45f887a07bac18e6',
                'amount': 466248.49581263884,
                'market': None,
                'panking': '97',
                'percent': 0.0466},
               {'address': '0x9b0e7Dc94fbF2AD2Ae33b553b293EB5926CAd88f',
                'amount': 465000.32092343,
                'market': None,
                'panking': '98',
                'percent': 0.0465},
               {'address': '0x93e96fe4f91dafc9ca8a4b160b1e152219df11b7',
                'amount': 459542.47327231,
                'market': None,
                'panking': '99',
                'percent': 0.046},
               {'address': '0xde86ea8867e63a4133e255a3e2ff912dfea6d658',
                'amount': 451558.358,
                'market': None,
                'panking': '100',
                'percent': 0.0452}],
 'time_now': '2018-09-29 17:25:39',
 'token': '0xe41d2489571d322189246dafa5ebde1f4699f498',
 'top_list': [63.6922, 68.1634, 73.15669999999996, 76.42699999999994]}
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_address.py", line 17, in process_item
    token_id = item['token_id']
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\item.py", line 59, in __getitem__
    return self._values[key]
KeyError: 'token_id'
2018-09-29 17:25:39 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:25:39 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:25:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 304,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49554,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 25, 39, 579208),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 25, 21, 609347)}
2018-09-29 17:25:39 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:31:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:31:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:31:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:31:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:31:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:31:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:31:42 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:31:42 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:31:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:31:42 [token_address] INFO: Spider opened: token_address
2018-09-29 17:31:46 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:31:46 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2A90>
2018-09-29 17:31:59 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:31:59 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:31:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 340,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49043,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 31, 59, 89318),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 31, 42, 37859)}
2018-09-29 17:31:59 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:33:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:33:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:33:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:33:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:33:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:33:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:33:56 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:33:56 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:33:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:33:56 [token_address] INFO: Spider opened: token_address
2018-09-29 17:34:00 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:34:00 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2B00>
2018-09-29 17:34:07 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:34:07 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:34:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 278,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 48553,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 34, 7, 656709),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 33, 56, 136475)}
2018-09-29 17:34:07 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:34:39 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:34:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:34:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:34:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:34:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:34:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:34:40 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:34:40 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:34:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:34:40 [token_address] INFO: Spider opened: token_address
2018-09-29 17:34:44 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:34:44 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2B00>
2018-09-29 17:34:58 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:34:58 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:34:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 334,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 48864,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 34, 58, 890),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 34, 40, 44629)}
2018-09-29 17:34:58 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:35:13 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:35:13 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:35:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:35:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:35:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:35:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:35:14 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:35:14 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:35:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:35:14 [token_address] INFO: Spider opened: token_address
2018-09-29 17:35:18 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:35:18 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2B00>
2018-09-29 17:35:20 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:35:20 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:35:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 316,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 48993,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 35, 20, 873763),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 35, 14, 815939)}
2018-09-29 17:35:20 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:35:52 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:35:52 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:35:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:35:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:35:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:35:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:35:53 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:35:53 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:35:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:35:53 [token_address] INFO: Spider opened: token_address
2018-09-29 17:35:57 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:35:57 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2B00>
2018-09-29 17:36:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:36:11 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:36:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 357,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49299,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 36, 11, 746965),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 35, 53, 743296)}
2018-09-29 17:36:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:36:22 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:36:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:36:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:36:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:36:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:36:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:36:23 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:36:23 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:36:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:36:23 [token_address] INFO: Spider opened: token_address
2018-09-29 17:36:27 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:36:27 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2B00>
2018-09-29 17:36:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:36:41 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:36:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 324,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49292,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 36, 41, 324267),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 36, 23, 298011)}
2018-09-29 17:36:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:37:10 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:37:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:37:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:37:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:37:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:37:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:37:11 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:37:11 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:37:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:37:11 [token_address] INFO: Spider opened: token_address
2018-09-29 17:37:15 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:37:15 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2B00>
2018-09-29 17:37:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/tokenholderchart/0xe41d2489571d322189246dafa5ebde1f4699f498> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\token_address.py", line 28, in parse
    addresses = response.css('center p::text').extract()[-1]
IndexError: list index out of range
2018-09-29 17:37:20 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:37:20 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:37:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 269,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1800,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 37, 20, 880204),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 37, 11, 238973)}
2018-09-29 17:37:20 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:37:47 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:37:47 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:37:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:37:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:37:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:37:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:37:48 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:37:48 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:37:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:37:48 [token_address] INFO: Spider opened: token_address
2018-09-29 17:37:52 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:37:52 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2B00>
2018-09-29 17:38:05 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:38:05 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:38:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 335,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49045,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 38, 5, 266204),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 37, 48, 383944)}
2018-09-29 17:38:05 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:39:05 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:39:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:39:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:39:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:39:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:39:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:39:06 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:39:06 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:39:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:39:06 [token_address] INFO: Spider opened: token_address
2018-09-29 17:39:10 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:39:10 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2AC8>
2018-09-29 17:39:17 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:39:17 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:39:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49298,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 39, 17, 629467),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 39, 6, 332229)}
2018-09-29 17:39:17 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:40:00 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:40:00 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:40:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:40:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:40:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:40:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:40:01 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:40:01 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:40:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:40:01 [token_address] INFO: Spider opened: token_address
2018-09-29 17:40:05 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:40:05 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2AC8>
2018-09-29 17:40:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/tokenholderchart/0x4470BB87d77b963A013DB939BE332f927f2b992e> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\token_address.py", line 47, in parse
    percent = float(percent.rstrip('%'))
ValueError: could not convert string to float: '-'
2018-09-29 17:40:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:40:11 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:40:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 360,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 44361,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 40, 11, 269065),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 40, 1, 325830)}
2018-09-29 17:40:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 17:58:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 17:58:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 17:58:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 17:58:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 17:58:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 17:58:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 17:58:45 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 17:58:45 [scrapy.core.engine] INFO: Spider opened
2018-09-29 17:58:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 17:58:45 [token_address] INFO: Spider opened: token_address
2018-09-29 17:58:49 [token_address] WARNING: Redis服务连接失败
2018-09-29 17:58:49 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B2AC8>
2018-09-29 17:58:56 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 17:58:56 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 17:58:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 359,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49326,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 9, 58, 56, 632670),
 'item_scraped_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 9, 58, 45, 616029)}
2018-09-29 17:58:56 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-29 18:02:23 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-29 18:02:23 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-29 18:02:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-29 18:02:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-29 18:02:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-29 18:02:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-29 18:02:24 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-29 18:02:24 [scrapy.core.engine] INFO: Spider opened
2018-09-29 18:02:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-29 18:02:24 [token_address] INFO: Spider opened: token_address
2018-09-29 18:02:28 [token_address] WARNING: Redis服务连接失败
2018-09-29 18:02:28 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2AC8>
2018-09-29 18:02:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-29 18:02:34 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-29 18:02:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49001,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 29, 10, 2, 34, 558037),
 'item_scraped_count': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 29, 10, 2, 24, 132409)}
2018-09-29 18:02:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 09:35:39 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 09:35:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 09:35:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 09:35:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 09:35:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 09:35:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 09:35:40 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 09:35:40 [scrapy.core.engine] INFO: Spider opened
2018-09-30 09:35:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 09:35:40 [token_address] INFO: Spider opened: token_address
2018-09-30 09:35:41 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 09:35:41 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B1C18>
2018-09-30 09:35:43 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 09:35:43 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 09:35:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49423,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 1, 35, 43, 83387),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 30, 1, 35, 40, 180782)}
2018-09-30 09:35:43 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 09:50:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 09:50:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 09:50:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 09:50:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 09:50:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 09:50:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 09:50:05 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 09:50:05 [scrapy.core.engine] INFO: Spider opened
2018-09-30 09:50:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 09:50:05 [eth_token] INFO: Spider opened: eth_token
2018-09-30 09:50:06 [eth_token] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 09:50:06 [eth_token] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B2C18>
2018-09-30 09:51:05 [scrapy.extensions.logstats] INFO: Crawled 88 pages (at 88 pages/min), scraped 60 items (at 60 items/min)
2018-09-30 09:52:05 [scrapy.extensions.logstats] INFO: Crawled 185 pages (at 97 pages/min), scraped 120 items (at 60 items/min)
2018-09-30 09:53:05 [scrapy.extensions.logstats] INFO: Crawled 283 pages (at 98 pages/min), scraped 178 items (at 58 items/min)
2018-09-30 09:54:05 [scrapy.extensions.logstats] INFO: Crawled 381 pages (at 98 pages/min), scraped 236 items (at 58 items/min)
2018-09-30 09:55:05 [scrapy.extensions.logstats] INFO: Crawled 482 pages (at 101 pages/min), scraped 296 items (at 60 items/min)
2018-09-30 09:56:05 [scrapy.extensions.logstats] INFO: Crawled 581 pages (at 99 pages/min), scraped 359 items (at 63 items/min)
2018-09-30 09:57:05 [scrapy.extensions.logstats] INFO: Crawled 678 pages (at 97 pages/min), scraped 417 items (at 58 items/min)
2018-09-30 09:58:05 [scrapy.extensions.logstats] INFO: Crawled 777 pages (at 99 pages/min), scraped 478 items (at 61 items/min)
2018-09-30 09:59:05 [scrapy.extensions.logstats] INFO: Crawled 876 pages (at 99 pages/min), scraped 533 items (at 55 items/min)
2018-09-30 10:00:05 [scrapy.extensions.logstats] INFO: Crawled 970 pages (at 94 pages/min), scraped 590 items (at 57 items/min)
2018-09-30 10:01:00 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 10:01:00 [eth_token] INFO: eth_token关闭mysql数据库连接
2018-09-30 10:01:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 4,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 4,
 'downloader/request_bytes': 513408,
 'downloader/request_count': 1049,
 'downloader/request_method_count/GET': 1049,
 'downloader/response_bytes': 9656868,
 'downloader/response_count': 1045,
 'downloader/response_status_count/200': 959,
 'downloader/response_status_count/403': 86,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 2, 1, 0, 993420),
 'item_scraped_count': 641,
 'log_count/INFO': 21,
 'request_depth_max': 17,
 'response_received_count': 1045,
 'retry/count': 4,
 'retry/reason_count/twisted.internet.error.TimeoutError': 4,
 'scheduler/dequeued': 1049,
 'scheduler/dequeued/memory': 1049,
 'scheduler/enqueued': 1049,
 'scheduler/enqueued/memory': 1049,
 'start_time': datetime.datetime(2018, 9, 30, 1, 50, 5, 365330)}
2018-09-30 10:01:00 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 10:04:52 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 10:04:52 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 10:04:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 10:04:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 10:04:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 10:04:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 10:04:52 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 10:04:52 [scrapy.core.engine] INFO: Spider opened
2018-09-30 10:04:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 10:04:53 [eth_token] INFO: Spider opened: eth_token
2018-09-30 10:04:54 [eth_token] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 10:04:54 [eth_token] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000579CCC0>
2018-09-30 10:05:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 10:05:48 [eth_token] INFO: eth_token关闭mysql数据库连接
2018-09-30 10:05:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 2,
 'downloader/request_bytes': 35682,
 'downloader/request_count': 76,
 'downloader/request_method_count/GET': 76,
 'downloader/response_bytes': 787851,
 'downloader/response_count': 74,
 'downloader/response_status_count/200': 68,
 'downloader/response_status_count/403': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 2, 5, 48, 481475),
 'item_scraped_count': 50,
 'log_count/INFO': 11,
 'request_depth_max': 14,
 'response_received_count': 74,
 'retry/count': 2,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 76,
 'scheduler/dequeued/memory': 76,
 'scheduler/enqueued': 76,
 'scheduler/enqueued/memory': 76,
 'start_time': datetime.datetime(2018, 9, 30, 2, 4, 53, 5302)}
2018-09-30 10:05:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 10:11:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 10:11:14 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 10:11:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 10:11:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 10:11:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 10:11:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 10:11:15 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 10:11:15 [scrapy.core.engine] INFO: Spider opened
2018-09-30 10:11:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 10:11:15 [eth_token] INFO: Spider opened: eth_token
2018-09-30 10:11:16 [eth_token] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 10:11:16 [eth_token] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000579DC88>
2018-09-30 10:11:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 10:11:25 [eth_token] INFO: eth_token关闭mysql数据库连接
2018-09-30 10:11:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5888,
 'downloader/request_count': 14,
 'downloader/request_method_count/GET': 14,
 'downloader/response_bytes': 198337,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 13,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 2, 11, 25, 807945),
 'log_count/INFO': 11,
 'request_depth_max': 13,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 9, 30, 2, 11, 15, 58951)}
2018-09-30 10:11:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 10:13:51 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 10:13:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 10:13:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 10:13:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 10:13:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 10:13:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 10:13:52 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 10:13:52 [scrapy.core.engine] INFO: Spider opened
2018-09-30 10:13:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 10:13:52 [token_address] INFO: Spider opened: token_address
2018-09-30 10:13:53 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 10:13:53 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000579EC88>
2018-09-30 10:13:55 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 10:13:55 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 10:13:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 348,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43937,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 2, 13, 55, 431897),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 30, 2, 13, 52, 613090)}
2018-09-30 10:13:55 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 10:14:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 10:14:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 10:14:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 10:14:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 10:14:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 10:14:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 10:14:32 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 10:14:32 [scrapy.core.engine] INFO: Spider opened
2018-09-30 10:14:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 10:14:32 [token_address] INFO: Spider opened: token_address
2018-09-30 10:14:33 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 10:14:33 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000579EC88>
2018-09-30 10:14:35 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 10:14:35 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 10:14:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 360,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43909,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 2, 14, 35, 560331),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 30, 2, 14, 32, 569160)}
2018-09-30 10:14:35 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 10:33:10 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 10:33:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 10:33:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 10:33:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 10:33:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 10:33:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 10:33:11 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 10:33:11 [scrapy.core.engine] INFO: Spider opened
2018-09-30 10:33:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 10:33:11 [token_address] INFO: Spider opened: token_address
2018-09-30 10:33:12 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 10:33:12 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000579DCC0>
2018-09-30 10:33:14 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 10:33:14 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 10:33:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 293,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43829,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 2, 33, 14, 490322),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 30, 2, 33, 11, 709162)}
2018-09-30 10:33:14 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 10:34:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 10:34:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 10:34:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 10:34:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 10:34:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 10:34:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 10:34:26 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 10:34:26 [scrapy.core.engine] INFO: Spider opened
2018-09-30 10:34:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 10:34:26 [token_address] INFO: Spider opened: token_address
2018-09-30 10:34:27 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 10:34:27 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000579EC88>
2018-09-30 10:34:29 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 10:34:29 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 10:34:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 347,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43834,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 2, 34, 29, 164791),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 30, 2, 34, 26, 527640)}
2018-09-30 10:34:29 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 10:38:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 10:38:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 10:38:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 10:38:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 10:38:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 10:38:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 10:38:27 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 10:38:27 [scrapy.core.engine] INFO: Spider opened
2018-09-30 10:38:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 10:38:27 [token_address] INFO: Spider opened: token_address
2018-09-30 10:38:28 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 10:38:28 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000579ECC0>
2018-09-30 10:38:30 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 10:38:30 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 10:38:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 347,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43833,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 2, 38, 30, 317976),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 30, 2, 38, 27, 658824)}
2018-09-30 10:38:30 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 11:36:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 11:36:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 11:36:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 11:36:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 11:36:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 11:36:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 11:36:05 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 11:36:05 [scrapy.core.engine] INFO: Spider opened
2018-09-30 11:36:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 11:36:05 [token_address] INFO: Spider opened: token_address
2018-09-30 11:36:06 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 11:36:06 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B1DA0>
2018-09-30 11:36:10 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 11:36:10 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 11:36:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 653,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 90879,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 3, 36, 10, 378040),
 'item_scraped_count': 2,
 'log_count/INFO': 11,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 9, 30, 3, 36, 5, 687032)}
2018-09-30 11:36:10 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 11:37:48 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 11:37:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 11:37:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 11:37:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 11:37:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 11:37:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 11:37:49 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 11:37:49 [scrapy.core.engine] INFO: Spider opened
2018-09-30 11:37:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 11:37:49 [weibo_mblog] INFO: Spider opened: weibo_mblog
2018-09-30 11:37:50 [weibo_mblog] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 11:37:50 [weibo_mblog] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B4CC0>
2018-09-30 11:38:28 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 11:38:28 [weibo_mblog] INFO: weibo_mblog关闭mysql数据库连接
2018-09-30 11:38:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 30163,
 'downloader/request_count': 62,
 'downloader/request_method_count/GET': 62,
 'downloader/response_bytes': 331981,
 'downloader/response_count': 62,
 'downloader/response_status_count/200': 62,
 'dupefilter/filtered': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 3, 38, 28, 482710),
 'item_scraped_count': 47,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 62,
 'scheduler/dequeued': 62,
 'scheduler/dequeued/memory': 62,
 'scheduler/enqueued': 62,
 'scheduler/enqueued/memory': 62,
 'start_time': datetime.datetime(2018, 9, 30, 3, 37, 49, 499428)}
2018-09-30 11:38:28 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 14:30:38 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 14:30:38 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 14:30:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 14:30:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 14:30:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 14:30:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 14:30:38 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 14:30:38 [scrapy.core.engine] INFO: Spider opened
2018-09-30 14:30:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 14:30:38 [token_address] INFO: Spider opened: token_address
2018-09-30 14:30:39 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 14:30:39 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A3DA0>
2018-09-30 14:30:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 14:30:41 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 14:30:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 316,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 44091,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 6, 30, 41, 967494),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 30, 6, 30, 38, 936321)}
2018-09-30 14:30:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 14:31:38 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 14:31:38 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 14:31:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 14:31:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 14:31:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 14:31:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 14:31:39 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 14:31:39 [scrapy.core.engine] INFO: Spider opened
2018-09-30 14:31:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 14:31:39 [token_address] INFO: Spider opened: token_address
2018-09-30 14:31:40 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 14:31:40 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A4CC0>
2018-09-30 14:31:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/tokenholderchart/0xd850942ef8811f2a866692a623011bde52a462c1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\token_address.py", line 27, in parse
    addresses = response.css('center p::text').extract()[-10]
IndexError: list index out of range
2018-09-30 14:31:43 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 14:31:43 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 14:31:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 335,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 44104,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 6, 31, 43, 550017),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2018, 9, 30, 6, 31, 39, 659794)}
2018-09-30 14:31:43 [scrapy.core.engine] INFO: Spider closed (finished)
2018-09-30 14:37:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-09-30 14:37:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-09-30 14:37:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-09-30 14:37:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-09-30 14:37:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-09-30 14:37:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-09-30 14:37:19 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-09-30 14:37:19 [scrapy.core.engine] INFO: Spider opened
2018-09-30 14:37:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-09-30 14:37:19 [token_address] INFO: Spider opened: token_address
2018-09-30 14:37:20 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-09-30 14:37:20 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A4CC0>
2018-09-30 14:37:22 [scrapy.core.engine] INFO: Closing spider (finished)
2018-09-30 14:37:22 [token_address] INFO: token_address关闭mysql数据库连接
2018-09-30 14:37:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 324,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43714,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 9, 30, 6, 37, 22, 482403),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 9, 30, 6, 37, 19, 326222)}
2018-09-30 14:37:22 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 13:34:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 13:34:14 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 13:34:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 13:34:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 13:34:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 13:34:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 13:34:15 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 13:34:15 [scrapy.core.engine] INFO: Spider opened
2018-10-08 13:34:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 13:34:15 [token_address] INFO: Spider opened: token_address
2018-10-08 13:34:19 [token_address] WARNING: Redis服务连接失败
2018-10-08 13:34:19 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A4BE0>
2018-10-08 13:34:25 [scrapy.core.scraper] ERROR: Error processing {'addresses': 45522,
 'data_list': [{'address': '0x00000000000000000000045261d4ee77acdb3286',
                'amount': 230530627.17739764,
                'market': None,
                'panking': '1',
                'percent': 23.0531},
               {'address': '0x000000000000000000000000000000000000dead',
                'amount': 126621362.75886619,
                'market': 'ENS-BurnAddress',
                'panking': '2',
                'percent': 12.6621},
               {'address': '0x000000000000000000007cd4706885c41853ec7b',
                'amount': 71086125.8328599,
                'market': None,
                'panking': '3',
                'percent': 7.1086},
               {'address': '0x00000000000000000000a1d9b0f8b27091344b43',
                'amount': 28910349.945,
                'market': None,
                'panking': '4',
                'percent': 2.891},
               {'address': '0x0000000000000000000011d2c3b7d0362c824b27',
                'amount': 18375000.0,
                'market': None,
                'panking': '5',
                'percent': 1.8375},
               {'address': '0x0000000000000000000077a7ad93a8fb3c9dfabb',
                'amount': 18000000.0,
                'market': None,
                'panking': '6',
                'percent': 1.8},
               {'address': '0x00000000000000000000d813878b269353a99c18',
                'amount': 16500000.0,
                'market': None,
                'panking': '7',
                'percent': 1.65},
               {'address': '0x000000000000000000000a85a0f055e3cc860540',
                'amount': 16000000.0,
                'market': None,
                'panking': '8',
                'percent': 1.6},
               {'address': '0x00000000000000000000b936df146397a6e4d837',
                'amount': 16000000.0,
                'market': None,
                'panking': '9',
                'percent': 1.6},
               {'address': '0x0000000000000000000006713099cafe3021ab07',
                'amount': 16000000.0,
                'market': None,
                'panking': '10',
                'percent': 1.6},
               {'address': '0x000000000000000000005599f6baafdd3e72f57e',
                'amount': 16000000.0,
                'market': None,
                'panking': '11',
                'percent': 1.6},
               {'address': '0x000000000000000000004399399acd6f8ee846e7',
                'amount': 15000000.0,
                'market': None,
                'panking': '12',
                'percent': 1.5},
               {'address': '0x00000000000000000000d82ca81df674be1ea918',
                'amount': 15000000.0,
                'market': None,
                'panking': '13',
                'percent': 1.5},
               {'address': '0x0000000000000000000011c37d742e7351e1b6ab',
                'amount': 15000000.0,
                'market': None,
                'panking': '14',
                'percent': 1.5},
               {'address': '0x00000000000000000000a116314021e6780f70f1',
                'amount': 14500000.0,
                'market': None,
                'panking': '15',
                'percent': 1.45},
               {'address': '0x00000000000000000000318d9867750a8909f7ff',
                'amount': 13500000.0,
                'market': None,
                'panking': '16',
                'percent': 1.35},
               {'address': '0x00000000000000000000cf1f718a15f0c689ce22',
                'amount': 13500000.0,
                'market': None,
                'panking': '17',
                'percent': 1.35},
               {'address': '0x000000000000000000008195523214b516aef9c3',
                'amount': 13468750.0,
                'market': None,
                'panking': '18',
                'percent': 1.3469},
               {'address': '0x00000000000000000000c7ef4632fd0cb2da09cd',
                'amount': 12500000.0,
                'market': None,
                'panking': '19',
                'percent': 1.25},
               {'address': '0x00000000000000000000a70f3edaa78876e91cff',
                'amount': 12000001.0,
                'market': None,
                'panking': '20',
                'percent': 1.2},
               {'address': '0x000000000000000000005ba3191ef557dd81a4f3',
                'amount': 10865544.672,
                'market': None,
                'panking': '21',
                'percent': 1.0866},
               {'address': '0x000000000000000000003e85ad86c61bc9cbab00',
                'amount': 10500000.0,
                'market': None,
                'panking': '22',
                'percent': 1.05},
               {'address': '0x0000000000000000000090a026d6a28cd030fed6',
                'amount': 10265881.2862,
                'market': None,
                'panking': '23',
                'percent': 1.0266},
               {'address': '0x0000000000000000000070eff469ab001f034a28',
                'amount': 10111419.5289,
                'market': None,
                'panking': '24',
                'percent': 1.0111},
               {'address': '0x00000000000000000000f5847b7b5f0378a6996a',
                'amount': 9254151.1,
                'market': None,
                'panking': '25',
                'percent': 0.9254},
               {'address': '0x00000000000000000000ac3c64ca4c2ffc4d206d',
                'amount': 9247708.0,
                'market': None,
                'panking': '26',
                'percent': 0.9248},
               {'address': '0x000000000000000000003107762b32a6d1d067ef',
                'amount': 8739678.0,
                'market': None,
                'panking': '27',
                'percent': 0.874},
               {'address': '0x00000000000000000000f1bfb21c820cf6c004c7',
                'amount': 8500000.0,
                'market': None,
                'panking': '28',
                'percent': 0.85},
               {'address': '0x00000000000000000000d5165820f09feaa944fd',
                'amount': 8323508.59,
                'market': None,
                'panking': '29',
                'percent': 0.8324},
               {'address': '0x0000000000000000000032133b952cbf0033c42c',
                'amount': 8000000.0,
                'market': None,
                'panking': '30',
                'percent': 0.8},
               {'address': '0x000000000000000000009707695d6a1d7e49e555',
                'amount': 8000000.0,
                'market': None,
                'panking': '31',
                'percent': 0.8},
               {'address': '0x00000000000000000000a1b2a16508b83402939f',
                'amount': 7936149.26486027,
                'market': None,
                'panking': '32',
                'percent': 0.7936},
               {'address': '0xf2b595c7327ef29b537e0981ef99d62257a73136',
                'amount': 7675000.0,
                'market': None,
                'panking': '33',
                'percent': 0.7675},
               {'address': '0x000000000000000000000dc9fc9ab9bc50595154',
                'amount': 7495000.0,
                'market': None,
                'panking': '34',
                'percent': 0.7495},
               {'address': '0x000000000000000000008846a88c7fb6b75c72a5',
                'amount': 7350963.23,
                'market': None,
                'panking': '35',
                'percent': 0.7351},
               {'address': '0x00000000000000000000fbab762d76d240e1acdf',
                'amount': 6594070.19249,
                'market': None,
                'panking': '36',
                'percent': 0.6594},
               {'address': '0xd850942ef8811f2a866692a623011bde52a462c1',
                'amount': 6274400.27478519,
                'market': 'VeChain',
                'panking': '37',
                'percent': 0.6274},
               {'address': '0x00000000000000000000fa391b5e72cf0b2af1f2',
                'amount': 5600000.0,
                'market': None,
                'panking': '38',
                'percent': 0.56},
               {'address': '0x00000000000000000000e850b4dfcf79590a869b',
                'amount': 5141888.524983456,
                'market': None,
                'panking': '39',
                'percent': 0.5142},
               {'address': '0x000000000000000000000393c60c902977ead179',
                'amount': 5000000.0,
                'market': None,
                'panking': '40',
                'percent': 0.5},
               {'address': '0x000000000000000000003957b5d510351d61373c',
                'amount': 4007664.0,
                'market': None,
                'panking': '41',
                'percent': 0.4008},
               {'address': '0x00000000000000000000720c41a1655d4e11c03e',
                'amount': 3699033.5812,
                'market': None,
                'panking': '42',
                'percent': 0.3699},
               {'address': '0x0000000000000000000010fea6b128e8ee4a3a34',
                'amount': 3641678.10443,
                'market': None,
                'panking': '43',
                'percent': 0.3642},
               {'address': '0x000000000000000000006cda2e4d88b68a942918',
                'amount': 3500000.0,
                'market': None,
                'panking': '44',
                'percent': 0.35},
               {'address': '0x0000000000000000000077f664b4f16e17ef68fe',
                'amount': 3214235.7230999996,
                'market': None,
                'panking': '45',
                'percent': 0.3214},
               {'address': '0xd94c9ff168dc6aebf9b6cc86deff54f3fb0afc33',
                'amount': 3018997.7538220626,
                'market': 'Yunbi_1',
                'panking': '46',
                'percent': 0.3019},
               {'address': '0x000000000000000000001e68731d4abcd49988d0',
                'amount': 2817182.1297,
                'market': None,
                'panking': '47',
                'percent': 0.2817},
               {'address': '0xddd033c3ea1013ba5e2efee6663181c8d64feb7d',
                'amount': 2392192.99,
                'market': None,
                'panking': '48',
                'percent': 0.2392},
               {'address': '0x00000000000000000000fa3e584d137080a3d81f',
                'amount': 2391901.91,
                'market': None,
                'panking': '49',
                'percent': 0.2392},
               {'address': '0x0000000000000000000097dc5ed89c9068d4c178',
                'amount': 2149036.77,
                'market': None,
                'panking': '50',
                'percent': 0.2149},
               {'address': '0x00000000000000000000937afcdbc58f6804d50c',
                'amount': 1973749.0,
                'market': None,
                'panking': '51',
                'percent': 0.1974},
               {'address': '0x00000000000000000000a63f013d051908824c37',
                'amount': 1801000.1710167,
                'market': None,
                'panking': '52',
                'percent': 0.1801},
               {'address': '0x0000000000000000000047249aba93b8aca77d9e',
                'amount': 1769906.4,
                'market': None,
                'panking': '53',
                'percent': 0.177},
               {'address': '0x0000000000000000000029bf85bcd25d1d4cea59',
                'amount': 1712182.7464,
                'market': None,
                'panking': '54',
                'percent': 0.1712},
               {'address': '0x00000000000000000000d08380fe274cf9aede9e',
                'amount': 1635498.98,
                'market': None,
                'panking': '55',
                'percent': 0.1635},
               {'address': '0x000000000000000000000c3c22193950db71628a',
                'amount': 1525000.0,
                'market': None,
                'panking': '56',
                'percent': 0.1525},
               {'address': '0x00000000000000000000f10bc75559089f135d68',
                'amount': 1500006.0,
                'market': None,
                'panking': '57',
                'percent': 0.15},
               {'address': '0x0000000000000000000068f0ebae23e9552619d7',
                'amount': 1495000.0,
                'market': None,
                'panking': '58',
                'percent': 0.1495},
               {'address': '0x000000000000000000002fdacbb1b6c53cc0c0f7',
                'amount': 1450000.0,
                'market': None,
                'panking': '59',
                'percent': 0.145},
               {'address': '0x0914c36c7197c952fff0664fcacb24403569218b',
                'amount': 1449999.99,
                'market': None,
                'panking': '60',
                'percent': 0.145},
               {'address': '0x00000000000000000000a9b7d6a5cd3899b90a5d',
                'amount': 1383378.59507234,
                'market': None,
                'panking': '61',
                'percent': 0.1383},
               {'address': '0x000000000000000000007c8d0a31cb14763aac14',
                'amount': 1363178.0793,
                'market': None,
                'panking': '62',
                'percent': 0.1363},
               {'address': '0x0000000000000000000093db71004296ec32cbfa',
                'amount': 1360000.01,
                'market': None,
                'panking': '63',
                'percent': 0.136},
               {'address': '0x000000000000000000000b563fdd1b62305218af',
                'amount': 1250000.0,
                'market': None,
                'panking': '64',
                'percent': 0.125},
               {'address': '0x2027ad28c11e650232ad6be516669a3b5f6595f7',
                'amount': 1215800.0,
                'market': None,
                'panking': '65',
                'percent': 0.1216},
               {'address': '0x000000000000000000005ac1b44b9459ac4bd256',
                'amount': 1039887.04901,
                'market': None,
                'panking': '66',
                'percent': 0.104},
               {'address': '0x00000000000000000000df45c4a369d657ef9e9c',
                'amount': 1034919.75456,
                'market': None,
                'panking': '67',
                'percent': 0.1035},
               {'address': '0x1062a747393198f70f71ec65a582423dba7e5ab3',
                'amount': 1029493.966168912,
                'market': 'Huobi_9',
                'panking': '68',
                'percent': 0.1029},
               {'address': '0x00000000000000000000c89e4bb396c90443fcaf',
                'amount': 1014542.07746,
                'market': None,
                'panking': '69',
                'percent': 0.1015},
               {'address': '0xf8e6e9ceac3828499dddf63ac91bbeb42a88e965',
                'amount': 1006250.0,
                'market': None,
                'panking': '70',
                'percent': 0.1006},
               {'address': '0x0017411bb487dd44dd5c4424225cc3b75bcc3f7a',
                'amount': 1000000.03,
                'market': None,
                'panking': '71',
                'percent': 0.1},
               {'address': '0x00000000000000000000634c0880e058edfdfee1',
                'amount': 1000000.0,
                'market': None,
                'panking': '72',
                'percent': 0.1},
               {'address': '0x0000000000000000000021c573722dc5ceda7f13',
                'amount': 1000000.0,
                'market': None,
                'panking': '73',
                'percent': 0.1},
               {'address': '0x00000000000000000000e6fd0b984b19458b13b1',
                'amount': 1000000.0,
                'market': None,
                'panking': '74',
                'percent': 0.1},
               {'address': '0x00000000000000000000f3f22e6ed7f5648901ed',
                'amount': 1000000.0,
                'market': None,
                'panking': '75',
                'percent': 0.1},
               {'address': '0x00000000000000000000b8ccd786ded530c198ad',
                'amount': 1000000.0,
                'market': None,
                'panking': '76',
                'percent': 0.1},
               {'address': '0x000000000000000000007c77572547634917687c',
                'amount': 1000000.0,
                'market': None,
                'panking': '77',
                'percent': 0.1},
               {'address': '0x000000000000000000007751430058c9235e358f',
                'amount': 935575.8185064493,
                'market': None,
                'panking': '78',
                'percent': 0.0936},
               {'address': '0xd4dcd2459bb78d7a645aa7e196857d421b10d93f',
                'amount': 922462.2570438418,
                'market': 'BigONE_1',
                'panking': '79',
                'percent': 0.0922},
               {'address': '0xa30d8157911ef23c46c0eb71889efe6a648a41f7',
                'amount': 859144.5064236297,
                'market': 'BigONE_2',
                'panking': '80',
                'percent': 0.0859},
               {'address': '0x2de79adcc8e2b791482f3932cfd7d3fe8ec998cb',
                'amount': 802120.31631309,
                'market': None,
                'panking': '81',
                'percent': 0.0802},
               {'address': '0x00000000000000000000421bde477eb2377f139a',
                'amount': 750000.0,
                'market': None,
                'panking': '82',
                'percent': 0.075},
               {'address': '0x000000000000000000005d6f7bb3ea147d4b18c4',
                'amount': 750000.0,
                'market': None,
                'panking': '83',
                'percent': 0.075},
               {'address': '0x000000000000000000002e8e0fa3515ad6a5e1c9',
                'amount': 698661.52018113,
                'market': None,
                'panking': '84',
                'percent': 0.0699},
               {'address': '0x00000000000000000000f44e7e5fc3fd4d7c48c0',
                'amount': 659400.805,
                'market': None,
                'panking': '85',
                'percent': 0.0659},
               {'address': '0x00000000000000000000885bec3b4e2f471818ff',
                'amount': 603300.0,
                'market': None,
                'panking': '86',
                'percent': 0.0603},
               {'address': '0x000000000000000000001b7775b5819a538f1242',
                'amount': 587996.24073,
                'market': None,
                'panking': '87',
                'percent': 0.0588},
               {'address': '0x0000000000000000000097df7d803806ddde0665',
                'amount': 572538.3076602853,
                'market': None,
                'panking': '88',
                'percent': 0.0573},
               {'address': '0x00000000000000000000d188f61210206dcf3daa',
                'amount': 501700.01,
                'market': None,
                'panking': '89',
                'percent': 0.0502},
               {'address': '0x00000000000000000000072882057c9329d39cd1',
                'amount': 500000.0,
                'market': None,
                'panking': '90',
                'percent': 0.05},
               {'address': '0x000000000000000000005af412b969c320bed4bf',
                'amount': 500000.0,
                'market': None,
                'panking': '91',
                'percent': 0.05},
               {'address': '0x00000000000000000000d2e8ba20023649983efc',
                'amount': 468001.32244,
                'market': None,
                'panking': '92',
                'percent': 0.0468},
               {'address': '0x36cdee18847731a1acdd815d889a61f98ea83770',
                'amount': 421369.54051,
                'market': None,
                'panking': '93',
                'percent': 0.0421},
               {'address': '0x00000000000000000000ca0d9bff547d32fb94ba',
                'amount': 372417.86554376053,
                'market': None,
                'panking': '94',
                'percent': 0.0372},
               {'address': '0x00000000000000000000acccb3faec317ddd1e06',
                'amount': 361165.0,
                'market': None,
                'panking': '95',
                'percent': 0.0361},
               {'address': '0x000000000000000000007c0c9c7b725dabb98bd3',
                'amount': 351055.16,
                'market': None,
                'panking': '96',
                'percent': 0.0351},
               {'address': '0xcbd96eda941676c96e3a02fa2a4154237df2fe47',
                'amount': 345205.0,
                'market': None,
                'panking': '97',
                'percent': 0.0345},
               {'address': '0x00000000000000000000094dab79d408a927369f',
                'amount': 329448.736613,
                'market': None,
                'panking': '98',
                'percent': 0.0329},
               {'address': '0x000000000000000000008560e01e750dbfcabd30',
                'amount': 317250.1562,
                'market': None,
                'panking': '99',
                'percent': 0.0317},
               {'address': '0x0000000000000000000063aabab84b7dd1e59760',
                'amount': 306400.0,
                'market': None,
                'panking': '100',
                'percent': 0.0306}],
 'time_now': '2018-10-08 13:34:21',
 'token': '0xd850942ef8811f2a866692a623011bde52a462c1',
 'top_list': [55.8023, 69.84920000000001, 89.02, 93.81219999999999]}
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_address.py", line 23, in process_item
    if not spider.redis.sismember('token_address', token):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 13:34:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 13:34:25 [token_address] INFO: token_address关闭mysql数据库连接
2018-10-08 13:34:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 359,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 44070,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 5, 34, 25, 539602),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 8, 5, 34, 15, 437025)}
2018-10-08 13:34:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 13:36:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 13:36:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 13:36:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 13:36:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 13:36:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 13:36:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 13:36:42 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 13:36:42 [scrapy.core.engine] INFO: Spider opened
2018-10-08 13:36:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 13:36:42 [token_address] INFO: Spider opened: token_address
2018-10-08 13:36:43 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-08 13:36:43 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A3CF8>
2018-10-08 13:36:45 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 13:36:45 [token_address] INFO: token_address关闭mysql数据库连接
2018-10-08 13:36:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 360,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43964,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 5, 36, 45, 136587),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 8, 5, 36, 42, 104413)}
2018-10-08 13:36:45 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 13:38:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 13:38:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 13:38:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 13:38:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 13:38:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 13:38:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 13:38:30 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 13:38:30 [scrapy.core.engine] INFO: Spider opened
2018-10-08 13:38:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 13:38:30 [token_address] INFO: Spider opened: token_address
2018-10-08 13:38:31 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-08 13:38:31 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A3C88>
2018-10-08 13:38:33 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 13:38:33 [token_address] INFO: token_address关闭mysql数据库连接
2018-10-08 13:38:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 334,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43971,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 5, 38, 33, 726798),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 8, 5, 38, 30, 740627)}
2018-10-08 13:38:33 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 13:40:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 13:40:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 13:40:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 13:40:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 13:40:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 13:40:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 13:40:18 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 13:40:18 [scrapy.core.engine] INFO: Spider opened
2018-10-08 13:40:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 13:40:18 [token_address] INFO: Spider opened: token_address
2018-10-08 13:40:19 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-08 13:40:19 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B3CC0>
2018-10-08 13:40:22 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 13:40:22 [token_address] INFO: token_address关闭mysql数据库连接
2018-10-08 13:40:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 324,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43983,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 5, 40, 22, 61994),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 8, 5, 40, 18, 654799)}
2018-10-08 13:40:22 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 13:42:12 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 13:42:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 13:42:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 13:42:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 13:42:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 13:42:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 13:42:13 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 13:42:13 [scrapy.core.engine] INFO: Spider opened
2018-10-08 13:42:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 13:42:13 [token_address] INFO: Spider opened: token_address
2018-10-08 13:42:14 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-08 13:42:14 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2D30>
2018-10-08 13:42:16 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 13:42:16 [token_address] INFO: token_address关闭mysql数据库连接
2018-10-08 13:42:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 347,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43964,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 5, 42, 16, 838559),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 8, 5, 42, 13, 725381)}
2018-10-08 13:42:16 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 13:45:16 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 13:45:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 13:45:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 13:45:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 13:45:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 13:45:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 13:45:16 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 13:45:16 [scrapy.core.engine] INFO: Spider opened
2018-10-08 13:45:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 13:45:16 [token_address] INFO: Spider opened: token_address
2018-10-08 13:45:17 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-08 13:45:18 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2CF8>
2018-10-08 13:45:19 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 13:45:19 [token_address] INFO: token_address关闭mysql数据库连接
2018-10-08 13:45:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 335,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43948,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 5, 45, 19, 936032),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 8, 5, 45, 16, 988863)}
2018-10-08 13:45:19 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 14:02:42 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 14:02:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 14:02:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 14:02:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 14:02:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 14:02:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 14:02:43 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 14:02:43 [scrapy.core.engine] INFO: Spider opened
2018-10-08 14:02:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 14:02:43 [token_address] INFO: Spider opened: token_address
2018-10-08 14:02:44 [token_address] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-08 14:02:44 [token_address] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B2CF8>
2018-10-08 14:02:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 14:02:46 [token_address] INFO: token_address关闭mysql数据库连接
2018-10-08 14:02:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 334,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43947,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 6, 2, 46, 723905),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 8, 6, 2, 43, 558724)}
2018-10-08 14:02:46 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 17:10:10 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 17:10:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 17:10:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 17:10:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 17:10:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 17:10:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 17:10:11 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 17:10:11 [scrapy.core.engine] INFO: Spider opened
2018-10-08 17:10:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 17:10:11 [weibo_mblog] INFO: Spider opened: weibo_mblog
2018-10-08 17:10:12 [weibo_mblog] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-08 17:10:12 [weibo_mblog] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B3D30>
2018-10-08 17:10:49 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 17:10:49 [weibo_mblog] INFO: weibo_mblog关闭mysql数据库连接
2018-10-08 17:10:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 28861,
 'downloader/request_count': 60,
 'downloader/request_method_count/GET': 60,
 'downloader/response_bytes': 316885,
 'downloader/response_count': 60,
 'downloader/response_status_count/200': 60,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 9, 10, 49, 535244),
 'item_scraped_count': 44,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 60,
 'scheduler/dequeued': 60,
 'scheduler/dequeued/memory': 60,
 'scheduler/enqueued': 60,
 'scheduler/enqueued/memory': 60,
 'start_time': datetime.datetime(2018, 10, 8, 9, 10, 11, 963095)}
2018-10-08 17:10:49 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 17:12:36 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 17:12:36 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 17:12:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 17:12:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 17:12:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 17:12:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 17:12:37 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 17:12:37 [scrapy.core.engine] INFO: Spider opened
2018-10-08 17:12:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 17:12:37 [weibo_mblog] INFO: Spider opened: weibo_mblog
2018-10-08 17:12:38 [weibo_mblog] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-08 17:12:38 [weibo_mblog] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A1D30>
2018-10-08 17:12:40 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 17:12:40 [weibo_mblog] INFO: weibo_mblog关闭mysql数据库连接
2018-10-08 17:12:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1410,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 36557,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 9, 12, 40, 832610),
 'log_count/INFO': 11,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'start_time': datetime.datetime(2018, 10, 8, 9, 12, 37, 44394)}
2018-10-08 17:12:40 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 17:13:23 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 17:13:23 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 17:13:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 17:13:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 17:13:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 17:13:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 17:13:24 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 17:13:24 [scrapy.core.engine] INFO: Spider opened
2018-10-08 17:13:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 17:13:24 [weibo_mblog] INFO: Spider opened: weibo_mblog
2018-10-08 17:13:28 [weibo_mblog] WARNING: Redis服务连接失败
2018-10-08 17:13:28 [weibo_mblog] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2C18>
2018-10-08 17:13:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=3908645609&containerid=1076033908645609&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:13:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=5941645212&containerid=1076035941645212&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:13:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=1305473650&containerid=1076031305473650&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:13:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=6311913111&containerid=1076036311913111&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:13:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=6286321083&containerid=1076036286321083&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:13:50 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 17:13:50 [weibo_mblog] INFO: weibo_mblog关闭mysql数据库连接
2018-10-08 17:13:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1410,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 36566,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 9, 13, 50, 742609),
 'log_count/ERROR': 5,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/ConnectionError': 5,
 'start_time': datetime.datetime(2018, 10, 8, 9, 13, 24, 823126)}
2018-10-08 17:13:50 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 17:15:03 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 17:15:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 17:15:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 17:15:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 17:15:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 17:15:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 17:15:04 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 17:15:04 [scrapy.core.engine] INFO: Spider opened
2018-10-08 17:15:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 17:15:04 [weibo_mblog] INFO: Spider opened: weibo_mblog
2018-10-08 17:15:08 [weibo_mblog] WARNING: Redis服务连接失败
2018-10-08 17:15:08 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TestScrapySpiderMiddleware.spider_opened of <test_scrapy.middlewares.TestScrapySpiderMiddleware object at 0x00000000054D8C18>>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 66, in spider_opened
    spider.redis.ping()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 777, in ping
    return self.execute_command('PING')
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:15:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=3908645609&containerid=1076033908645609&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:15:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=5941645212&containerid=1076035941645212&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:15:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=1305473650&containerid=1076031305473650&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:15:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=6311913111&containerid=1076036311913111&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:15:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=6286321083&containerid=1076036286321083&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:15:29 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 17:15:29 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'WeiboSpider' object has no attribute 'connect'
2018-10-08 17:15:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1410,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 35458,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 9, 15, 29, 820276),
 'log_count/ERROR': 7,
 'log_count/INFO': 8,
 'log_count/WARNING': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/ConnectionError': 5,
 'start_time': datetime.datetime(2018, 10, 8, 9, 15, 4, 518829)}
2018-10-08 17:15:29 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-08 17:29:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-08 17:29:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-08 17:29:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-08 17:29:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-08 17:29:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-08 17:29:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-08 17:29:18 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-08 17:29:18 [scrapy.core.engine] INFO: Spider opened
2018-10-08 17:29:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-08 17:29:18 [weibo_mblog] INFO: Spider opened: weibo_mblog
2018-10-08 17:29:22 [weibo_mblog] WARNING: Redis服务连接失败
2018-10-08 17:29:22 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TestScrapySpiderMiddleware.spider_opened of <test_scrapy.middlewares.TestScrapySpiderMiddleware object at 0x00000000054D7C50>>
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 66, in spider_opened
    spider.redis.ping()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 777, in ping
    return self.execute_command('PING')
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pydispatch\robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 71, in spider_opened
    raise CloseSpider()
scrapy.exceptions.CloseSpider
2018-10-08 17:29:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=3908645609&containerid=1076033908645609&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:29:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=5941645212&containerid=1076035941645212&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:29:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=1305473650&containerid=1076031305473650&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:29:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=6311913111&containerid=1076036311913111&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:29:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://m.weibo.cn/api/container/getIndex?uid=6286321083&containerid=1076036286321083&page=1> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\attention\weibo_mblog.py", line 48, in parse
    if self.redis.sismember('mblog_urls', request_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-08 17:29:43 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-08 17:29:43 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_common.py", line 12, in close_spider
    spider.connect.close()
AttributeError: 'WeiboSpider' object has no attribute 'connect'
2018-10-08 17:29:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1410,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 35862,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 8, 9, 29, 43, 898126),
 'log_count/ERROR': 7,
 'log_count/INFO': 8,
 'log_count/WARNING': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/ConnectionError': 5,
 'start_time': datetime.datetime(2018, 10, 8, 9, 29, 18, 652682)}
2018-10-08 17:29:43 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-10 17:31:42 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-10 17:31:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-10 17:31:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-10 17:31:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-10 17:31:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-10 17:31:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-10 17:31:43 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-10 17:31:43 [scrapy.core.engine] INFO: Spider opened
2018-10-10 17:31:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-10 17:31:43 [token_transaction] INFO: Spider opened: token_transaction
2018-10-10 17:31:47 [token_transaction] WARNING: Redis服务连接失败
2018-10-10 17:31:47 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A1F60>
2018-10-10 17:31:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-10 17:31:48 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-10 17:31:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 213,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1799,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 10, 9, 31, 48, 584170),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 10, 9, 31, 43, 652562)}
2018-10-10 17:31:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-10 17:33:03 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-10 17:33:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-10 17:33:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-10 17:33:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-10 17:33:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-10 17:33:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-10 17:33:03 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-10 17:33:03 [scrapy.core.engine] INFO: Spider opened
2018-10-10 17:33:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-10 17:33:03 [token_transaction] INFO: Spider opened: token_transaction
2018-10-10 17:33:07 [token_transaction] WARNING: Redis服务连接失败
2018-10-10 17:33:07 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A3E10>
2018-10-10 17:33:09 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-10 17:33:09 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-10 17:33:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 291,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14610,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 10, 9, 33, 9, 74121),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 10, 9, 33, 3, 861711)}
2018-10-10 17:33:09 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 09:54:00 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 09:54:00 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 09:54:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 09:54:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 09:54:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 09:54:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 09:54:01 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 09:54:01 [scrapy.core.engine] INFO: Spider opened
2018-10-11 09:54:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 09:54:01 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 09:54:05 [token_transaction] WARNING: Redis服务连接失败
2018-10-11 09:54:05 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A2EF0>
2018-10-11 09:54:07 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 09:54:07 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 09:54:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 245,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14766,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 1, 54, 7, 212464),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 1, 54, 1, 631145)}
2018-10-11 09:54:07 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 09:59:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 09:59:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 09:59:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 09:59:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 09:59:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 09:59:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 09:59:18 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 09:59:18 [scrapy.core.engine] INFO: Spider opened
2018-10-11 09:59:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 09:59:18 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 09:59:22 [token_transaction] WARNING: Redis服务连接失败
2018-10-11 09:59:22 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005883DD8>
2018-10-11 09:59:23 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 09:59:23 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 09:59:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 222,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14320,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 1, 59, 23, 801692),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 1, 59, 18, 415881)}
2018-10-11 09:59:23 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 09:59:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 09:59:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 09:59:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 09:59:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 09:59:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 09:59:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 09:59:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 09:59:29 [scrapy.core.engine] INFO: Spider opened
2018-10-11 09:59:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 09:59:29 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 09:59:33 [token_transaction] WARNING: Redis服务连接失败
2018-10-11 09:59:33 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057F3E10>
2018-10-11 09:59:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 09:59:34 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 09:59:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 210,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1799,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 1, 59, 34, 752713),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 1, 59, 29, 610304)}
2018-10-11 09:59:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 09:59:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 09:59:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 09:59:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 09:59:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 09:59:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 09:59:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 09:59:42 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 09:59:42 [scrapy.core.engine] INFO: Spider opened
2018-10-11 09:59:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 09:59:42 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 09:59:46 [token_transaction] WARNING: Redis服务连接失败
2018-10-11 09:59:46 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D3E10>
2018-10-11 09:59:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 09:59:47 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 09:59:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 245,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14405,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 1, 59, 47, 358539),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 1, 59, 42, 26929)}
2018-10-11 09:59:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 10:01:09 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 10:01:09 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 10:01:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 10:01:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 10:01:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 10:01:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 10:01:10 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 10:01:10 [scrapy.core.engine] INFO: Spider opened
2018-10-11 10:01:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 10:01:10 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 10:01:14 [token_transaction] WARNING: Redis服务连接失败
2018-10-11 10:01:14 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057E3E10>
2018-10-11 10:01:16 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 10:01:16 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 10:01:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 253,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14571,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 2, 1, 16, 236731),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 2, 1, 10, 794921)}
2018-10-11 10:01:16 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 10:01:51 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 10:01:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 10:01:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 10:01:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 10:01:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 10:01:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 10:01:52 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 10:01:52 [scrapy.core.engine] INFO: Spider opened
2018-10-11 10:01:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 10:01:52 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 10:01:56 [token_transaction] WARNING: Redis服务连接失败
2018-10-11 10:01:56 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058A3DD8>
2018-10-11 10:01:57 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 10:01:57 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 10:01:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 291,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14908,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 2, 1, 57, 475612),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 2, 1, 52, 132601)}
2018-10-11 10:01:57 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 10:15:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 10:15:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 10:15:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 10:15:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 10:15:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 10:15:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 10:15:45 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 10:15:45 [scrapy.core.engine] INFO: Spider opened
2018-10-11 10:15:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 10:15:45 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 10:15:49 [token_transaction] WARNING: Redis服务连接失败
2018-10-11 10:15:49 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005813E48>
2018-10-11 10:15:50 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 10:15:50 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 10:15:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 260,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14768,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 2, 15, 50, 910118),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 2, 15, 45, 512509)}
2018-10-11 10:15:50 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 10:49:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 10:49:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 10:49:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 10:49:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 10:49:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 10:49:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 10:49:56 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 10:49:56 [scrapy.core.engine] INFO: Spider opened
2018-10-11 10:49:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 10:49:56 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 10:49:57 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 10:49:57 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058D4F60>
2018-10-11 10:49:58 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 10:49:58 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 10:49:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 248,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15382,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 2, 49, 58, 848781),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 2, 49, 56, 680657)}
2018-10-11 10:49:58 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 10:52:19 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 10:52:19 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 10:52:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 10:52:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 10:52:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 10:52:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 10:52:20 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 10:52:20 [scrapy.core.engine] INFO: Spider opened
2018-10-11 10:52:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 10:52:20 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 10:52:21 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 10:52:21 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005854F28>
2018-10-11 10:52:22 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 10:52:22 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 10:52:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 304,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15359,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 2, 52, 22, 696677),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 2, 52, 20, 637559)}
2018-10-11 10:52:22 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 10:52:36 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 10:52:36 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 10:52:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 10:52:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 10:52:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 10:52:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 10:52:37 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 10:52:37 [scrapy.core.engine] INFO: Spider opened
2018-10-11 10:52:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 10:52:37 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 10:52:38 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 10:52:38 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B3F28>
2018-10-11 10:52:38 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 10:52:38 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 10:52:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 213,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1798,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 2, 52, 38, 985608),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 2, 52, 37, 166504)}
2018-10-11 10:52:38 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 10:52:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 10:52:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 10:52:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 10:52:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 10:52:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 10:52:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 10:52:44 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 10:52:44 [scrapy.core.engine] INFO: Spider opened
2018-10-11 10:52:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 10:52:44 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 10:52:45 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 10:52:45 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005824F60>
2018-10-11 10:52:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 10:52:46 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 10:52:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 237,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13502,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 2, 52, 46, 120016),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 2, 52, 44, 93901)}
2018-10-11 10:52:46 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 10:52:57 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 10:52:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 10:52:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 10:52:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 10:52:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 10:52:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 10:52:58 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 10:52:58 [scrapy.core.engine] INFO: Spider opened
2018-10-11 10:52:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 10:52:58 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 10:52:59 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 10:52:59 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058A4EB8>
2018-10-11 10:53:00 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 10:53:00 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 10:53:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 249,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 12950,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 2, 53, 0, 239824),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 2, 52, 58, 231709)}
2018-10-11 10:53:00 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 11:53:58 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 11:53:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 11:53:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 11:53:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 11:53:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 11:53:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 11:53:59 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 11:53:59 [scrapy.core.engine] INFO: Spider opened
2018-10-11 11:53:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 11:53:59 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 11:54:00 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 11:54:00 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A4F60>
2018-10-11 11:54:01 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 11:54:01 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 11:54:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 278,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11948,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 3, 54, 1, 454828),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 3, 53, 59, 463715)}
2018-10-11 11:54:01 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 11:54:49 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 11:54:49 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 11:54:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 11:54:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 11:54:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 11:54:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 11:54:50 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 11:54:50 [scrapy.core.engine] INFO: Spider opened
2018-10-11 11:54:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 11:54:50 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 11:54:51 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 11:54:51 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057A4F98>
2018-10-11 11:54:52 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 11:54:52 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 11:54:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 279,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13681,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 3, 54, 52, 542350),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 3, 54, 50, 528234)}
2018-10-11 11:54:52 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 11:55:06 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 11:55:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 11:55:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 11:55:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 11:55:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 11:55:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 11:55:07 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 11:55:07 [scrapy.core.engine] INFO: Spider opened
2018-10-11 11:55:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 11:55:07 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 11:55:08 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 11:55:08 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005845F98>
2018-10-11 11:55:09 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 11:55:09 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 11:55:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 260,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 12092,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 3, 55, 9, 220303),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 3, 55, 7, 262191)}
2018-10-11 11:55:09 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 11:55:22 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 11:55:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 11:55:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 11:55:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 11:55:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 11:55:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 11:55:23 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 11:55:23 [scrapy.core.engine] INFO: Spider opened
2018-10-11 11:55:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 11:55:23 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 11:55:24 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 11:55:24 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005854FD0>
2018-10-11 11:55:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 11:55:25 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 11:55:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 299,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 12131,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 3, 55, 25, 347226),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 3, 55, 23, 363112)}
2018-10-11 11:55:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 11:55:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 11:55:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 11:55:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 11:55:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 11:55:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 11:55:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 11:55:45 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 11:55:45 [scrapy.core.engine] INFO: Spider opened
2018-10-11 11:55:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 11:55:45 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 11:55:46 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 11:55:46 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6F28>
2018-10-11 11:55:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 11:55:47 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 11:55:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 284,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 12228,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 3, 55, 47, 67468),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 3, 55, 45, 133358)}
2018-10-11 11:55:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 11:56:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 11:56:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 11:56:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 11:56:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 11:56:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 11:56:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 11:56:05 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 11:56:05 [scrapy.core.engine] INFO: Spider opened
2018-10-11 11:56:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 11:56:05 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 11:56:06 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 11:56:06 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005876F60>
2018-10-11 11:56:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x1776e1f26f98b1a5df9cd347953a26dd3cb46671> (referer: https://etherscan.io/tokentxns)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\token_transaction.py", line 99, in token_price
    td = tr[2].css('td')[1]
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-11 11:56:07 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 11:56:07 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 11:56:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1143,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 25819,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 3, 56, 7, 990665),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2018, 10, 11, 3, 56, 5, 181504)}
2018-10-11 11:56:07 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 12:01:08 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 12:01:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 12:01:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 12:01:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 12:01:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 12:01:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 12:01:09 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 12:01:09 [scrapy.core.engine] INFO: Spider opened
2018-10-11 12:01:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 12:01:09 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 12:01:10 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 12:01:10 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057E5F98>
2018-10-11 12:01:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 12:01:11 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 12:01:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 299,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11900,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 4, 1, 11, 473622),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 4, 1, 9, 306498)}
2018-10-11 12:01:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 12:01:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 12:01:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 12:01:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 12:01:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 12:01:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 12:01:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 12:01:18 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 12:01:18 [scrapy.core.engine] INFO: Spider opened
2018-10-11 12:01:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 12:01:18 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 12:01:19 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 12:01:19 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005855F28>
2018-10-11 12:01:20 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 12:01:20 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 12:01:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 292,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11903,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 4, 1, 20, 176120),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 4, 1, 18, 79000)}
2018-10-11 12:01:20 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 12:01:24 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 12:01:24 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 12:01:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 12:01:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 12:01:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 12:01:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 12:01:25 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 12:01:25 [scrapy.core.engine] INFO: Spider opened
2018-10-11 12:01:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 12:01:25 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 12:01:26 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 12:01:26 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005895FD0>
2018-10-11 12:01:27 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 12:01:27 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 12:01:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 279,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11889,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 4, 1, 27, 763554),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 4, 1, 25, 760439)}
2018-10-11 12:01:27 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 12:02:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 12:02:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 12:02:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 12:02:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 12:02:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 12:02:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 12:02:05 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 12:02:05 [scrapy.core.engine] INFO: Spider opened
2018-10-11 12:02:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 12:02:05 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 12:02:06 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 12:02:06 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B5F98>
2018-10-11 12:02:07 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 12:02:07 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 12:02:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 222,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14749,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 4, 2, 7, 630834),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 4, 2, 5, 552715)}
2018-10-11 12:02:07 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 12:52:09 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 12:52:09 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 12:52:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 12:52:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 12:52:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 12:52:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 12:52:10 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 12:52:10 [scrapy.core.engine] INFO: Spider opened
2018-10-11 12:52:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 12:52:10 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 12:52:11 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 12:52:11 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B3F98>
2018-10-11 12:52:13 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 12:52:13 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 12:52:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 805,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 26133,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 4, 52, 13, 804242),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 11, 4, 52, 10, 663062)}
2018-10-11 12:52:13 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 13:38:38 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 13:38:38 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 13:38:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 13:38:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 13:38:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 13:38:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 13:38:39 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 13:38:39 [scrapy.core.engine] INFO: Spider opened
2018-10-11 13:38:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 13:38:39 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 13:38:40 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 13:38:40 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D3F98>
2018-10-11 13:38:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://etherscan.io/token/0x905e337c6c8645263d3521205aa37bf4d034e745> (referer: https://etherscan.io/tokentxns)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\token_address\token_transaction.py", line 104, in token_price
    usd_price = float(price.split('@')[0][1:])
ValueError: could not convert string to float: '$0.0325 '
2018-10-11 13:38:42 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 13:38:42 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 13:38:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 807,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 26175,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 5, 38, 42, 777357),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 10, 11, 5, 38, 39, 576174)}
2018-10-11 13:38:42 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 13:40:27 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 13:40:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 13:40:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 13:40:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 13:40:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 13:40:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 13:40:28 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 13:40:28 [scrapy.core.engine] INFO: Spider opened
2018-10-11 13:40:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 13:40:28 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 13:40:29 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 13:40:29 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005883F98>
2018-10-11 13:40:31 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 13:40:31 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 13:40:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 754,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 26348,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 5, 40, 31, 683586),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 11, 5, 40, 28, 550407)}
2018-10-11 13:40:31 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:10:19 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:10:19 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:10:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:10:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:10:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:10:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:10:20 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:10:20 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:10:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:10:20 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:10:21 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:10:21 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058D7320>
2018-10-11 15:10:22 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:10:22 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:10:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 237,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13208,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 10, 22, 995725),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 10, 20, 901321)}
2018-10-11 15:10:22 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:10:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:10:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:10:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:10:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:10:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:10:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:10:42 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:10:42 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:10:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:10:42 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:10:43 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:10:43 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058642E8>
2018-10-11 15:10:44 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:10:44 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:10:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 299,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13281,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 10, 44, 531763),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 10, 42, 347759)}
2018-10-11 15:10:44 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:11:00 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:11:00 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:11:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:11:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:11:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:11:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:11:01 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:11:01 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:11:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:11:01 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:11:02 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:11:02 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6278>
2018-10-11 15:11:03 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:11:03 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:11:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 301,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15318,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 11, 3, 411598),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 11, 1, 321194)}
2018-10-11 15:11:03 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:11:22 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:11:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:11:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:11:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:11:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:11:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:11:23 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:11:23 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:11:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:11:23 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:11:24 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:11:24 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D4320>
2018-10-11 15:11:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:11:25 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:11:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 256,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15306,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 11, 25, 335037),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 11, 23, 201433)}
2018-10-11 15:11:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:11:30 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:11:30 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:11:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:11:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:11:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:11:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:11:31 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:11:31 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:11:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:11:31 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:11:32 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:11:32 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B5320>
2018-10-11 15:11:33 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:11:33 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:11:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 291,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14050,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 11, 33, 962852),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 11, 31, 716448)}
2018-10-11 15:11:33 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:11:37 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:11:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:11:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:11:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:11:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:11:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:11:38 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:11:38 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:11:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:11:38 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:11:39 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:11:39 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005815320>
2018-10-11 15:11:40 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:11:40 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:11:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 222,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13976,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 11, 40, 797664),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 11, 38, 690660)}
2018-10-11 15:11:40 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:11:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:11:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:11:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:11:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:11:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:11:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:11:45 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:11:45 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:11:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:11:45 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:11:46 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:11:46 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058542E8>
2018-10-11 15:11:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:11:47 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:11:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 301,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13945,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 11, 47, 864476),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 11, 45, 805273)}
2018-10-11 15:11:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:12:07 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:12:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:12:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:12:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:12:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:12:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:12:08 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:12:08 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:12:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:12:08 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:12:09 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:12:09 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C52E8>
2018-10-11 15:12:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:12:11 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:12:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 797,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 25204,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 12, 11, 405813),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 11, 7, 12, 8, 679657)}
2018-10-11 15:12:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:14:27 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:14:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:14:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:14:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:14:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:14:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:14:28 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:14:28 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:14:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:14:28 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:14:29 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:14:29 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005915320>
2018-10-11 15:14:30 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:14:30 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:14:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 303,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 12632,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 14, 30, 544969),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 14, 28, 500852)}
2018-10-11 15:14:30 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:15:21 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:15:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:15:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:15:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:15:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:15:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:15:22 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:15:22 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:15:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:15:22 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:15:23 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:15:23 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000584A2B0>
2018-10-11 15:15:24 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:15:24 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:15:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 301,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13456,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 15, 24, 331644),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 15, 22, 254526)}
2018-10-11 15:15:24 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:15:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:15:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:15:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:15:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:15:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:15:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:15:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:15:29 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:15:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:15:29 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:15:30 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:15:30 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000059552E8>
2018-10-11 15:15:31 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:15:31 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:15:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 249,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15216,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 15, 31, 512055),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 15, 29, 470938)}
2018-10-11 15:15:31 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:15:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:15:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:15:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:15:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:15:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:15:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:15:36 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:15:36 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:15:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:15:36 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:15:37 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:15:37 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005815320>
2018-10-11 15:15:38 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:15:38 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:15:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 291,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15186,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 15, 38, 798472),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 15, 36, 580345)}
2018-10-11 15:15:38 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:15:42 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:15:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:15:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:15:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:15:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:15:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:15:43 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:15:43 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:15:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:15:43 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:15:44 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:15:44 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058A42E8>
2018-10-11 15:15:45 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:15:45 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:15:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 301,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15222,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 15, 45, 915879),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 15, 43, 874762)}
2018-10-11 15:15:45 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:15:52 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:15:52 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:15:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:15:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:15:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:15:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:15:52 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:15:52 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:15:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:15:52 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:15:53 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:15:53 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058A52E8>
2018-10-11 15:15:55 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:15:55 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:15:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 301,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15224,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 15, 55, 59001),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 15, 52, 981283)}
2018-10-11 15:15:55 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:15:59 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:15:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:15:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:15:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:15:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:15:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:16:00 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:16:00 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:16:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:16:00 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:16:01 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:16:01 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005826320>
2018-10-11 15:16:02 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:16:02 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:16:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 303,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15220,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 16, 2, 303415),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 16, 0, 84288)}
2018-10-11 15:16:02 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:16:07 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:16:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:16:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:16:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:16:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:16:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:16:08 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:16:08 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:16:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:16:08 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:16:09 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:16:09 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005895320>
2018-10-11 15:16:10 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:16:10 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:16:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 249,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15215,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 16, 10, 563888),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 16, 8, 456767)}
2018-10-11 15:16:10 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:16:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:16:14 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:16:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:16:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:16:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:16:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:16:15 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:16:15 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:16:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:16:15 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:16:16 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:16:16 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058652E8>
2018-10-11 15:16:17 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:16:17 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:16:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 260,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 15183,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 16, 17, 983312),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 16, 15, 895193)}
2018-10-11 15:16:17 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:20:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:20:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:20:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:20:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:20:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:20:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:20:44 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:20:44 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:20:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:20:44 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:20:45 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:20:45 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C6320>
2018-10-11 15:20:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:20:46 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:20:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 256,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14554,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 20, 46, 241255),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 20, 44, 47129)}
2018-10-11 15:20:46 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:20:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:20:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:20:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:20:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:20:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:20:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:20:51 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:20:51 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:20:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:20:51 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:20:52 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:20:52 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058D5320>
2018-10-11 15:20:53 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:20:53 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:20:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 292,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14525,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 20, 53, 866691),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 20, 51, 502556)}
2018-10-11 15:20:53 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:21:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:21:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:21:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:21:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:21:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:21:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:21:56 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:21:56 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:21:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:21:56 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:21:57 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:21:57 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005945320>
2018-10-11 15:21:58 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:21:58 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:21:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 222,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13817,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 21, 58, 829407),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 21, 56, 747287)}
2018-10-11 15:21:58 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:22:03 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:22:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:22:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:22:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:22:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:22:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:22:03 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:22:03 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:22:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:22:03 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:22:04 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:22:04 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005874320>
2018-10-11 15:22:05 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:22:05 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:22:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 265,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13827,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 22, 5, 969815),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 22, 3, 917698)}
2018-10-11 15:22:05 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:22:10 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:22:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:22:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:22:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:22:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:22:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:22:11 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:22:11 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:22:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:22:11 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:22:12 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:22:12 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C42E8>
2018-10-11 15:22:13 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:22:13 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:22:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 751,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 26507,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 22, 13, 715258),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 11, 7, 22, 11, 13103)}
2018-10-11 15:22:13 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:26:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:26:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:26:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:26:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:26:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:26:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:26:36 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:26:36 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:26:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:26:36 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:26:37 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:26:37 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057CA2B0>
2018-10-11 15:26:39 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:26:39 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:26:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 810,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 24462,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 26, 39, 162255),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 11, 7, 26, 36, 449100)}
2018-10-11 15:26:39 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:27:12 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:27:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:27:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:27:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:27:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:27:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:27:13 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:27:13 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:27:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:27:13 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:27:14 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:27:14 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058162E8>
2018-10-11 15:27:15 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:27:15 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:27:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 291,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14319,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 27, 15, 663541),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 27, 13, 445414)}
2018-10-11 15:27:15 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:27:21 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:27:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:27:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:27:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:27:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:27:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:27:22 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:27:22 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:27:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:27:22 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:27:23 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:27:23 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058752E8>
2018-10-11 15:27:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:27:25 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:27:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 253,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14288,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 27, 25, 348095),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 27, 22, 865953)}
2018-10-11 15:27:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:27:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:27:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:27:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:27:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:27:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:27:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:27:30 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:27:30 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:27:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:27:30 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:27:31 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:27:31 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058A6278>
2018-10-11 15:27:32 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:27:32 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:27:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 284,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14186,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 27, 32, 901527),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 27, 30, 775405)}
2018-10-11 15:27:32 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:27:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:27:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:27:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:27:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:27:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:27:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:27:41 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:27:41 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:27:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:27:41 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:27:42 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:27:42 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C52E8>
2018-10-11 15:27:44 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:27:44 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:27:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 292,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 13550,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 27, 44, 738204),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 11, 7, 27, 41, 528020)}
2018-10-11 15:27:44 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-11 15:27:48 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-11 15:27:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-11 15:27:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-11 15:27:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-11 15:27:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-11 15:27:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-11 15:27:49 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-11 15:27:49 [scrapy.core.engine] INFO: Spider opened
2018-10-11 15:27:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-11 15:27:49 [token_transaction] INFO: Spider opened: token_transaction
2018-10-11 15:27:50 [token_transaction] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-11 15:27:50 [token_transaction] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C52E8>
2018-10-11 15:28:02 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-11 15:28:02 [token_transaction] INFO: token_transaction关闭mysql数据库连接
2018-10-11 15:28:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 760,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 26252,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 11, 7, 28, 2, 904243),
 'item_scraped_count': 1,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 11, 7, 27, 49, 899499)}
2018-10-11 15:28:02 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 11:05:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 11:05:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 11:05:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 11:05:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 11:05:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 11:05:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 11:05:12 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 11:05:12 [scrapy.core.engine] INFO: Spider opened
2018-10-12 11:05:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 11:05:12 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 11:05:16 [btc_policy_new] WARNING: Redis服务连接失败
2018-10-12 11:05:16 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B42E8>
2018-10-12 11:05:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://app.blockmeta.com/w1/news/list?num=20&page=1&cat_id=572> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\news\btc_policy_new.py", line 54, in parse
    if self.redis.sismember('news_urls', origin_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-12 11:05:21 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 11:05:21 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 11:05:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 311,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 8680,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 3, 5, 21, 34450),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ConnectionError': 1,
 'start_time': datetime.datetime(2018, 10, 12, 3, 5, 12, 601968)}
2018-10-12 11:05:21 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 11:06:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 11:06:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 11:06:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 11:06:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 11:06:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 11:06:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 11:06:12 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 11:06:12 [scrapy.core.engine] INFO: Spider opened
2018-10-12 11:06:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 11:06:12 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 11:06:13 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 11:06:13 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B5320>
2018-10-12 11:06:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285923>: HTTP status code is not handled or not allowed
2018-10-12 11:06:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/286867>: HTTP status code is not handled or not allowed
2018-10-12 11:06:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/287517>: HTTP status code is not handled or not allowed
2018-10-12 11:06:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/287904>: HTTP status code is not handled or not allowed
2018-10-12 11:06:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/288785>: HTTP status code is not handled or not allowed
2018-10-12 11:06:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/289069>: HTTP status code is not handled or not allowed
2018-10-12 11:06:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280199>: HTTP status code is not handled or not allowed
2018-10-12 11:06:18 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280682>: HTTP status code is not handled or not allowed
2018-10-12 11:06:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280596>: HTTP status code is not handled or not allowed
2018-10-12 11:06:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/281623>: HTTP status code is not handled or not allowed
2018-10-12 11:06:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/282362>: HTTP status code is not handled or not allowed
2018-10-12 11:06:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285618>: HTTP status code is not handled or not allowed
2018-10-12 11:06:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285835>: HTTP status code is not handled or not allowed
2018-10-12 11:06:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/278320>: HTTP status code is not handled or not allowed
2018-10-12 11:06:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279512>: HTTP status code is not handled or not allowed
2018-10-12 11:06:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279545>: HTTP status code is not handled or not allowed
2018-10-12 11:06:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279665>: HTTP status code is not handled or not allowed
2018-10-12 11:06:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279860>: HTTP status code is not handled or not allowed
2018-10-12 11:06:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279898>: HTTP status code is not handled or not allowed
2018-10-12 11:06:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280024>: HTTP status code is not handled or not allowed
2018-10-12 11:06:26 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 11:06:26 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 11:06:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7336,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 31670,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/521': 20,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 3, 6, 26, 210178),
 'httperror/response_ignored_count': 20,
 'httperror/response_ignored_status_count/521': 20,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 3, 6, 12, 588399)}
2018-10-12 11:06:26 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 11:09:49 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 11:09:49 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 11:09:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 11:09:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 11:09:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 11:09:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 11:09:50 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 11:09:50 [scrapy.core.engine] INFO: Spider opened
2018-10-12 11:09:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 11:09:50 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 11:09:51 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 11:09:51 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057BB2B0>
2018-10-12 11:09:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280199>: HTTP status code is not handled or not allowed
2018-10-12 11:09:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280682>: HTTP status code is not handled or not allowed
2018-10-12 11:09:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280596>: HTTP status code is not handled or not allowed
2018-10-12 11:09:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/281623>: HTTP status code is not handled or not allowed
2018-10-12 11:09:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/282362>: HTTP status code is not handled or not allowed
2018-10-12 11:09:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285618>: HTTP status code is not handled or not allowed
2018-10-12 11:09:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285835>: HTTP status code is not handled or not allowed
2018-10-12 11:09:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285923>: HTTP status code is not handled or not allowed
2018-10-12 11:09:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/286867>: HTTP status code is not handled or not allowed
2018-10-12 11:09:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/287517>: HTTP status code is not handled or not allowed
2018-10-12 11:09:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/287904>: HTTP status code is not handled or not allowed
2018-10-12 11:09:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/288785>: HTTP status code is not handled or not allowed
2018-10-12 11:09:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/289069>: HTTP status code is not handled or not allowed
2018-10-12 11:09:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/278320>: HTTP status code is not handled or not allowed
2018-10-12 11:10:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279512>: HTTP status code is not handled or not allowed
2018-10-12 11:10:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279545>: HTTP status code is not handled or not allowed
2018-10-12 11:10:01 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279665>: HTTP status code is not handled or not allowed
2018-10-12 11:10:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279860>: HTTP status code is not handled or not allowed
2018-10-12 11:10:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279898>: HTTP status code is not handled or not allowed
2018-10-12 11:10:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280024>: HTTP status code is not handled or not allowed
2018-10-12 11:10:03 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 11:10:03 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 11:10:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7448,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 31842,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/521': 20,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 3, 10, 3, 783623),
 'httperror/response_ignored_count': 20,
 'httperror/response_ignored_status_count/521': 20,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 3, 9, 50, 258849)}
2018-10-12 11:10:03 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 11:10:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 11:10:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 11:10:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 11:10:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 11:10:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 11:10:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 11:10:26 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 11:10:26 [scrapy.core.engine] INFO: Spider opened
2018-10-12 11:10:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 11:10:26 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 11:10:27 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 11:10:27 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B9358>
2018-10-12 11:10:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285835>: HTTP status code is not handled or not allowed
2018-10-12 11:10:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285923>: HTTP status code is not handled or not allowed
2018-10-12 11:10:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/286867>: HTTP status code is not handled or not allowed
2018-10-12 11:10:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/287517>: HTTP status code is not handled or not allowed
2018-10-12 11:10:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/287904>: HTTP status code is not handled or not allowed
2018-10-12 11:10:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/288785>: HTTP status code is not handled or not allowed
2018-10-12 11:10:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/289069>: HTTP status code is not handled or not allowed
2018-10-12 11:10:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279665>: HTTP status code is not handled or not allowed
2018-10-12 11:10:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279860>: HTTP status code is not handled or not allowed
2018-10-12 11:10:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279898>: HTTP status code is not handled or not allowed
2018-10-12 11:10:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280024>: HTTP status code is not handled or not allowed
2018-10-12 11:10:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280199>: HTTP status code is not handled or not allowed
2018-10-12 11:10:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280682>: HTTP status code is not handled or not allowed
2018-10-12 11:10:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/280596>: HTTP status code is not handled or not allowed
2018-10-12 11:10:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/281623>: HTTP status code is not handled or not allowed
2018-10-12 11:10:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/282362>: HTTP status code is not handled or not allowed
2018-10-12 11:10:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/285618>: HTTP status code is not handled or not allowed
2018-10-12 11:10:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/278320>: HTTP status code is not handled or not allowed
2018-10-12 11:10:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279512>: HTTP status code is not handled or not allowed
2018-10-12 11:10:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <521 https://www.8btc.com/article/279545>: HTTP status code is not handled or not allowed
2018-10-12 11:10:39 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 11:10:39 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 11:10:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 6233,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 31762,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/521': 20,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 3, 10, 39, 244651),
 'httperror/response_ignored_count': 20,
 'httperror/response_ignored_status_count/521': 20,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 3, 10, 26, 147902)}
2018-10-12 11:10:39 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:13:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:13:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:13:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:13:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:13:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:13:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:13:34 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:13:34 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:13:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:13:34 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:13:35 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:13:35 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7358>
2018-10-12 15:13:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:13:47 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:13:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7251,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 388047,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 13, 47, 630483),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 7, 13, 34, 685250)}
2018-10-12 15:13:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:14:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:14:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:14:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:14:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:14:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:14:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:14:18 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:14:18 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:14:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:14:18 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:14:19 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:14:19 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B9358>
2018-10-12 15:14:31 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:14:31 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:14:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7468,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 383332,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 14, 31, 79594),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 7, 14, 18, 130761)}
2018-10-12 15:14:31 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:18:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:18:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:18:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:18:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:18:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:18:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:18:33 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:18:33 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:18:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:18:33 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:18:34 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:18:34 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7320>
2018-10-12 15:18:47 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:18:47 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:18:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7239,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 385808,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 18, 47, 538839),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 7, 18, 33, 902415)}
2018-10-12 15:18:47 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:19:16 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:19:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:19:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:19:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:19:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:19:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:19:17 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:19:17 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:19:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:19:17 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:19:18 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:19:18 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7320>
2018-10-12 15:19:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://app.blockmeta.com/w1/news/list?num=20&page=1&cat_id=572> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\news\btc_policy_new.py", line 31, in parse
    origin_url = article['id'] #内容页链接
TypeError: string indices must be integers
2018-10-12 15:19:19 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:19:19 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:19:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 246,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 8681,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 19, 19, 132506),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 10, 12, 7, 19, 17, 790904)}
2018-10-12 15:19:19 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:19:37 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:19:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:19:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:19:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:19:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:19:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:19:38 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:19:38 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:19:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:19:38 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:19:39 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:19:39 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7320>
2018-10-12 15:19:40 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:19:40 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:19:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 611,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 29608,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 19, 40, 350759),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 12, 7, 19, 38, 619156)}
2018-10-12 15:19:40 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:26:12 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:26:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:26:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:26:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:26:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:26:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:26:13 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:26:13 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:26:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:26:13 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:26:14 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:26:14 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6358>
2018-10-12 15:26:15 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:26:15 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:26:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 679,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 29742,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 26, 15, 730140),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 12, 7, 26, 13, 340336)}
2018-10-12 15:26:15 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:30:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:30:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:30:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:30:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:30:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:30:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:30:36 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:30:36 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:30:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:30:36 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:30:37 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:30:37 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6358>
2018-10-12 15:30:38 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:30:38 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:30:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 667,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 29607,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 30, 38, 935193),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 12, 7, 30, 36, 844790)}
2018-10-12 15:30:38 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:38:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:38:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:38:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:38:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:38:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:38:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:38:18 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:38:18 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:38:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:38:18 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:38:19 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:38:19 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6320>
2018-10-12 15:38:21 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:38:21 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:38:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 646,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 29609,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 38, 21, 127202),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 12, 7, 38, 18, 880798)}
2018-10-12 15:38:21 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:38:46 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:38:46 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:38:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:38:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:38:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:38:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:38:46 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:38:46 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:38:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:38:46 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:38:47 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:38:48 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C6320>
2018-10-12 15:38:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:38:48 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:38:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 702,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 29736,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 38, 48, 994054),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 12, 7, 38, 46, 964050)}
2018-10-12 15:38:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:44:08 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:44:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:44:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:44:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:44:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:44:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:44:08 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:44:08 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:44:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:44:08 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:44:09 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:44:10 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C6358>
2018-10-12 15:44:10 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:44:10 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:44:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 702,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 29602,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 44, 10, 922087),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 12, 7, 44, 8, 952484)}
2018-10-12 15:44:10 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:45:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:45:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:45:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:45:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:45:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:45:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:45:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:45:29 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:45:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:45:29 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:45:30 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:45:30 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7390>
2018-10-12 15:45:31 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:45:31 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:45:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 661,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 29602,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 45, 31, 425034),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 12, 7, 45, 29, 599831)}
2018-10-12 15:45:31 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:46:10 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:46:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:46:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:46:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:46:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:46:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:46:11 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:46:11 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:46:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:46:11 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:46:12 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:46:12 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6358>
2018-10-12 15:46:13 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:46:13 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:46:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 635,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 29622,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 46, 13, 317323),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2018, 10, 12, 7, 46, 11, 269719)}
2018-10-12 15:46:13 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:47:39 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:47:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:47:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:47:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:47:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:47:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:47:40 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:47:40 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:47:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:47:40 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:47:41 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:47:41 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6390>
2018-10-12 15:47:43 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:47:43 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:47:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1478,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 64282,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 47, 43, 377289),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 10, 12, 7, 47, 40, 286483)}
2018-10-12 15:47:43 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:51:45 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:51:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:51:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:51:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:51:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:51:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:51:46 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:51:46 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:51:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:51:46 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:51:47 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:51:47 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6358>
2018-10-12 15:51:49 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:51:49 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:51:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 991,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 42724,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 51, 49, 35867),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 10, 12, 7, 51, 46, 362262)}
2018-10-12 15:51:49 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 15:57:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 15:57:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 15:57:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 15:57:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 15:57:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 15:57:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 15:57:41 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 15:57:41 [scrapy.core.engine] INFO: Spider opened
2018-10-12 15:57:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 15:57:41 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 15:57:42 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 15:57:42 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7358>
2018-10-12 15:57:55 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 15:57:55 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 15:57:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7160,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 383573,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 7, 57, 55, 173526),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 7, 57, 41, 186901)}
2018-10-12 15:57:55 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 16:00:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 16:00:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 16:00:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 16:00:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 16:00:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 16:00:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 16:00:41 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 16:00:41 [scrapy.core.engine] INFO: Spider opened
2018-10-12 16:00:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 16:00:41 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 16:00:42 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 16:00:42 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7390>
2018-10-12 16:00:43 [btc_policy_new] INFO: btc_policy_new巴西证交会：允许基金投资国外加密产品并提出6项ICO防范措施内容已过滤a标签。。。
2018-10-12 16:00:44 [btc_policy_new] INFO: btc_policy_new案评｜致命“代投”——投资者一定要了解的法律风险内容已过滤a标签。。。
2018-10-12 16:00:44 [btc_policy_new] INFO: btc_policy_new“区块链应用”常见法律风险问答（下）内容已过滤a标签。。。
2018-10-12 16:00:45 [btc_policy_new] INFO: btc_policy_new币改、票改、链改系列之三：数字资产托管的法律分析内容已过滤a标签。。。
2018-10-12 16:00:46 [btc_policy_new] INFO: btc_policy_new最高法新规推动网络空间治理法治化 法律和区块链的完美碰撞？内容已过滤a标签。。。
2018-10-12 16:00:48 [btc_policy_new] INFO: btc_policy_new马耳他：加密货币相关法案将于今年11月1日实施内容已过滤a标签。。。
2018-10-12 16:00:49 [btc_policy_new] INFO: btc_policy_new法国考虑制定首个针对加密资产中介的监管框架，强制交易所等服务提供商进行登记内容已过滤a标签。。。
2018-10-12 16:00:51 [btc_policy_new] INFO: btc_policy_new将token视为证券，ICO在阿联酋走上合法化之路内容已过滤a标签。。。
2018-10-12 16:00:52 [btc_policy_new] INFO: btc_policy_new一张图看懂全球数字货币监管态度内容已过滤a标签。。。
2018-10-12 16:00:53 [btc_policy_new] INFO: btc_policy_new必读的全球区块链政策（下篇）：看遍各地区的营商环境内容已过滤a标签。。。
2018-10-12 16:00:53 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 16:00:53 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 16:00:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7443,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 383965,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 8, 0, 53, 696736),
 'item_scraped_count': 20,
 'log_count/INFO': 21,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 8, 0, 41, 184307)}
2018-10-12 16:00:53 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 16:02:59 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 16:02:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 16:02:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 16:02:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 16:03:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 16:03:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 16:03:00 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 16:03:00 [scrapy.core.engine] INFO: Spider opened
2018-10-12 16:03:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 16:03:00 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 16:03:01 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 16:03:01 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B8390>
2018-10-12 16:03:02 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 16:03:02 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 16:03:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 336,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 8680,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 8, 3, 2, 108586),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 12, 8, 3, 0, 797184)}
2018-10-12 16:03:02 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 16:05:19 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 16:05:19 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 16:05:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 16:05:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 16:05:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 16:05:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 16:05:20 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 16:05:20 [scrapy.core.engine] INFO: Spider opened
2018-10-12 16:05:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 16:05:20 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 16:05:21 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 16:05:21 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7390>
2018-10-12 16:05:22 [btc_policy_new] INFO: btc_policy_new巴西证交会：允许基金投资国外加密产品并提出6项ICO防范措施内容已过滤a标签。。。
2018-10-12 16:05:23 [btc_policy_new] INFO: btc_policy_new案评｜致命“代投”——投资者一定要了解的法律风险内容已过滤a标签。。。
2018-10-12 16:05:24 [btc_policy_new] INFO: btc_policy_new“区块链应用”常见法律风险问答（下）内容已过滤a标签。。。
2018-10-12 16:05:25 [btc_policy_new] INFO: btc_policy_new币改、票改、链改系列之三：数字资产托管的法律分析内容已过滤a标签。。。
2018-10-12 16:05:26 [btc_policy_new] INFO: btc_policy_new最高法新规推动网络空间治理法治化 法律和区块链的完美碰撞？内容已过滤a标签。。。
2018-10-12 16:05:28 [btc_policy_new] INFO: btc_policy_new马耳他：加密货币相关法案将于今年11月1日实施内容已过滤a标签。。。
2018-10-12 16:05:29 [btc_policy_new] INFO: btc_policy_new法国考虑制定首个针对加密资产中介的监管框架，强制交易所等服务提供商进行登记内容已过滤a标签。。。
2018-10-12 16:05:31 [btc_policy_new] INFO: btc_policy_new将token视为证券，ICO在阿联酋走上合法化之路内容已过滤a标签。。。
2018-10-12 16:05:31 [btc_policy_new] INFO: btc_policy_new一张图看懂全球数字货币监管态度内容已过滤a标签。。。
2018-10-12 16:05:34 [btc_policy_new] INFO: btc_policy_new必读的全球区块链政策（下篇）：看遍各地区的营商环境内容已过滤a标签。。。
2018-10-12 16:05:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 16:05:34 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 16:05:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7326,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 385920,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 8, 5, 34, 64945),
 'item_scraped_count': 20,
 'log_count/INFO': 21,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 8, 5, 20, 196977)}
2018-10-12 16:05:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 16:06:05 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 16:06:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 16:06:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 16:06:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 16:06:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 16:06:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 16:06:06 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 16:06:06 [scrapy.core.engine] INFO: Spider opened
2018-10-12 16:06:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 16:06:06 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 16:06:07 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 16:06:07 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7390>
2018-10-12 16:06:07 [btc_policy_new] INFO: btc_policy_new巴西证交会：允许基金投资国外加密产品并提出6项ICO防范措施内容已过滤a标签。。。
2018-10-12 16:06:09 [btc_policy_new] INFO: btc_policy_new案评｜致命“代投”——投资者一定要了解的法律风险内容已过滤a标签。。。
2018-10-12 16:06:10 [btc_policy_new] INFO: btc_policy_new“区块链应用”常见法律风险问答（下）内容已过滤a标签。。。
2018-10-12 16:06:10 [btc_policy_new] INFO: btc_policy_new币改、票改、链改系列之三：数字资产托管的法律分析内容已过滤a标签。。。
2018-10-12 16:06:12 [btc_policy_new] INFO: btc_policy_new最高法新规推动网络空间治理法治化 法律和区块链的完美碰撞？内容已过滤a标签。。。
2018-10-12 16:06:13 [btc_policy_new] INFO: btc_policy_new马耳他：加密货币相关法案将于今年11月1日实施内容已过滤a标签。。。
2018-10-12 16:06:15 [btc_policy_new] INFO: btc_policy_new法国考虑制定首个针对加密资产中介的监管框架，强制交易所等服务提供商进行登记内容已过滤a标签。。。
2018-10-12 16:06:16 [btc_policy_new] INFO: btc_policy_new将token视为证券，ICO在阿联酋走上合法化之路内容已过滤a标签。。。
2018-10-12 16:06:17 [btc_policy_new] INFO: btc_policy_new一张图看懂全球数字货币监管态度内容已过滤a标签。。。
2018-10-12 16:06:19 [btc_policy_new] INFO: btc_policy_new必读的全球区块链政策（下篇）：看遍各地区的营商环境内容已过滤a标签。。。
2018-10-12 16:06:19 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 16:06:19 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 16:06:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7503,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 382447,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 8, 6, 19, 69735),
 'item_scraped_count': 20,
 'log_count/INFO': 21,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 10, 12, 8, 6, 6, 142709)}
2018-10-12 16:06:19 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-12 16:08:13 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-12 16:08:13 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-12 16:08:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-12 16:08:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-12 16:08:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-12 16:08:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-12 16:08:14 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-12 16:08:14 [scrapy.core.engine] INFO: Spider opened
2018-10-12 16:08:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-12 16:08:14 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-10-12 16:08:15 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-12 16:08:15 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7358>
2018-10-12 16:08:16 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-12 16:08:16 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-10-12 16:08:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 289,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 8680,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 12, 8, 8, 16, 91614),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 12, 8, 8, 14, 764612)}
2018-10-12 16:08:16 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-17 13:46:12 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-17 13:46:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-17 13:46:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-17 13:46:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-17 13:46:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-17 13:46:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-17 13:46:13 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-17 13:46:13 [scrapy.core.engine] INFO: Spider opened
2018-10-17 13:46:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-17 13:46:13 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-17 13:46:17 [bsj_kuaixun] WARNING: Redis服务连接失败
2018-10-17 13:46:17 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B62B0>
2018-10-17 13:46:21 [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.bishijie.com/kuaixun/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\newsletter\bsj_kuaixun.py", line 68, in parse
    if self.redis.sismember('newsletter_urls', origin_url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2018-10-17 13:46:21 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-17 13:46:21 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-17 13:46:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 304,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 41545,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 17, 5, 46, 21, 764078),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ConnectionError': 1,
 'start_time': datetime.datetime(2018, 10, 17, 5, 46, 13, 248591)}
2018-10-17 13:46:21 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-17 13:47:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-17 13:47:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-17 13:47:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-17 13:47:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-17 13:47:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-17 13:47:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-17 13:47:19 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-17 13:47:19 [scrapy.core.engine] INFO: Spider opened
2018-10-17 13:47:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-17 13:47:19 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-17 13:47:20 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-17 13:47:20 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B4320>
2018-10-17 13:47:21 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-17 13:47:21 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-17 13:47:21 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。区块链概念股游久游戏近日被上交所警示
2018-10-17 13:47:21 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。区块链概念股游久游戏近日被上交所警示
2018-10-17 13:47:23 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-17 13:47:23 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-17 13:47:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2289,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100611,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 17, 5, 47, 23, 956636),
 'item_scraped_count': 5,
 'log_count/INFO': 15,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 17, 5, 47, 19, 297369)}
2018-10-17 13:47:23 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 14:46:15 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 14:46:15 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 14:46:15 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 14:46:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:46:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:46:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:46:16 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 14:46:16 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:46:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:46:16 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 14:46:17 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 14:46:17 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B4978>
2018-10-22 14:46:18 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:46:18 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:46:18 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。OKEx将于今日15时开放TRX充提
2018-10-22 14:46:18 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。OKEx将于今日15时开放TRX充提
2018-10-22 14:46:19 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:46:19 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:46:19 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。ZIL放量拉升，4小时成交量达1.8万ETH
2018-10-22 14:46:19 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。ZIL放量拉升，4小时成交量达1.8万ETH
2018-10-22 14:46:19 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:46:19 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:46:19 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。马尼拉大学将于2019年开始提供区块链选修单元
2018-10-22 14:46:19 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。马尼拉大学将于2019年开始提供区块链选修单元
2018-10-22 14:46:20 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:46:20 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:46:20 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。台湾“金管会”：最早将于明年6月制定ICO监管办法
2018-10-22 14:46:20 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。台湾“金管会”：最早将于明年6月制定ICO监管办法
2018-10-22 14:46:21 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:46:21 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:46:21 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。美国地区法官对暗网非法交易者判处20年徒刑
2018-10-22 14:46:21 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。美国地区法官对暗网非法交易者判处20年徒刑
2018-10-22 14:46:21 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 14:46:21 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 14:46:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2339,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101042,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 6, 46, 21, 245318),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 6, 46, 16, 823065)}
2018-10-22 14:46:21 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 14:49:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 14:49:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 14:49:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 14:49:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:49:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:49:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:49:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 14:49:29 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:49:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:49:29 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 14:49:30 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 14:49:30 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B58D0>
2018-10-22 14:49:31 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:49:31 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:49:31 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。OKEx将于今日15时开放TRX充提
2018-10-22 14:49:31 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。OKEx将于今日15时开放TRX充提
2018-10-22 14:49:32 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:49:32 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:49:32 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。ZIL放量拉升，4小时成交量达1.8万ETH
2018-10-22 14:49:32 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。ZIL放量拉升，4小时成交量达1.8万ETH
2018-10-22 14:49:32 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:49:32 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:49:32 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。马尼拉大学将于2019年开始提供区块链选修单元
2018-10-22 14:49:32 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。马尼拉大学将于2019年开始提供区块链选修单元
2018-10-22 14:49:33 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:49:33 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:49:33 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。台湾“金管会”：最早将于明年6月制定ICO监管办法
2018-10-22 14:49:33 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。台湾“金管会”：最早将于明年6月制定ICO监管办法
2018-10-22 14:49:34 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 14:49:34 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 14:49:34 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。美国地区法官对暗网非法交易者判处20年徒刑
2018-10-22 14:49:34 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。美国地区法官对暗网非法交易者判处20年徒刑
2018-10-22 14:49:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 14:49:34 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 14:49:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2586,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101045,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 6, 49, 34, 20052),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 6, 49, 29, 675244)}
2018-10-22 14:49:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 15:01:06 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 15:01:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 15:01:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 15:01:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 15:01:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 15:01:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 15:01:07 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 15:01:07 [scrapy.core.engine] INFO: Spider opened
2018-10-22 15:01:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 15:01:07 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 15:01:08 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 15:01:08 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B4940>
2018-10-22 15:01:09 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:01:09 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:01:09 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。A股收盘：区块链板块上涨5.20%
2018-10-22 15:01:09 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。A股收盘：区块链板块上涨5.20%
2018-10-22 15:01:09 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:01:09 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:01:09 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。台湾“金管会”：最早将于明年6月制定ICO监管办法
2018-10-22 15:01:09 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。台湾“金管会”：最早将于明年6月制定ICO监管办法
2018-10-22 15:01:10 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:01:10 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:01:10 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。美国地区法官对暗网非法交易者判处20年徒刑
2018-10-22 15:01:10 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。美国地区法官对暗网非法交易者判处20年徒刑
2018-10-22 15:01:10 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:01:10 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:01:10 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。OKEx将于今日15时开放TRX充提
2018-10-22 15:01:10 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。OKEx将于今日15时开放TRX充提
2018-10-22 15:01:11 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:01:11 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:01:11 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。寻找“币圈锦鲤”活动今日启动！价值百万大礼包等你来领
2018-10-22 15:01:11 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。寻找“币圈锦鲤”活动今日启动！价值百万大礼包等你来领
2018-10-22 15:01:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 15:01:11 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 15:01:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2539,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100982,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 7, 1, 11, 379733),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 7, 1, 7, 117525)}
2018-10-22 15:01:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 15:09:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 15:09:14 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 15:09:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 15:09:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 15:09:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 15:09:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 15:09:15 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 15:09:15 [scrapy.core.engine] INFO: Spider opened
2018-10-22 15:09:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 15:09:15 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 15:09:16 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 15:09:16 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B4940>
2018-10-22 15:09:17 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:09:17 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:09:17 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。徐明星：某自媒体多次针对本人指明道姓写黑稿
2018-10-22 15:09:17 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。徐明星：某自媒体多次针对本人指明道姓写黑稿
2018-10-22 15:09:17 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:09:17 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:09:17 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。美国地区法官对暗网非法交易者判处20年徒刑
2018-10-22 15:09:17 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。美国地区法官对暗网非法交易者判处20年徒刑
2018-10-22 15:09:18 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:09:18 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:09:18 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。OKEx将于今日15时开放TRX充提
2018-10-22 15:09:18 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。OKEx将于今日15时开放TRX充提
2018-10-22 15:09:18 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:09:18 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:09:18 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。寻找“币圈锦鲤”活动今日启动！价值百万大礼包等你来领
2018-10-22 15:09:18 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。寻找“币圈锦鲤”活动今日启动！价值百万大礼包等你来领
2018-10-22 15:09:19 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:09:19 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:09:19 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。A股收盘：区块链板块上涨5.20%
2018-10-22 15:09:19 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。A股收盘：区块链板块上涨5.20%
2018-10-22 15:09:19 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 15:09:19 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 15:09:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2434,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101329,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 7, 9, 19, 574823),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 7, 9, 15, 374416)}
2018-10-22 15:09:19 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 15:35:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 15:35:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 15:35:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 15:35:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 15:35:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 15:35:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 15:35:12 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 15:35:12 [scrapy.core.engine] INFO: Spider opened
2018-10-22 15:35:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 15:35:12 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 15:35:13 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 15:35:13 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B5908>
2018-10-22 15:35:13 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:35:13 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:35:13 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。DigiFinex交易所创始人：STO是IPO在区块链时代的升级版本
2018-10-22 15:35:13 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。DigiFinex交易所创始人：STO是IPO在区块链时代的升级版本
2018-10-22 15:35:14 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:35:14 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:35:14 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Mati Greenspan：经济学家等会将数字货币视为一种潜在的有效金融工具
2018-10-22 15:35:14 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Mati Greenspan：经济学家等会将数字货币视为一种潜在的有效金融工具
2018-10-22 15:35:15 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:35:15 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:35:15 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。韩国区块链媒体BlockDaily获ONO资本投资
2018-10-22 15:35:15 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。韩国区块链媒体BlockDaily获ONO资本投资
2018-10-22 15:35:15 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:35:15 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:35:15 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。末日博士Nouriel Roubini：至少92％的ICO是垃圾币的印刷机
2018-10-22 15:35:15 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。末日博士Nouriel Roubini：至少92％的ICO是垃圾币的印刷机
2018-10-22 15:35:16 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 15:35:16 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 15:35:16 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。用户爆料称Bitfinex声称被盗了12万个比特币的黑客事件是编造出来
2018-10-22 15:35:16 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。用户爆料称Bitfinex声称被盗了12万个比特币的黑客事件是编造出来
2018-10-22 15:35:16 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 15:35:16 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 15:35:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2528,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101954,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 7, 35, 16, 240973),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 7, 35, 12, 238366)}
2018-10-22 15:35:16 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 17:16:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 17:16:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 17:16:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 17:16:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 17:16:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 17:16:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 17:16:56 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 17:16:56 [scrapy.core.engine] INFO: Spider opened
2018-10-22 17:16:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 17:16:56 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 17:16:57 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 17:16:57 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B4940>
2018-10-22 17:16:58 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:16:58 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:16:58 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。中国电子商务协会会长：数字资产是否违法可从三个方面判断
2018-10-22 17:16:58 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。中国电子商务协会会长：数字资产是否违法可从三个方面判断
2018-10-22 17:16:58 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:16:58 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:16:58 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Sberbank准备为用户提供提供代币销售服务
2018-10-22 17:16:58 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Sberbank准备为用户提供提供代币销售服务
2018-10-22 17:16:59 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:16:59 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:16:59 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。农发行费县支行普及数字货币知识
2018-10-22 17:16:59 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。农发行费县支行普及数字货币知识
2018-10-22 17:17:00 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:17:00 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:17:00 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。DigiFinex今日开启“幸运大转盘”交易抽奖活动
2018-10-22 17:17:00 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。DigiFinex今日开启“幸运大转盘”交易抽奖活动
2018-10-22 17:17:00 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:17:00 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:17:00 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。专家建议利用区块链等技术抑制“黑油”泛滥
2018-10-22 17:17:00 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。专家建议利用区块链等技术抑制“黑油”泛滥
2018-10-22 17:17:00 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 17:17:00 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 17:17:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2448,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101842,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 9, 17, 0, 855050),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 9, 16, 56, 383640)}
2018-10-22 17:17:00 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 17:27:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 17:27:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 17:27:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 17:27:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 17:27:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 17:27:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 17:27:42 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 17:27:42 [scrapy.core.engine] INFO: Spider opened
2018-10-22 17:27:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 17:27:42 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 17:27:43 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 17:27:43 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B38D0>
2018-10-22 17:27:44 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:27:44 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:27:44 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。刚刚火币内部互转约3025万枚USDT
2018-10-22 17:27:44 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。刚刚火币内部互转约3025万枚USDT
2018-10-22 17:27:45 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:27:45 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:27:45 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。科学家吴宇：我国仍处于区块链2.0时代
2018-10-22 17:27:45 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。科学家吴宇：我国仍处于区块链2.0时代
2018-10-22 17:27:45 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:27:45 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:27:45 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。巴哈马国务大臣：巴哈马致力于吸引区块链等技术项目 政府正在努力完善加密等监管框架
2018-10-22 17:27:45 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。巴哈马国务大臣：巴哈马致力于吸引区块链等技术项目 政府正在努力完善加密等监管框架
2018-10-22 17:27:46 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:27:46 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:27:46 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。火币全球站恢复EOS充币业务
2018-10-22 17:27:46 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。火币全球站恢复EOS充币业务
2018-10-22 17:27:46 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:27:46 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:27:46 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。陕西省工商联副主席：区块链技术要以解决企业生产经营过程中的实际问题为出发点
2018-10-22 17:27:46 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。陕西省工商联副主席：区块链技术要以解决企业生产经营过程中的实际问题为出发点
2018-10-22 17:27:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 17:27:46 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 17:27:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2367,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100952,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 9, 27, 46, 796703),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 9, 27, 42, 744609)}
2018-10-22 17:27:46 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 17:33:16 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 17:33:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 17:33:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 17:33:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 17:33:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 17:33:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 17:33:16 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 17:33:16 [scrapy.core.engine] INFO: Spider opened
2018-10-22 17:33:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 17:33:16 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 17:33:17 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 17:33:17 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B5940>
2018-10-22 17:33:18 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:33:18 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:33:18 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。币安上线收益率大赛，设250万奖池
2018-10-22 17:33:18 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。币安上线收益率大赛，设250万奖池
2018-10-22 17:33:19 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:33:19 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:33:19 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。巴哈马国务大臣：巴哈马致力于吸引区块链等技术项目 政府正在努力完善加密等监管框架
2018-10-22 17:33:19 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。巴哈马国务大臣：巴哈马致力于吸引区块链等技术项目 政府正在努力完善加密等监管框架
2018-10-22 17:33:20 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:33:20 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:33:20 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。火币全球站恢复EOS充币业务
2018-10-22 17:33:20 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。火币全球站恢复EOS充币业务
2018-10-22 17:33:21 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:33:21 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:33:21 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。陕西省工商联副主席：区块链技术要以解决企业生产经营过程中的实际问题为出发点
2018-10-22 17:33:21 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。陕西省工商联副主席：区块链技术要以解决企业生产经营过程中的实际问题为出发点
2018-10-22 17:33:21 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:33:21 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:33:21 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。刚刚火币内部互转约3025万枚USDT
2018-10-22 17:33:21 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。刚刚火币内部互转约3025万枚USDT
2018-10-22 17:33:21 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 17:33:21 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 17:33:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2433,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100994,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 9, 33, 21, 722361),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 9, 33, 16, 927152)}
2018-10-22 17:33:21 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 17:34:30 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 17:34:30 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 17:34:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 17:34:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 17:34:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 17:34:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 17:34:31 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 17:34:31 [scrapy.core.engine] INFO: Spider opened
2018-10-22 17:34:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 17:34:31 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 17:34:32 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 17:34:32 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B5860>
2018-10-22 17:34:33 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:34:33 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:34:33 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。币安上线收益率大赛，设250万奖池
2018-10-22 17:34:33 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。币安上线收益率大赛，设250万奖池
2018-10-22 17:34:34 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:34:34 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:34:34 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。巴哈马国务大臣：巴哈马致力于吸引区块链等技术项目 政府正在努力完善加密等监管框架
2018-10-22 17:34:34 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。巴哈马国务大臣：巴哈马致力于吸引区块链等技术项目 政府正在努力完善加密等监管框架
2018-10-22 17:34:34 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:34:34 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:34:34 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。火币全球站恢复EOS充币业务
2018-10-22 17:34:34 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。火币全球站恢复EOS充币业务
2018-10-22 17:34:35 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:34:35 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:34:35 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。陕西省工商联副主席：区块链技术要以解决企业生产经营过程中的实际问题为出发点
2018-10-22 17:34:35 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。陕西省工商联副主席：区块链技术要以解决企业生产经营过程中的实际问题为出发点
2018-10-22 17:34:35 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:34:35 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:34:35 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。刚刚火币内部互转约3025万枚USDT
2018-10-22 17:34:35 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。刚刚火币内部互转约3025万枚USDT
2018-10-22 17:34:35 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 17:34:35 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 17:34:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2458,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100995,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 9, 34, 35, 923902),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 9, 34, 31, 387893)}
2018-10-22 17:34:35 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 17:35:05 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 17:35:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 17:35:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 17:35:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 17:35:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 17:35:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 17:35:06 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 17:35:06 [scrapy.core.engine] INFO: Spider opened
2018-10-22 17:35:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 17:35:06 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 17:35:07 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 17:35:07 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B3908>
2018-10-22 17:35:07 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 17:35:07 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 17:35:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 252,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 6064,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 9, 35, 7, 618744),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 10, 22, 9, 35, 6, 383341)}
2018-10-22 17:35:07 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 17:35:36 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-22 17:35:36 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-22 17:35:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-22 17:35:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 17:35:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 17:35:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 17:35:37 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-22 17:35:37 [scrapy.core.engine] INFO: Spider opened
2018-10-22 17:35:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 17:35:37 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-22 17:35:38 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-22 17:35:38 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B3908>
2018-10-22 17:35:39 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:35:39 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:35:39 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。澳大利亚证券监管机构（ASIC）停止区块链公司Global Tech Exchange的ICO项目
2018-10-22 17:35:39 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。澳大利亚证券监管机构（ASIC）停止区块链公司Global Tech Exchange的ICO项目
2018-10-22 17:35:39 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:35:39 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:35:39 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。火币全球站恢复EOS充币业务
2018-10-22 17:35:39 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。火币全球站恢复EOS充币业务
2018-10-22 17:35:40 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:35:40 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:35:40 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。陕西省工商联副主席：区块链技术要以解决企业生产经营过程中的实际问题为出发点
2018-10-22 17:35:40 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。陕西省工商联副主席：区块链技术要以解决企业生产经营过程中的实际问题为出发点
2018-10-22 17:35:41 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:35:41 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:35:41 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。刚刚火币内部互转约3025万枚USDT
2018-10-22 17:35:41 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。刚刚火币内部互转约3025万枚USDT
2018-10-22 17:35:41 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-22 17:35:41 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-22 17:35:41 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。币安上线收益率大赛，设250万奖池
2018-10-22 17:35:41 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。币安上线收益率大赛，设250万奖池
2018-10-22 17:35:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 17:35:41 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-22 17:35:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2426,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100376,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 9, 35, 41, 848956),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 22, 9, 35, 37, 691718)}
2018-10-22 17:35:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-23 09:20:59 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-23 09:20:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-23 09:20:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-23 09:20:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 09:21:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 09:21:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 09:21:00 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-23 09:21:00 [scrapy.core.engine] INFO: Spider opened
2018-10-23 09:21:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 09:21:00 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-23 09:21:01 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-23 09:21:01 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C5898>
2018-10-23 09:21:02 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:21:02 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:21:02 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。比特大陆：AsicBoost不存在专利权问题 矿场都能使用
2018-10-23 09:21:02 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。比特大陆：AsicBoost不存在专利权问题 矿场都能使用
2018-10-23 09:21:02 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:21:02 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:21:02 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。调查显示：英国伦敦市民投资加密货币的可能性是英国其他地区的两倍
2018-10-23 09:21:02 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。调查显示：英国伦敦市民投资加密货币的可能性是英国其他地区的两倍
2018-10-23 09:21:03 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:21:03 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:21:03 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。冰岛公民凭本国地热资源纷纷入局加密挖矿 引发政府官员担忧
2018-10-23 09:21:03 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。冰岛公民凭本国地热资源纷纷入局加密挖矿 引发政府官员担忧
2018-10-23 09:21:03 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:21:03 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:21:03 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Circle CEO敦促世界经济体建立全球数字货币规则
2018-10-23 09:21:03 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Circle CEO敦促世界经济体建立全球数字货币规则
2018-10-23 09:21:04 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:21:04 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:21:04 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。PIVX宣布与终极格斗冠军赛冠军合作
2018-10-23 09:21:04 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。PIVX宣布与终极格斗冠军赛冠军合作
2018-10-23 09:21:04 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 09:21:04 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-23 09:21:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2439,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101381,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 1, 21, 4, 357446),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 23, 1, 21, 0, 284213)}
2018-10-23 09:21:04 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-23 09:27:36 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-23 09:27:36 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-23 09:27:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-23 09:27:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 09:27:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 09:27:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 09:27:37 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-23 09:27:37 [scrapy.core.engine] INFO: Spider opened
2018-10-23 09:27:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 09:27:37 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-23 09:27:38 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-23 09:27:38 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C4908>
2018-10-23 09:27:39 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:27:39 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:27:39 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数字货币交易所WEX无法提现已提交警方报告
2018-10-23 09:27:39 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数字货币交易所WEX无法提现已提交警方报告
2018-10-23 09:27:39 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:27:39 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:27:39 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。PIVX宣布与终极格斗冠军赛冠军合作
2018-10-23 09:27:39 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。PIVX宣布与终极格斗冠军赛冠军合作
2018-10-23 09:27:40 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:27:40 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:27:40 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。比特大陆：AsicBoost不存在专利权问题 矿场都能使用
2018-10-23 09:27:40 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。比特大陆：AsicBoost不存在专利权问题 矿场都能使用
2018-10-23 09:27:40 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:27:40 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:27:40 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。DTA对政府使用区块链表示怀疑
2018-10-23 09:27:40 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。DTA对政府使用区块链表示怀疑
2018-10-23 09:27:41 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:27:41 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:27:41 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Circle CEO敦促世界经济体建立全球数字货币规则
2018-10-23 09:27:41 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Circle CEO敦促世界经济体建立全球数字货币规则
2018-10-23 09:27:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 09:27:41 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-23 09:27:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2438,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101635,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 1, 27, 41, 518645),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 23, 1, 27, 37, 461836)}
2018-10-23 09:27:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-23 09:28:57 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-23 09:28:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-23 09:28:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-23 09:28:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 09:28:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 09:28:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 09:28:58 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-23 09:28:58 [scrapy.core.engine] INFO: Spider opened
2018-10-23 09:28:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 09:28:58 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-23 09:28:59 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-23 09:28:59 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B48D0>
2018-10-23 09:29:00 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:29:00 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:29:00 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数字货币交易所WEX无法提现已提交警方报告
2018-10-23 09:29:00 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数字货币交易所WEX无法提现已提交警方报告
2018-10-23 09:29:01 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:29:01 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:29:01 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Circle CEO敦促世界经济体建立全球数字货币规则
2018-10-23 09:29:01 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Circle CEO敦促世界经济体建立全球数字货币规则
2018-10-23 09:29:01 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:29:01 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:29:01 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。PIVX宣布与终极格斗冠军赛冠军合作
2018-10-23 09:29:01 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。PIVX宣布与终极格斗冠军赛冠军合作
2018-10-23 09:29:02 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:29:02 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:29:02 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。比特大陆：AsicBoost不存在专利权问题 矿场都能使用
2018-10-23 09:29:02 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。比特大陆：AsicBoost不存在专利权问题 矿场都能使用
2018-10-23 09:29:03 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:29:03 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:29:03 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。DTA对政府使用区块链表示怀疑
2018-10-23 09:29:03 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。DTA对政府使用区块链表示怀疑
2018-10-23 09:29:03 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 09:29:03 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-23 09:29:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2580,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101629,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 1, 29, 3, 101863),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 23, 1, 28, 58, 804618)}
2018-10-23 09:29:03 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-23 09:30:09 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-23 09:30:09 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-23 09:30:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-23 09:30:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 09:30:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 09:30:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 09:30:10 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-23 09:30:10 [scrapy.core.engine] INFO: Spider opened
2018-10-23 09:30:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 09:30:10 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-23 09:30:11 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-23 09:30:11 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C5908>
2018-10-23 09:30:12 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:30:12 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:30:12 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数字货币交易所WEX无法提现已提交警方报告
2018-10-23 09:30:12 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数字货币交易所WEX无法提现已提交警方报告
2018-10-23 09:30:12 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:30:12 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:30:12 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。PIVX宣布与终极格斗冠军赛冠军合作
2018-10-23 09:30:12 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。PIVX宣布与终极格斗冠军赛冠军合作
2018-10-23 09:30:13 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:30:13 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:30:13 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。比特大陆：AsicBoost不存在专利权问题 矿场都能使用
2018-10-23 09:30:13 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。比特大陆：AsicBoost不存在专利权问题 矿场都能使用
2018-10-23 09:30:13 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:30:13 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:30:13 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。DTA对政府使用区块链表示怀疑
2018-10-23 09:30:13 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。DTA对政府使用区块链表示怀疑
2018-10-23 09:30:14 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:30:14 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:30:14 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Circle CEO敦促世界经济体建立全球数字货币规则
2018-10-23 09:30:14 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Circle CEO敦促世界经济体建立全球数字货币规则
2018-10-23 09:30:14 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 09:30:14 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-23 09:30:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2477,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101645,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 1, 30, 14, 673536),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 23, 1, 30, 10, 566328)}
2018-10-23 09:30:14 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-23 09:50:21 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-23 09:50:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-23 09:50:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-23 09:50:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 09:50:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 09:50:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 09:50:22 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-23 09:50:22 [scrapy.core.engine] INFO: Spider opened
2018-10-23 09:50:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 09:50:22 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-23 09:50:23 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-23 09:50:23 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C48D0>
2018-10-23 09:50:24 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:50:24 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:50:24 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。江卓尔：BTC手续费稳定在0.1美元是因为用户流失
2018-10-23 09:50:24 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。江卓尔：BTC手续费稳定在0.1美元是因为用户流失
2018-10-23 09:50:25 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 09:50:25 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 09:50:25 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。A股开盘：区块链板块下跌0.12%
2018-10-23 09:50:25 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。A股开盘：区块链板块下跌0.12%
2018-10-23 09:50:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 09:50:25 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-23 09:50:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1094,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 65621,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 1, 50, 25, 159229),
 'item_scraped_count': 2,
 'log_count/INFO': 19,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 10, 23, 1, 50, 22, 472623)}
2018-10-23 09:50:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-23 10:33:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-23 10:33:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-23 10:33:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-23 10:33:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 10:33:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 10:33:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 10:33:30 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-23 10:33:30 [scrapy.core.engine] INFO: Spider opened
2018-10-23 10:33:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 10:33:30 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-23 10:33:31 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-23 10:33:31 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C2908>
2018-10-23 10:33:32 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:33:33 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:33:33 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:33:33 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。淡马锡旗下祥峰投资宣布投资币安 支持在新加坡建法币交易所
2018-10-23 10:33:33 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。淡马锡旗下祥峰投资宣布投资币安 支持在新加坡建法币交易所
2018-10-23 10:33:34 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:33:35 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:33:35 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 10:33:35 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-23 10:33:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2213,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100754,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 2, 33, 35, 7919),
 'item_scraped_count': 5,
 'log_count/INFO': 18,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 23, 2, 33, 30, 552664)}
2018-10-23 10:33:35 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-23 10:40:02 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-23 10:40:02 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-23 10:40:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-23 10:40:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 10:40:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 10:40:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 10:40:03 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-23 10:40:03 [scrapy.core.engine] INFO: Spider opened
2018-10-23 10:40:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 10:40:03 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-23 10:40:04 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-23 10:40:04 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B4908>
2018-10-23 10:40:05 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:40:05 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:40:05 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。人民网总裁叶蓁蓁：区块链行业没有改变
2018-10-23 10:40:05 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。人民网总裁叶蓁蓁：区块链行业没有改变
2018-10-23 10:40:06 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:40:06 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:40:06 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。特斯拉创始人马斯克：想买比特币吗？
2018-10-23 10:40:06 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。特斯拉创始人马斯克：想买比特币吗？
2018-10-23 10:40:07 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:40:07 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:40:07 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。DASH今日15:00上线DigiFinex
2018-10-23 10:40:07 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。DASH今日15:00上线DigiFinex
2018-10-23 10:40:07 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:40:07 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:40:07 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。今日人民网总裁叶蓁蓁表示，人民网区块链平台正式上线。
2018-10-23 10:40:07 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。今日人民网总裁叶蓁蓁表示，人民网区块链平台正式上线。
2018-10-23 10:40:07 [bsj_kuaixun] WARNING: 快讯推送失败 400
2018-10-23 10:40:08 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:40:08 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:40:08 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。淡马锡旗下祥峰投资宣布投资币安 支持在新加坡建法币交易所
2018-10-23 10:40:08 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。淡马锡旗下祥峰投资宣布投资币安 支持在新加坡建法币交易所
2018-10-23 10:40:08 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 10:40:08 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-23 10:40:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2535,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100749,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 2, 40, 8, 159658),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 23, 2, 40, 3, 863650)}
2018-10-23 10:40:08 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-23 10:44:39 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-23 10:44:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-23 10:44:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-23 10:44:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 10:44:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 10:44:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 10:44:40 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-23 10:44:40 [scrapy.core.engine] INFO: Spider opened
2018-10-23 10:44:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 10:44:40 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-23 10:44:41 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-23 10:44:41 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C5908>
2018-10-23 10:44:42 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:44:42 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:44:42 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。人民网总裁叶蓁蓁：区块链行业没有改变
2018-10-23 10:44:42 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。人民网总裁叶蓁蓁：区块链行业没有改变
2018-10-23 10:44:43 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:44:43 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:44:43 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。DASH今日15:00上线DigiFinex
2018-10-23 10:44:43 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。DASH今日15:00上线DigiFinex
2018-10-23 10:44:43 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:44:43 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:44:43 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。今日人民网总裁叶蓁蓁表示，人民网区块链平台正式上线。
2018-10-23 10:44:43 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。今日人民网总裁叶蓁蓁表示，人民网区块链平台正式上线。
2018-10-23 10:44:43 [bsj_kuaixun] WARNING: 快讯推送失败 400
2018-10-23 10:44:43 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:44:43 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:44:43 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。淡马锡旗下祥峰投资宣布投资币安 支持在新加坡建法币交易所
2018-10-23 10:44:43 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。淡马锡旗下祥峰投资宣布投资币安 支持在新加坡建法币交易所
2018-10-23 10:44:44 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-23 10:44:44 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-23 10:44:44 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。特斯拉创始人马斯克：想买比特币吗？
2018-10-23 10:44:44 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。特斯拉创始人马斯克：想买比特币吗？
2018-10-23 10:44:44 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 10:44:44 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-23 10:44:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2421,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 100764,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 2, 44, 44, 408208),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 23, 2, 44, 40, 95199)}
2018-10-23 10:44:44 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 09:07:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-30 09:07:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 09:07:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-30 09:07:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 09:07:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 09:07:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 09:07:44 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-30 09:07:44 [scrapy.core.engine] INFO: Spider opened
2018-10-30 09:07:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 09:07:44 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-30 09:07:45 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-30 09:07:45 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057AEDD8>
2018-10-30 09:07:46 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 09:07:46 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 09:07:46 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Smartlands宣布和全球律师事务所CMS合作 寻求法律支持以应对监管要求
2018-10-30 09:07:46 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Smartlands宣布和全球律师事务所CMS合作 寻求法律支持以应对监管要求
2018-10-30 09:07:46 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 09:07:46 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 09:07:46 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 09:07:46 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。普华永道聘用400多名区块链专家以应对加密货币客户需求
2018-10-30 09:07:46 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。普华永道聘用400多名区块链专家以应对加密货币客户需求
2018-10-30 09:07:46 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 09:07:47 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 09:07:47 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 09:07:47 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。可信区块链推进计划政策法律研究组将于31号正式成立
2018-10-30 09:07:47 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。可信区块链推进计划政策法律研究组将于31号正式成立
2018-10-30 09:07:47 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 09:07:48 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 09:07:48 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 09:07:48 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。PRL官方发布公告回应凌晨暂停PRL交易事件
2018-10-30 09:07:48 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。PRL官方发布公告回应凌晨暂停PRL交易事件
2018-10-30 09:07:48 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 09:07:48 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 09:07:48 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 09:07:48 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数据显示：BTC换手率显著提高 下跌势能减弱
2018-10-30 09:07:48 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数据显示：BTC换手率显著提高 下跌势能减弱
2018-10-30 09:07:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 09:07:48 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-30 09:07:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2430,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 102700,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 1, 7, 48, 873853),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'log_count/WARNING': 4,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 30, 1, 7, 44, 312592)}
2018-10-30 09:07:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 09:07:48 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:21:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-30 11:21:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 11:21:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-30 11:21:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 11:21:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 11:21:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 11:21:56 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-30 11:21:56 [scrapy.core.engine] INFO: Spider opened
2018-10-30 11:21:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:21:56 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-30 11:21:57 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-30 11:21:57 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057BCDD8>
2018-10-30 11:21:58 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:21:58 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:21:58 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数据显示：昨日各类交易所成交额较前日都有明显增长
2018-10-30 11:21:58 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数据显示：昨日各类交易所成交额较前日都有明显增长
2018-10-30 11:21:58 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:21:59 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:21:59 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:21:59 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数字货币贷款平台Nexo将支持稳定币
2018-10-30 11:21:59 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数字货币贷款平台Nexo将支持稳定币
2018-10-30 11:21:59 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:21:59 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:21:59 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:21:59 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。EOS NewYork创建EEC 支持Windows上的EOS开发人员
2018-10-30 11:21:59 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。EOS NewYork创建EEC 支持Windows上的EOS开发人员
2018-10-30 11:21:59 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:22:00 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:22:00 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:22:00 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。支付宝使用区块链理赔技术提升理赔效率
2018-10-30 11:22:00 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。支付宝使用区块链理赔技术提升理赔效率
2018-10-30 11:22:00 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:22:00 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:22:00 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:22:00 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Bithumb DEX成交量相比昨日激增5倍
2018-10-30 11:22:00 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Bithumb DEX成交量相比昨日激增5倍
2018-10-30 11:22:00 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 11:22:00 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-30 11:22:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2439,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 102936,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 3, 22, 0, 840460),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'log_count/WARNING': 4,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 30, 3, 21, 56, 818230)}
2018-10-30 11:22:00 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 11:22:00 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:33:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-30 11:33:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 11:33:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-30 11:33:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 11:33:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 11:33:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 11:33:18 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-30 11:33:18 [scrapy.core.engine] INFO: Spider opened
2018-10-30 11:33:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:33:18 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-30 11:33:19 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-30 11:33:19 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ABDD8>
2018-10-30 11:33:20 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:33:20 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:33:20 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Coincheck今日重开 提供BTC、ETC、LTC和BCH存储和购买业务
2018-10-30 11:33:20 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Coincheck今日重开 提供BTC、ETC、LTC和BCH存储和购买业务
2018-10-30 11:33:20 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:33:21 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:33:21 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:33:21 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。EOSTitan提供监测EOS/USD实时交易对价格合约工具
2018-10-30 11:33:21 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。EOSTitan提供监测EOS/USD实时交易对价格合约工具
2018-10-30 11:33:21 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:33:22 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:33:22 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:33:22 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Insight Chain行情调研：近七成投资者认为恒星币在目前价格下具有长期投资价值
2018-10-30 11:33:22 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Insight Chain行情调研：近七成投资者认为恒星币在目前价格下具有长期投资价值
2018-10-30 11:33:22 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 11:33:22 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-30 11:33:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1594,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 78722,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 3, 33, 22, 333439),
 'item_scraped_count': 3,
 'log_count/INFO': 23,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 10, 30, 3, 33, 18, 871241)}
2018-10-30 11:33:22 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 11:33:22 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:39:23 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-30 11:39:23 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 11:39:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-30 11:39:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 11:39:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 11:39:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 11:39:25 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-30 11:39:25 [scrapy.core.engine] INFO: Spider opened
2018-10-30 11:39:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:39:25 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-30 11:39:26 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-30 11:39:26 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B0DA0>
2018-10-30 11:39:27 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:39:27 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:39:27 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Coincheck今日重开 提供BTC、ETC、LTC和BCH存储和购买业务
2018-10-30 11:39:27 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:39:27 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Coincheck今日重开 提供BTC、ETC、LTC和BCH存储和购买业务
2018-10-30 11:39:27 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:39:27 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:39:27 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Bithumb DEX成交量相比昨日激增5倍
2018-10-30 11:39:27 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:39:27 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Bithumb DEX成交量相比昨日激增5倍
2018-10-30 11:39:28 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:39:28 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:39:28 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数据显示：昨日各类交易所成交额较前日都有明显增长
2018-10-30 11:39:28 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:39:28 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数据显示：昨日各类交易所成交额较前日都有明显增长
2018-10-30 11:39:28 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:39:28 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:39:28 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。EOSTitan提供监测EOS/USD实时交易对价格合约工具
2018-10-30 11:39:28 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:39:28 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。EOSTitan提供监测EOS/USD实时交易对价格合约工具
2018-10-30 11:39:29 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:39:29 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:39:29 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Insight Chain行情调研：近七成投资者认为恒星币在目前价格下具有长期投资价值
2018-10-30 11:39:29 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:39:29 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Insight Chain行情调研：近七成投资者认为恒星币在目前价格下具有长期投资价值
2018-10-30 11:39:29 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 11:39:29 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-30 11:39:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2418,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 102659,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 3, 39, 29, 402434),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'log_count/WARNING': 5,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 30, 3, 39, 25, 133190)}
2018-10-30 11:39:29 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 11:40:09 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-30 11:40:09 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 11:40:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-30 11:40:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 11:40:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 11:40:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 11:40:10 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-30 11:40:10 [scrapy.core.engine] INFO: Spider opened
2018-10-30 11:40:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:40:10 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-30 11:40:11 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-30 11:40:11 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057AFDD8>
2018-10-30 11:40:12 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:40:12 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:40:12 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。OKEx披露风险与合规信息
2018-10-30 11:40:12 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:40:12 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。OKEx披露风险与合规信息
2018-10-30 11:40:13 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:40:13 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:40:13 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数据显示：昨日各类交易所成交额较前日都有明显增长
2018-10-30 11:40:13 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:40:13 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数据显示：昨日各类交易所成交额较前日都有明显增长
2018-10-30 11:40:13 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:40:13 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:40:13 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。EOSTitan提供监测EOS/USD实时交易对价格合约工具
2018-10-30 11:40:13 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:40:13 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。EOSTitan提供监测EOS/USD实时交易对价格合约工具
2018-10-30 11:40:14 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:40:14 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:40:14 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Insight Chain行情调研：近七成投资者认为恒星币在目前价格下具有长期投资价值
2018-10-30 11:40:14 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:40:14 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Insight Chain行情调研：近七成投资者认为恒星币在目前价格下具有长期投资价值
2018-10-30 11:40:15 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 11:40:15 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 11:40:15 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Coincheck今日重开 提供BTC、ETC、LTC和BCH存储和购买业务
2018-10-30 11:40:15 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 11:40:15 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Coincheck今日重开 提供BTC、ETC、LTC和BCH存储和购买业务
2018-10-30 11:40:15 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 11:40:15 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-30 11:40:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2517,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 102949,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 3, 40, 15, 178052),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'log_count/WARNING': 5,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 30, 3, 40, 10, 523786)}
2018-10-30 11:40:15 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 12:49:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-30 12:49:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 12:49:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-30 12:49:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 12:49:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 12:49:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 12:49:51 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-30 12:49:51 [scrapy.core.engine] INFO: Spider opened
2018-10-30 12:49:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 12:49:51 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-30 12:49:52 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-30 12:49:52 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ACDD8>
2018-10-30 12:49:53 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:49:53 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:49:53 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。MyToken币助手正式上线火币通用积分HT
2018-10-30 12:49:53 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 12:49:53 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。MyToken币助手正式上线火币通用积分HT
2018-10-30 12:49:53 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:49:53 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:49:53 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。
2018-10-30 12:49:53 [bsj_kuaixun] WARNING: 快讯推送失败 400
2018-10-30 12:49:53 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。
2018-10-30 12:49:54 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:49:54 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:49:54 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。《英雄联盟》11月3日开启S8赛季决赛，“IG对阵FNC”竞猜已登录菩提预测市场
2018-10-30 12:49:54 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 12:49:54 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。《英雄联盟》11月3日开启S8赛季决赛，“IG对阵FNC”竞猜已登录菩提预测市场
2018-10-30 12:49:54 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:49:54 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:49:54 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。某钓鱼工单系统持续对交易所用户进行攻击
2018-10-30 12:49:54 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 12:49:54 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。某钓鱼工单系统持续对交易所用户进行攻击
2018-10-30 12:49:55 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:49:55 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:49:55 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Bit-Z将于10月31日15：00上线WET
2018-10-30 12:49:55 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 12:49:55 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Bit-Z将于10月31日15：00上线WET
2018-10-30 12:49:55 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 12:49:55 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-30 12:49:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2555,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 102329,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 4, 49, 55, 566157),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'log_count/WARNING': 5,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 30, 4, 49, 51, 74900)}
2018-10-30 12:49:55 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 12:53:53 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-10-30 12:53:53 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 12:53:53 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-10-30 12:53:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 12:53:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 12:53:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 12:53:54 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-10-30 12:53:54 [scrapy.core.engine] INFO: Spider opened
2018-10-30 12:53:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 12:53:54 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-10-30 12:53:55 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-10-30 12:53:55 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057BCDD8>
2018-10-30 12:53:56 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:53:56 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:53:56 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。MyToken币助手正式上线火币通用积分HT
2018-10-30 12:53:56 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 12:53:56 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。MyToken币助手正式上线火币通用积分HT
2018-10-30 12:53:57 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:53:57 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:53:57 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。
2018-10-30 12:53:57 [bsj_kuaixun] WARNING: 快讯推送失败 400
2018-10-30 12:53:57 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。
2018-10-30 12:53:57 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:53:57 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:53:57 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。《英雄联盟》11月3日开启S8赛季决赛，“IG对阵FNC”竞猜已登录菩提预测市场
2018-10-30 12:53:57 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 12:53:57 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。《英雄联盟》11月3日开启S8赛季决赛，“IG对阵FNC”竞猜已登录菩提预测市场
2018-10-30 12:53:58 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:53:58 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:53:58 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。某钓鱼工单系统持续对交易所用户进行攻击
2018-10-30 12:53:58 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 12:53:58 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。某钓鱼工单系统持续对交易所用户进行攻击
2018-10-30 12:53:58 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-10-30 12:53:58 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-10-30 12:53:58 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。Bit-Z将于10月31日15：00上线WET
2018-10-30 12:53:58 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-10-30 12:53:58 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。Bit-Z将于10月31日15：00上线WET
2018-10-30 12:53:58 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 12:53:58 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-10-30 12:53:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2435,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 102328,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 4, 53, 58, 986080),
 'item_scraped_count': 5,
 'log_count/INFO': 31,
 'log_count/WARNING': 5,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 10, 30, 4, 53, 54, 612829)}
2018-10-30 12:53:58 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-01 09:16:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-01 09:16:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-01 09:16:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-01 09:16:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-01 09:16:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-01 09:16:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-01 09:16:12 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-01 09:16:12 [scrapy.core.engine] INFO: Spider opened
2018-11-01 09:16:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-01 09:16:12 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-11-01 09:16:16 [bsj_kuaixun] WARNING: Redis服务连接失败
2018-11-01 09:16:16 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ADCC0>
2018-11-01 09:16:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <301 http://www.bishijie.com/kuaixun/>: HTTP status code is not handled or not allowed
2018-11-01 09:16:17 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-01 09:16:17 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-11-01 09:16:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 259,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 520,
 'downloader/response_count': 1,
 'downloader/response_status_count/301': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 1, 1, 16, 17, 119941),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/301': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 11, 1, 1, 16, 12, 798734)}
2018-11-01 09:16:17 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-01 09:55:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-01 09:55:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-01 09:55:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-01 09:55:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-01 09:55:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-01 09:55:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-01 09:55:44 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-01 09:55:44 [scrapy.core.engine] INFO: Spider opened
2018-11-01 09:55:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-01 09:55:44 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-11-01 09:55:45 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-01 09:55:45 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057AFDA0>
2018-11-01 09:55:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <301 http://www.bishijie.com/kuaixun/>: HTTP status code is not handled or not allowed
2018-11-01 09:55:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-01 09:55:46 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-11-01 09:55:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 271,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 520,
 'downloader/response_count': 1,
 'downloader/response_status_count/301': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 1, 1, 55, 46, 198405),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/301': 1,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 11, 1, 1, 55, 44, 898202)}
2018-11-01 09:55:46 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-01 09:57:19 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-01 09:57:19 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-01 09:57:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-01 09:57:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-01 09:57:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-01 09:57:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-01 09:57:20 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-01 09:57:20 [scrapy.core.engine] INFO: Spider opened
2018-11-01 09:57:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-01 09:57:20 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-11-01 09:57:21 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-01 09:57:21 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057AFDD8>
2018-11-01 09:57:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <301 http://www.bishijie.com/kuaixun/>: HTTP status code is not handled or not allowed
2018-11-01 09:57:21 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-01 09:57:21 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-11-01 09:57:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 223,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 520,
 'downloader/response_count': 1,
 'downloader/response_status_count/301': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 1, 1, 57, 21, 432512),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/301': 1,
 'log_count/INFO': 12,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 11, 1, 1, 57, 20, 24703)}
2018-11-01 09:57:21 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-01 09:59:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-01 09:59:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-01 09:59:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-01 09:59:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-01 09:59:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-01 09:59:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-01 09:59:17 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-01 09:59:17 [scrapy.core.engine] INFO: Spider opened
2018-11-01 09:59:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-01 09:59:17 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-11-01 09:59:18 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-01 09:59:18 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057AFDD8>
2018-11-01 09:59:20 [bsj_kuaixun] INFO: 开启推送而且bsj_kuaixun爬虫在推送列表。。。
2018-11-01 09:59:20 [bsj_kuaixun] INFO: 时间段检查通过。。。
2018-11-01 09:59:20 [bsj_kuaixun] INFO: bsj_kuaixun正在推送消息。。。数据显示：昨日总供给量最高的 USD 稳定币是 TUSD
2018-11-01 09:59:20 [bsj_kuaixun] WARNING: 快讯推送成功 200
2018-11-01 09:59:20 [bsj_kuaixun] INFO: bsj_kuaixun推送完毕。。。数据显示：昨日总供给量最高的 USD 稳定币是 TUSD
2018-11-01 09:59:22 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-01 09:59:22 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-11-01 09:59:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2153,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 101118,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 6,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 1, 1, 59, 22, 611619),
 'item_scraped_count': 5,
 'log_count/INFO': 15,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 6,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2018, 11, 1, 1, 59, 17, 948793)}
2018-11-01 09:59:22 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-01 09:59:45 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-01 09:59:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-01 09:59:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-01 09:59:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-01 09:59:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-01 09:59:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-01 09:59:46 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-01 09:59:46 [scrapy.core.engine] INFO: Spider opened
2018-11-01 09:59:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-01 09:59:46 [bsj_kuaixun] INFO: Spider opened: bsj_kuaixun
2018-11-01 09:59:47 [bsj_kuaixun] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-01 09:59:47 [bsj_kuaixun] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057AFDD8>
2018-11-01 09:59:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-01 09:59:48 [bsj_kuaixun] INFO: bsj_kuaixun关闭mysql数据库连接
2018-11-01 09:59:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 252,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42619,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 1, 1, 59, 48, 605571),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 11, 1, 1, 59, 46, 907759)}
2018-11-01 09:59:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-07 11:06:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-07 11:06:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-07 11:06:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-07 11:06:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-07 11:06:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-07 11:06:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-07 11:06:32 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-07 11:06:32 [scrapy.core.engine] INFO: Spider opened
2018-11-07 11:06:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-07 11:06:32 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-11-07 11:06:33 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-07 11:06:33 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057C7A20>
2018-11-07 11:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/303444>: HTTP status code is not handled or not allowed
2018-11-07 11:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/303587>: HTTP status code is not handled or not allowed
2018-11-07 11:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/303873>: HTTP status code is not handled or not allowed
2018-11-07 11:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302797>: HTTP status code is not handled or not allowed
2018-11-07 11:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302831>: HTTP status code is not handled or not allowed
2018-11-07 11:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302914>: HTTP status code is not handled or not allowed
2018-11-07 11:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/303052>: HTTP status code is not handled or not allowed
2018-11-07 11:06:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302326>: HTTP status code is not handled or not allowed
2018-11-07 11:06:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302433>: HTTP status code is not handled or not allowed
2018-11-07 11:06:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302542>: HTTP status code is not handled or not allowed
2018-11-07 11:06:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302535>: HTTP status code is not handled or not allowed
2018-11-07 11:06:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302705>: HTTP status code is not handled or not allowed
2018-11-07 11:06:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302541>: HTTP status code is not handled or not allowed
2018-11-07 11:06:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302140>: HTTP status code is not handled or not allowed
2018-11-07 11:06:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302173>: HTTP status code is not handled or not allowed
2018-11-07 11:06:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302131>: HTTP status code is not handled or not allowed
2018-11-07 11:06:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302085>: HTTP status code is not handled or not allowed
2018-11-07 11:06:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/300252>: HTTP status code is not handled or not allowed
2018-11-07 11:06:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/300227>: HTTP status code is not handled or not allowed
2018-11-07 11:06:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/300674>: HTTP status code is not handled or not allowed
2018-11-07 11:06:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-07 11:06:48 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-11-07 11:06:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7376,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 19256,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 20,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 7, 3, 6, 48, 911660),
 'httperror/response_ignored_count': 20,
 'httperror/response_ignored_status_count/302': 20,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 11, 7, 3, 6, 32, 35978)}
2018-11-07 11:06:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-07 11:09:37 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-07 11:09:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-07 11:09:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-07 11:09:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-07 11:09:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-07 11:09:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-07 11:09:38 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-07 11:09:38 [scrapy.core.engine] INFO: Spider opened
2018-11-07 11:09:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-07 11:09:38 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-11-07 11:09:39 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-07 11:09:39 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B99E8>
2018-11-07 11:09:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302797>: HTTP status code is not handled or not allowed
2018-11-07 11:09:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302831>: HTTP status code is not handled or not allowed
2018-11-07 11:09:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302914>: HTTP status code is not handled or not allowed
2018-11-07 11:09:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302541>: HTTP status code is not handled or not allowed
2018-11-07 11:09:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/303052>: HTTP status code is not handled or not allowed
2018-11-07 11:09:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/303444>: HTTP status code is not handled or not allowed
2018-11-07 11:09:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/303587>: HTTP status code is not handled or not allowed
2018-11-07 11:09:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/303873>: HTTP status code is not handled or not allowed
2018-11-07 11:09:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302326>: HTTP status code is not handled or not allowed
2018-11-07 11:09:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302433>: HTTP status code is not handled or not allowed
2018-11-07 11:09:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302542>: HTTP status code is not handled or not allowed
2018-11-07 11:09:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302535>: HTTP status code is not handled or not allowed
2018-11-07 11:09:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302705>: HTTP status code is not handled or not allowed
2018-11-07 11:09:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/300252>: HTTP status code is not handled or not allowed
2018-11-07 11:09:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/300227>: HTTP status code is not handled or not allowed
2018-11-07 11:09:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/300674>: HTTP status code is not handled or not allowed
2018-11-07 11:09:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302085>: HTTP status code is not handled or not allowed
2018-11-07 11:09:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302140>: HTTP status code is not handled or not allowed
2018-11-07 11:09:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302173>: HTTP status code is not handled or not allowed
2018-11-07 11:09:52 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <302 https://m.8btc.com/article/302131>: HTTP status code is not handled or not allowed
2018-11-07 11:09:52 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-07 11:09:52 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-11-07 11:09:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7264,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 19256,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 20,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 7, 3, 9, 52, 880109),
 'httperror/response_ignored_count': 20,
 'httperror/response_ignored_status_count/302': 20,
 'log_count/INFO': 31,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 11, 7, 3, 9, 38, 760399)}
2018-11-07 11:09:52 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-07 11:11:05 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-07 11:11:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-07 11:11:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-07 11:11:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-07 11:11:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-07 11:11:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-07 11:11:06 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-07 11:11:06 [scrapy.core.engine] INFO: Spider opened
2018-11-07 11:11:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-07 11:11:06 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-11-07 11:11:07 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-07 11:11:07 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B6A20>
2018-11-07 11:11:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-07 11:11:34 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-11-07 11:11:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 14425,
 'downloader/request_count': 41,
 'downloader/request_method_count/GET': 41,
 'downloader/response_bytes': 872690,
 'downloader/response_count': 41,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/302': 20,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 7, 3, 11, 34, 143859),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2018, 11, 7, 3, 11, 6, 441076)}
2018-11-07 11:11:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-07 11:13:06 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-07 11:13:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-07 11:13:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-07 11:13:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-07 11:13:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-07 11:13:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-07 11:13:07 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-07 11:13:07 [scrapy.core.engine] INFO: Spider opened
2018-11-07 11:13:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-07 11:13:07 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-11-07 11:13:08 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-07 11:13:08 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B7A58>
2018-11-07 11:13:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-07 11:13:34 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-11-07 11:13:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 14396,
 'downloader/request_count': 41,
 'downloader/request_method_count/GET': 41,
 'downloader/response_bytes': 873247,
 'downloader/response_count': 41,
 'downloader/response_status_count/200': 21,
 'downloader/response_status_count/302': 20,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 7, 3, 13, 34, 682873),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2018, 11, 7, 3, 13, 7, 473507)}
2018-11-07 11:13:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-11-07 11:18:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2018-11-07 11:18:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-11-07 11:18:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'SPIDER_MODULES': ['test_scrapy.spiders']}
2018-11-07 11:18:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-11-07 11:18:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-11-07 11:18:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-11-07 11:18:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2018-11-07 11:18:29 [scrapy.core.engine] INFO: Spider opened
2018-11-07 11:18:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-11-07 11:18:29 [btc_policy_new] INFO: Spider opened: btc_policy_new
2018-11-07 11:18:30 [btc_policy_new] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2018-11-07 11:18:30 [btc_policy_new] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057B8A90>
2018-11-07 11:18:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-11-07 11:18:46 [btc_policy_new] INFO: btc_policy_new关闭mysql数据库连接
2018-11-07 11:18:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 8587,
 'downloader/request_count': 21,
 'downloader/request_method_count/GET': 21,
 'downloader/response_bytes': 315548,
 'downloader/response_count': 21,
 'downloader/response_status_count/200': 21,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 11, 7, 3, 18, 46, 198093),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 21,
 'scheduler/dequeued/memory': 21,
 'scheduler/enqueued': 21,
 'scheduler/enqueued/memory': 21,
 'start_time': datetime.datetime(2018, 11, 7, 3, 18, 29, 176997)}
2018-11-07 11:18:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 13:46:52 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 13:46:52 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 13:46:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 13:46:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 13:46:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 13:46:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 13:46:53 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 13:46:53 [scrapy.core.engine] INFO: Spider opened
2019-01-10 13:46:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 13:46:53 [trading_view] INFO: Spider opened: trading_view
2019-01-10 13:46:54 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 13:46:54 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB38>
2019-01-10 13:46:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 13:46:57 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 13:46:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 518,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 145668,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 5, 46, 57, 807693),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 1, 10, 5, 46, 53, 875278)}
2019-01-10 13:46:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:39:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:39:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:39:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:39:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:39:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:39:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:39:30 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:39:30 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:39:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:39:30 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:39:31 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:39:31 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBB38>
2019-01-10 15:39:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:39:34 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:39:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 518,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 145693,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 39, 34, 803917),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 1, 10, 7, 39, 30, 711710)}
2019-01-10 15:39:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:43:27 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:43:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:43:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:43:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:43:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:43:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:43:28 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:43:28 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:43:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:43:28 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:43:29 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:43:29 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB38>
2019-01-10 15:43:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:43:32 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:43:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 518,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 145693,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 43, 32, 966445),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 1, 10, 7, 43, 28, 468636)}
2019-01-10 15:43:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:43:46 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:43:46 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:43:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:43:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:43:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:43:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:43:47 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:43:47 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:43:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:43:47 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:43:48 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:43:48 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBB38>
2019-01-10 15:43:50 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:43:50 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:43:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 518,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 145715,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 43, 50, 607282),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 2,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 1, 10, 7, 43, 47, 779875)}
2019-01-10 15:43:50 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:46:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:46:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:46:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:46:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:46:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:46:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:46:41 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:46:41 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:46:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:46:41 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:46:42 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:46:42 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB38>
2019-01-10 15:46:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:46:45 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:46:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234310,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 46, 45, 788935),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 46, 41, 148727)}
2019-01-10 15:46:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:48:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:48:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:48:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:48:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:48:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:48:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:48:41 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:48:41 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:48:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:48:41 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:48:42 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:48:42 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EBB38>
2019-01-10 15:48:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:48:46 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:48:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234305,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 48, 46, 141977),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 48, 41, 253168)}
2019-01-10 15:48:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:48:59 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:48:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:48:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:48:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:49:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:49:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:49:00 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:49:00 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:49:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:49:00 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:49:01 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:49:01 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB38>
2019-01-10 15:49:04 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:49:04 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:49:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234326,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 49, 4, 119213),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 49, 0, 405407)}
2019-01-10 15:49:04 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:49:12 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:49:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:49:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:49:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:49:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:49:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:49:13 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:49:13 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:49:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:49:13 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:49:14 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:49:14 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBB38>
2019-01-10 15:49:24 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:49:24 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:49:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234338,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 49, 24, 527652),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 49, 13, 778833)}
2019-01-10 15:49:24 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:49:54 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:49:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:49:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:49:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:49:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:49:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:49:55 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:49:55 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:49:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:49:55 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:49:56 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:49:56 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDB00>
2019-01-10 15:50:00 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:50:00 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:50:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234303,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 50, 0, 466857),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 49, 55, 481047)}
2019-01-10 15:50:00 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:50:42 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:50:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:50:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:50:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:50:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:50:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:50:43 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:50:43 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:50:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:50:43 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:50:44 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:50:44 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ECB00>
2019-01-10 15:50:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cn.tradingview.com/u/TouFrancis/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\trading_view.py", line 39, in parse
    symbol = article.css('.tv-widget-idea__symbol::text()').extract_first()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 252, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 255, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 192, in <genexpr>
    for selector in parse(css))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 222, in selector_to_xpath
    xpath = self.xpath_pseudo_element(xpath, selector.pseudo_element)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\csstranslator.py", line 65, in xpath_pseudo_element
    % pseudo_element.name)
cssselect.xpath.ExpressionError: The functional pseudo-element ::text() is unknown
2019-01-10 15:50:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:50:46 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:50:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65276,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 50, 46, 813546),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ExpressionError': 1,
 'start_time': datetime.datetime(2019, 1, 10, 7, 50, 43, 900940)}
2019-01-10 15:50:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:50:54 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:50:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:50:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:50:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:50:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:50:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:50:55 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:50:55 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:50:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:50:55 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:50:56 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:50:56 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBC50>
2019-01-10 15:50:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cn.tradingview.com/u/TouFrancis/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\trading_view.py", line 39, in parse
    symbol = article.css('.tv-widget-idea__symbol::text()').extract_first()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 252, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 255, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 192, in <genexpr>
    for selector in parse(css))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 222, in selector_to_xpath
    xpath = self.xpath_pseudo_element(xpath, selector.pseudo_element)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\csstranslator.py", line 65, in xpath_pseudo_element
    % pseudo_element.name)
cssselect.xpath.ExpressionError: The functional pseudo-element ::text() is unknown
2019-01-10 15:50:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:50:57 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:50:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65290,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 50, 57, 465166),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ExpressionError': 1,
 'start_time': datetime.datetime(2019, 1, 10, 7, 50, 55, 358162)}
2019-01-10 15:50:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:51:38 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:51:38 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:51:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:51:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:51:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:51:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:51:39 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:51:39 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:51:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:51:39 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:51:40 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:51:40 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ECC50>
2019-01-10 15:51:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cn.tradingview.com/u/TouFrancis/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\trading_view.py", line 39, in parse
    symbol = article.css('.tv-widget-idea__symbol::text()').extract_first()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 252, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 255, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 192, in <genexpr>
    for selector in parse(css))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 222, in selector_to_xpath
    xpath = self.xpath_pseudo_element(xpath, selector.pseudo_element)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\csstranslator.py", line 65, in xpath_pseudo_element
    % pseudo_element.name)
cssselect.xpath.ExpressionError: The functional pseudo-element ::text() is unknown
2019-01-10 15:51:44 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:51:44 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:51:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65272,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 51, 44, 368051),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ExpressionError': 1,
 'start_time': datetime.datetime(2019, 1, 10, 7, 51, 39, 279442)}
2019-01-10 15:51:44 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:51:49 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:51:49 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:51:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:51:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:51:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:51:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:51:50 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:51:50 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:51:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:51:50 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:51:51 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:51:51 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDB00>
2019-01-10 15:51:55 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:51:55 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:51:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234299,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 51, 55, 211475),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 51, 50, 968867)}
2019-01-10 15:51:55 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:54:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:54:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:54:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:54:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:54:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:54:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:54:51 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:54:51 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:54:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:54:51 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:54:52 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:54:52 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB70>
2019-01-10 15:55:29 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:55:29 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:55:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 1036,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 234303,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 55, 29, 96373),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 10, 7, 54, 51, 769305)}
2019-01-10 15:55:29 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:55:57 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:55:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:55:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:55:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:55:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:55:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:55:57 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:55:57 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:55:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:55:57 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:55:58 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:55:58 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ECB00>
2019-01-10 15:56:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:56:02 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:56:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234336,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 56, 2, 968847),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 55, 57, 894838)}
2019-01-10 15:56:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:57:39 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:57:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:57:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:57:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:57:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:57:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:57:40 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:57:40 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:57:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:57:40 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:57:41 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:57:41 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EBB38>
2019-01-10 15:57:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:57:46 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:57:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234306,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 57, 46, 406774),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 57, 40, 79361)}
2019-01-10 15:57:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:58:03 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:58:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:58:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:58:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:58:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:58:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:58:04 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:58:04 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:58:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:58:04 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:58:05 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:58:05 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCAC8>
2019-01-10 15:58:09 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:58:09 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:58:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234325,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 58, 9, 462622),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 58, 4, 12213)}
2019-01-10 15:58:09 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:58:51 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:58:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:58:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:58:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:58:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:58:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:58:52 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:58:52 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:58:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:58:52 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:58:53 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:58:53 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DAB70>
2019-01-10 15:58:58 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:58:58 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:58:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234352,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 58, 58, 608924),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 58, 52, 519111)}
2019-01-10 15:58:58 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 15:59:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 15:59:14 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 15:59:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 15:59:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 15:59:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 15:59:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 15:59:15 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 15:59:15 [scrapy.core.engine] INFO: Spider opened
2019-01-10 15:59:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 15:59:15 [trading_view] INFO: Spider opened: trading_view
2019-01-10 15:59:16 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 15:59:16 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB38>
2019-01-10 15:59:22 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 15:59:22 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 15:59:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234312,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 7, 59, 22, 229973),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 7, 59, 15, 889362)}
2019-01-10 15:59:22 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:00:51 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:00:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:00:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:00:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:00:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:00:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:00:52 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:00:52 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:00:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:00:52 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:00:53 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:00:53 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DAB38>
2019-01-10 16:00:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cn.tradingview.com/u/TouFrancis/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid expression

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\trading_view.py", line 40, in parse
    idea = article.xpath('span .tv-card-label').extract_first()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 232, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 228, in xpath
    **kwargs)
  File "src/lxml/etree.pyx", line 1577, in lxml.etree._Element.xpath
  File "src/lxml/xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src/lxml/xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid expression in span .tv-card-label
2019-01-10 16:00:55 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:00:55 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:00:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65267,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 0, 55, 765203),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2019, 1, 10, 8, 0, 52, 354397)}
2019-01-10 16:00:55 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:01:08 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:01:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:01:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:01:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:01:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:01:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:01:09 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:01:09 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:01:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:01:09 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:01:10 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:01:10 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBB38>
2019-01-10 16:01:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:01:16 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:01:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234292,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 1, 16, 609654),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 1, 9, 688842)}
2019-01-10 16:01:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:01:36 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:01:36 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:01:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:01:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:01:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:01:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:01:37 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:01:37 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:01:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:01:37 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:01:38 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:01:38 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB70>
2019-01-10 16:01:43 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:01:43 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:01:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234303,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 1, 43, 255915),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 1, 37, 120104)}
2019-01-10 16:01:43 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:01:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:01:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:01:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:01:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:01:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:01:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:01:56 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:01:56 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:01:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:01:56 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:01:57 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:01:57 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDB00>
2019-01-10 16:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cn.tradingview.com/u/TouFrancis/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "D:\liuluyang\test_scrapy\test_scrapy\middlewares.py", line 40, in process_spider_output
    for i in result:
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\trading_view.py", line 40, in parse
    idea = article.css('.tv-card-label::text()').extract_first()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 252, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\selector.py", line 255, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 192, in <genexpr>
    for selector in parse(css))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\cssselect\xpath.py", line 222, in selector_to_xpath
    xpath = self.xpath_pseudo_element(xpath, selector.pseudo_element)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\parsel\csstranslator.py", line 65, in xpath_pseudo_element
    % pseudo_element.name)
cssselect.xpath.ExpressionError: The functional pseudo-element ::text() is unknown
2019-01-10 16:02:00 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:02:00 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:02:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65265,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 2, 0, 35755),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ExpressionError': 1,
 'start_time': datetime.datetime(2019, 1, 10, 8, 1, 56, 281748)}
2019-01-10 16:02:00 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:02:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:02:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:02:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:02:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:02:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:02:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:02:12 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:02:12 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:02:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:02:12 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:02:13 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:02:13 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ECB38>
2019-01-10 16:02:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:02:16 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:02:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234303,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 2, 16, 688592),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 2, 12, 602583)}
2019-01-10 16:02:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:03:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:03:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:03:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:03:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:03:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:03:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:03:12 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:03:12 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:03:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:03:12 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:03:13 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:03:13 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ECB38>
2019-01-10 16:03:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:03:17 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:03:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234278,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 3, 17, 255123),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 3, 12, 652714)}
2019-01-10 16:03:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:03:34 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:03:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:03:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:03:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:03:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:03:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:03:34 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:03:34 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:03:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:03:34 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:03:35 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:03:35 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBB38>
2019-01-10 16:03:40 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:03:40 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:03:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234300,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 3, 40, 430302),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 3, 34, 951890)}
2019-01-10 16:03:40 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:06:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:06:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:06:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:06:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:06:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:06:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:06:41 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:06:41 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:06:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:06:41 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:06:42 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:06:42 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDB70>
2019-01-10 16:06:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:06:47 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:06:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234285,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 6, 47, 104592),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 6, 41, 933982)}
2019-01-10 16:06:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:08:40 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:08:40 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:08:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:08:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:08:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:08:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:08:41 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:08:41 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:08:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:08:41 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:08:42 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:08:42 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ECB70>
2019-01-10 16:08:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:08:46 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:08:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234284,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 8, 46, 253525),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 8, 41, 138915)}
2019-01-10 16:08:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:10:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:10:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:10:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:10:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:10:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:10:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:10:44 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:10:44 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:10:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:10:44 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:10:45 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:10:45 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB38>
2019-01-10 16:10:49 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:10:49 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:10:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234283,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 10, 49, 444130),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 10, 44, 664918)}
2019-01-10 16:10:49 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:11:20 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:11:20 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:11:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:11:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:11:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:11:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:11:21 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:11:21 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:11:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:11:21 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:11:22 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:11:22 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDB70>
2019-01-10 16:11:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:11:25 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:11:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234316,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 11, 25, 433604),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 11, 21, 495397)}
2019-01-10 16:11:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:12:30 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:12:30 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:12:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:12:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:12:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:12:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:12:31 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:12:31 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:12:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:12:31 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:12:32 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:12:32 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EDB70>
2019-01-10 16:12:36 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:12:36 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:12:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234291,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 12, 36, 812958),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 12, 31, 790348)}
2019-01-10 16:12:36 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:13:48 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:13:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:13:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:13:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:13:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:13:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:13:49 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:13:49 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:13:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:13:49 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:13:50 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:13:50 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB38>
2019-01-10 16:13:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:13:53 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:13:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234286,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 13, 53, 722907),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 13, 49, 120498)}
2019-01-10 16:13:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:16:03 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:16:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:16:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:16:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:16:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:16:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:16:04 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:16:04 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:16:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:16:04 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:16:05 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:16:05 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBB70>
2019-01-10 16:16:13 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:16:13 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:16:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234292,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 16, 13, 938382),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 16, 4, 663966)}
2019-01-10 16:16:13 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:21:55 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:21:55 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:21:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:21:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:21:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:21:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:21:56 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:21:56 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:21:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:21:56 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:21:57 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:21:57 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DEB00>
2019-01-10 16:22:01 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:22:01 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:22:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234305,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 22, 1, 857542),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 21, 56, 701532)}
2019-01-10 16:22:01 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:23:00 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:23:00 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:23:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:23:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:23:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:23:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:23:01 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:23:01 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:23:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:23:01 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:23:02 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:23:02 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB70>
2019-01-10 16:23:07 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:23:07 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:23:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234290,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 23, 7, 438371),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 23, 1, 60559)}
2019-01-10 16:23:07 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:23:27 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:23:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:23:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:23:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:23:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:23:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:23:28 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:23:28 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:23:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:23:28 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:23:29 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:23:29 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DEAC8>
2019-01-10 16:23:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:23:34 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:23:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234297,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 23, 34, 54832),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 23, 28, 881223)}
2019-01-10 16:23:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:23:58 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:23:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:23:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:23:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:23:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:23:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:23:59 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:23:59 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:23:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:23:59 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:24:00 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:24:00 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDB70>
2019-01-10 16:24:06 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:24:06 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:24:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 806,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 234294,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 24, 6, 203303),
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 1, 10, 8, 23, 59, 761690)}
2019-01-10 16:24:06 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:29:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:29:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:29:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:29:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:29:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:29:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:29:51 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:29:51 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:29:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:29:51 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:29:52 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:29:52 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDA90>
2019-01-10 16:29:55 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:29:55 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:29:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65265,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 29, 55, 201060),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 1, 10, 8, 29, 51, 384653)}
2019-01-10 16:29:55 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-10 16:30:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-10 16:30:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-10 16:30:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-10 16:30:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-10 16:30:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-10 16:30:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-10 16:30:45 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-10 16:30:45 [scrapy.core.engine] INFO: Spider opened
2019-01-10 16:30:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-10 16:30:45 [trading_view] INFO: Spider opened: trading_view
2019-01-10 16:30:46 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-10 16:30:46 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCB38>
2019-01-10 16:30:49 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-10 16:30:49 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-10 16:30:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65262,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 10, 8, 30, 49, 116375),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 1, 10, 8, 30, 45, 177366)}
2019-01-10 16:30:49 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-11 09:30:21 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 09:30:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 09:30:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 09:30:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 09:30:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 09:30:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 09:30:22 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 09:30:22 [scrapy.core.engine] INFO: Spider opened
2019-01-11 09:30:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 09:30:22 [trading_view] INFO: Spider opened: trading_view
2019-01-11 09:30:26 [trading_view] WARNING: Redis服务连接失败
2019-01-11 09:30:26 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDB38>
2019-01-11 09:30:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cn.tradingview.com/u/TouFrancis/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\trading_view.py", line 26, in parse
    if self.redis.sismember('TradingView_idea', url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2019-01-11 09:30:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-11 09:30:32 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 09:30:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65239,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 11, 1, 30, 32, 740876),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ConnectionError': 1,
 'start_time': datetime.datetime(2019, 1, 11, 1, 30, 22, 463914)}
2019-01-11 09:30:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-11 09:30:41 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 09:30:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 09:30:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 09:30:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 09:30:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 09:30:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 09:30:42 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 09:30:42 [scrapy.core.engine] INFO: Spider opened
2019-01-11 09:30:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 09:30:42 [trading_view] INFO: Spider opened: trading_view
2019-01-11 09:30:46 [trading_view] WARNING: Redis服务连接失败
2019-01-11 09:30:46 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDB00>
2019-01-11 09:30:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cn.tradingview.com/u/TouFrancis/> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 667, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 484, in connect
    sock = self._connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 541, in _connect
    raise err
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 529, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\spiders\trading_view.py", line 26, in parse
    if self.redis.sismember('TradingView_idea', url):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 1634, in sismember
    return self.execute_command('SISMEMBER', name, value)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\client.py", line 673, in execute_command
    connection.send_command(*args)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 610, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 585, in send_packed_command
    self.connect()
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\redis\connection.py", line 489, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 10061 connecting to localhost:6379. 由于目标计算机积极拒绝，无法连接。.
2019-01-11 09:30:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-11 09:30:52 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 09:30:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65230,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 11, 1, 30, 52, 669907),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ConnectionError': 1,
 'start_time': datetime.datetime(2019, 1, 11, 1, 30, 42, 404247)}
2019-01-11 09:30:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-11 09:31:21 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 09:31:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 09:31:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 09:31:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 09:31:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 09:31:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 09:31:22 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 09:31:22 [scrapy.core.engine] INFO: Spider opened
2019-01-11 09:31:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 09:31:22 [trading_view] INFO: Spider opened: trading_view
2019-01-11 09:31:23 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 09:31:23 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057ECBA8>
2019-01-11 09:31:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-11 09:31:25 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 09:31:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 65230,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 11, 1, 31, 25, 767621),
 'log_count/INFO': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 1, 11, 1, 31, 22, 223893)}
2019-01-11 09:31:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-11 16:25:37 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 16:25:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 16:25:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 16:25:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 16:25:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 16:25:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 16:25:38 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 16:25:38 [scrapy.core.engine] INFO: Spider opened
2019-01-11 16:25:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 16:25:38 [trading_view] INFO: Spider opened: trading_view
2019-01-11 16:25:39 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 16:25:39 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EBDD8>
2019-01-11 16:25:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-11 16:25:47 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 16:25:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3120,
 'downloader/request_count': 11,
 'downloader/request_method_count/GET': 11,
 'downloader/response_bytes': 1282888,
 'downloader/response_count': 11,
 'downloader/response_status_count/200': 11,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 11, 8, 25, 47, 740430),
 'item_scraped_count': 10,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 11,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 1, 11, 8, 25, 38, 193797)}
2019-01-11 16:25:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-11 16:29:25 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 16:29:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 16:29:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 16:29:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 16:29:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 16:29:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 16:29:26 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 16:29:26 [scrapy.core.engine] INFO: Spider opened
2019-01-11 16:29:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 16:29:26 [trading_view] INFO: Spider opened: trading_view
2019-01-11 16:29:27 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 16:29:27 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBF28>
2019-01-11 16:29:36 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-11 16:29:36 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 16:29:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3120,
 'downloader/request_count': 11,
 'downloader/request_method_count/GET': 11,
 'downloader/response_bytes': 1282789,
 'downloader/response_count': 11,
 'downloader/response_status_count/200': 11,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 11, 8, 29, 36, 856507),
 'item_scraped_count': 10,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 11,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 1, 11, 8, 29, 26, 847934)}
2019-01-11 16:29:36 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-11 16:55:08 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 16:55:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 16:55:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 16:55:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 16:55:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 16:55:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 16:55:09 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 16:55:09 [scrapy.core.engine] INFO: Spider opened
2019-01-11 16:55:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 16:55:09 [trading_view] INFO: Spider opened: trading_view
2019-01-11 16:55:10 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 16:55:10 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDDA0>
2019-01-11 16:55:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-11 16:55:20 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 16:55:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3120,
 'downloader/request_count': 11,
 'downloader/request_method_count/GET': 11,
 'downloader/response_bytes': 1282487,
 'downloader/response_count': 11,
 'downloader/response_status_count/200': 11,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 11, 8, 55, 20, 837869),
 'item_scraped_count': 10,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 11,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2019, 1, 11, 8, 55, 9, 902935)}
2019-01-11 16:55:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-11 16:59:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 16:59:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 16:59:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 16:59:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 16:59:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 16:59:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 16:59:45 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 16:59:45 [scrapy.core.engine] INFO: Spider opened
2019-01-11 16:59:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 16:59:45 [trading_view] INFO: Spider opened: trading_view
2019-01-11 16:59:46 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 16:59:46 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DEDA0>
2019-01-11 16:59:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-11 16:59:56 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 16:59:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2548,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 782417,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 11, 8, 59, 56, 967823),
 'item_scraped_count': 8,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 1, 11, 8, 59, 45, 587530)}
2019-01-11 16:59:56 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-11 17:04:27 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 17:04:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 17:04:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 17:04:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 17:04:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 17:04:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 17:04:28 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 17:04:28 [scrapy.core.engine] INFO: Spider opened
2019-01-11 17:04:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 17:04:28 [trading_view] INFO: Spider opened: trading_view
2019-01-11 17:04:29 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 17:04:29 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EDDD8>
2019-01-11 17:04:40 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-01-11 17:04:40 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-01-11 17:04:43 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 17:04:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5438,
 'downloader/request_count': 19,
 'downloader/request_method_count/GET': 19,
 'downloader/response_bytes': 1998247,
 'downloader/response_count': 19,
 'downloader/response_status_count/200': 19,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2019, 1, 11, 9, 4, 43, 198715),
 'log_count/INFO': 12,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 19,
 'scheduler/dequeued/memory': 19,
 'scheduler/enqueued': 19,
 'scheduler/enqueued/memory': 19,
 'start_time': datetime.datetime(2019, 1, 11, 9, 4, 28, 845894)}
2019-01-11 17:04:43 [scrapy.core.engine] INFO: Spider closed (shutdown)
2019-01-11 17:11:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 17:11:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 17:11:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 17:11:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 17:11:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 17:11:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 17:11:32 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 17:11:32 [scrapy.core.engine] INFO: Spider opened
2019-01-11 17:11:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 17:11:32 [trading_view] INFO: Spider opened: trading_view
2019-01-11 17:11:33 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 17:11:33 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EDDD8>
2019-01-11 17:11:44 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-01-11 17:11:44 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-01-11 17:11:44 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-01-11 17:12:17 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 17:12:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 17:12:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 17:12:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 17:12:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 17:12:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 17:12:18 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 17:12:18 [scrapy.core.engine] INFO: Spider opened
2019-01-11 17:12:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 17:12:18 [trading_view] INFO: Spider opened: trading_view
2019-01-11 17:12:19 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 17:12:19 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EDE10>
2019-01-11 17:12:28 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-01-11 17:12:28 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-01-11 17:12:29 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-01-11 17:12:29 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://cn.tradingview.com/chart/USOIL/ZXFyAROL/. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2019-01-11 17:13:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-11 17:13:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-11 17:13:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-11 17:13:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-11 17:13:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-11 17:13:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-11 17:13:36 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-11 17:13:36 [scrapy.core.engine] INFO: Spider opened
2019-01-11 17:13:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-11 17:13:36 [trading_view] INFO: Spider opened: trading_view
2019-01-11 17:13:37 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-11 17:13:37 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DED68>
2019-01-11 17:13:50 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-11 17:13:50 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-11 17:13:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5438,
 'downloader/request_count': 19,
 'downloader/request_method_count/GET': 19,
 'downloader/response_bytes': 1998412,
 'downloader/response_count': 19,
 'downloader/response_status_count/200': 19,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 11, 9, 13, 50, 581358),
 'item_scraped_count': 18,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 19,
 'scheduler/dequeued/memory': 19,
 'scheduler/enqueued': 19,
 'scheduler/enqueued/memory': 19,
 'start_time': datetime.datetime(2019, 1, 11, 9, 13, 36, 306141)}
2019-01-11 17:13:50 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 09:39:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 09:39:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 09:39:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 09:39:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 09:39:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 09:39:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 09:39:12 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 09:39:12 [scrapy.core.engine] INFO: Spider opened
2019-01-21 09:39:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 09:39:12 [trading_view] INFO: Spider opened: trading_view
2019-01-21 09:39:13 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 09:39:13 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCF60>
2019-01-21 09:39:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 09:39:52 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 09:39:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17345,
 'downloader/request_count': 61,
 'downloader/request_method_count/GET': 61,
 'downloader/response_bytes': 4681181,
 'downloader/response_count': 61,
 'downloader/response_status_count/200': 61,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 1, 39, 52, 263097),
 'item_scraped_count': 57,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 61,
 'scheduler/dequeued': 61,
 'scheduler/dequeued/memory': 61,
 'scheduler/enqueued': 61,
 'scheduler/enqueued/memory': 61,
 'start_time': datetime.datetime(2019, 1, 21, 1, 39, 12, 414622)}
2019-01-21 09:39:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 09:42:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 09:42:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 09:42:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 09:42:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 09:42:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 09:42:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 09:42:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 09:42:29 [scrapy.core.engine] INFO: Spider opened
2019-01-21 09:42:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 09:42:29 [trading_view] INFO: Spider opened: trading_view
2019-01-21 09:42:30 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 09:42:30 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCF60>
2019-01-21 09:42:42 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 09:42:42 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 09:42:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5258,
 'downloader/request_count': 19,
 'downloader/request_method_count/GET': 19,
 'downloader/response_bytes': 1913456,
 'downloader/response_count': 19,
 'downloader/response_status_count/200': 19,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 1, 42, 42, 17802),
 'item_scraped_count': 15,
 'log_count/INFO': 11,
 'request_depth_max': 1,
 'response_received_count': 19,
 'scheduler/dequeued': 19,
 'scheduler/dequeued/memory': 19,
 'scheduler/enqueued': 19,
 'scheduler/enqueued/memory': 19,
 'start_time': datetime.datetime(2019, 1, 21, 1, 42, 29, 457083)}
2019-01-21 09:42:42 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 09:43:20 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 09:43:20 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 09:43:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 09:43:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 09:43:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 09:43:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 09:43:21 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 09:43:21 [scrapy.core.engine] INFO: Spider opened
2019-01-21 09:43:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 09:43:21 [trading_view] INFO: Spider opened: trading_view
2019-01-21 09:43:22 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 09:43:22 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBF60>
2019-01-21 09:43:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 09:43:25 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 09:43:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243119,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 1, 43, 25, 102266),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 1, 43, 21, 468058)}
2019-01-21 09:43:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 09:43:30 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 09:43:30 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 09:43:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 09:43:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 09:43:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 09:43:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 09:43:31 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 09:43:31 [scrapy.core.engine] INFO: Spider opened
2019-01-21 09:43:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 09:43:31 [trading_view] INFO: Spider opened: trading_view
2019-01-21 09:43:32 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 09:43:32 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBF60>
2019-01-21 09:43:36 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 09:43:36 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 09:43:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243087,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 1, 43, 36, 480917),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 1, 43, 31, 968659)}
2019-01-21 09:43:36 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:10:50 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:10:50 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:10:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:10:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:10:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:10:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:10:51 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:10:51 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:10:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:10:51 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:10:52 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:10:52 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000587CF60>
2019-01-21 10:10:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:10:56 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:10:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243112,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 10, 56, 124095),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 10, 51, 42805)}
2019-01-21 10:10:56 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:13:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:13:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:13:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:13:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:13:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:13:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:13:19 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:13:19 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:13:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:13:19 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:13:20 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:13:20 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCEB8>
2019-01-21 10:13:24 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:13:24 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:13:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243197,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 13, 24, 206565),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 13, 19, 345287)}
2019-01-21 10:13:24 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:14:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:14:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:14:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:14:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:14:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:14:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:14:19 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:14:19 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:14:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:14:19 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:14:20 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:14:20 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCF28>
2019-01-21 10:14:24 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:14:24 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:14:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243202,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 14, 24, 390805),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 14, 19, 627533)}
2019-01-21 10:14:24 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:31:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:31:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:31:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:31:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:31:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:31:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:31:32 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:31:32 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:31:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:31:32 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:31:33 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:31:33 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EDF28>
2019-01-21 10:31:37 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:31:37 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:31:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243132,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 31, 37, 135474),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 31, 32, 389202)}
2019-01-21 10:31:37 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:32:12 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:32:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:32:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:32:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:32:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:32:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:32:13 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:32:13 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:32:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:32:13 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:32:14 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:32:14 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCF60>
2019-01-21 10:32:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:32:16 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:32:16 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_tradingview.py", line 54, in close_spider
    *fields
TypeError: execute() takes from 2 to 3 positional arguments but 13 were given
2019-01-21 10:32:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243167,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 32, 16, 812743),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 32, 13, 360546)}
2019-01-21 10:32:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:34:22 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:34:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:34:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:34:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:34:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:34:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:34:23 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:34:23 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:34:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:34:23 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:34:24 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:34:24 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDEF0>
2019-01-21 10:34:27 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:34:27 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:34:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243123,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 34, 27, 680228),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 34, 23, 681000)}
2019-01-21 10:34:27 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:35:59 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:35:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:35:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:35:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:36:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:36:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:36:00 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:36:00 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:36:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:36:00 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:36:01 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:36:01 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DDF28>
2019-01-21 10:36:05 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:36:05 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:36:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243093,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 36, 5, 366816),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 36, 0, 622544)}
2019-01-21 10:36:05 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:38:03 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:38:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:38:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:38:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:38:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:38:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:38:04 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:38:04 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:38:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:38:04 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:38:05 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:38:05 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EAEF0>
2019-01-21 10:38:08 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:38:08 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:38:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243119,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 38, 8, 943884),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 38, 4, 226614)}
2019-01-21 10:38:08 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:39:29 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:39:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:39:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:39:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:39:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:39:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:39:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:39:29 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:39:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:39:29 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:39:30 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:39:30 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x000000000587EEF0>
2019-01-21 10:39:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:39:34 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:39:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243178,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 39, 34, 344769),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 39, 29, 933516)}
2019-01-21 10:39:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:42:19 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:42:19 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:42:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:42:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:42:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:42:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:42:20 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:42:20 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:42:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:42:20 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:42:21 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:42:21 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D8F60>
2019-01-21 10:42:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:42:25 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:42:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243147,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 42, 25, 197541),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 42, 20, 689283)}
2019-01-21 10:42:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:44:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:44:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:44:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:44:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:44:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:44:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:44:27 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:44:27 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:44:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:44:27 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:44:28 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:44:28 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D8F28>
2019-01-21 10:44:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:44:32 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:44:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243140,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 44, 32, 526621),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 44, 27, 940359)}
2019-01-21 10:44:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:52:49 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:52:49 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:52:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:52:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:52:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:52:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:52:49 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:52:49 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:52:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:52:50 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:52:51 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:52:51 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D7EF0>
2019-01-21 10:52:54 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:52:54 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:52:54 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_tradingview.py", line 40, in close_spider
    """
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 170, in execute
    result = self._query(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 328, in _query
    conn.query(q)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 515, in query
    self._execute_command(COMMAND.COM_QUERY, sql)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 745, in _execute_command
    raise err.InterfaceError("(0, '')")
pymysql.err.InterfaceError: (0, '')
2019-01-21 10:52:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243154,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 52, 54, 304920),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 52, 50, 8675)}
2019-01-21 10:52:54 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:53:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:53:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:53:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:53:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:53:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:53:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:53:34 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:53:34 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:53:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:53:34 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:53:35 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:53:35 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D9F28>
2019-01-21 10:53:38 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:53:38 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:53:38 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_tradingview.py", line 40, in close_spider
    """
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 170, in execute
    result = self._query(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 328, in _query
    conn.query(q)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 515, in query
    self._execute_command(COMMAND.COM_QUERY, sql)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 745, in _execute_command
    raise err.InterfaceError("(0, '')")
pymysql.err.InterfaceError: (0, '')
2019-01-21 10:53:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243198,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 53, 38, 5420),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 53, 34, 641227)}
2019-01-21 10:53:38 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:56:58 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:56:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:56:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:56:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:56:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:56:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:56:58 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:56:58 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:56:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:56:58 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:56:59 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:56:59 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057E9F28>
2019-01-21 10:57:03 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:57:03 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:57:03 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_tradingview.py", line 40, in close_spider
    """
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 170, in execute
    result = self._query(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 328, in _query
    conn.query(q)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 515, in query
    self._execute_command(COMMAND.COM_QUERY, sql)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 745, in _execute_command
    raise err.InterfaceError("(0, '')")
pymysql.err.InterfaceError: (0, '')
2019-01-21 10:57:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243207,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 57, 3, 793190),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 56, 58, 929912)}
2019-01-21 10:57:03 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:59:10 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:59:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:59:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:59:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:59:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:59:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:59:11 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:59:11 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:59:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:59:11 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:59:12 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:59:12 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D7F28>
2019-01-21 10:59:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:59:16 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:59:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243135,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 59, 16, 484780),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 59, 11, 610501)}
2019-01-21 10:59:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:59:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:59:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:59:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:59:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:59:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:59:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:59:27 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:59:27 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:59:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:59:27 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:59:28 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:59:28 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DAEF0>
2019-01-21 10:59:31 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:59:31 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:59:31 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_tradingview.py", line 54, in close_spider
    """
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 170, in execute
    result = self._query(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 328, in _query
    conn.query(q)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 515, in query
    self._execute_command(COMMAND.COM_QUERY, sql)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 745, in _execute_command
    raise err.InterfaceError("(0, '')")
pymysql.err.InterfaceError: (0, '')
2019-01-21 10:59:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243166,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 59, 31, 368829),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 59, 27, 668618)}
2019-01-21 10:59:31 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 10:59:45 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 10:59:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 10:59:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 10:59:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 10:59:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 10:59:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 10:59:46 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 10:59:46 [scrapy.core.engine] INFO: Spider opened
2019-01-21 10:59:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 10:59:46 [trading_view] INFO: Spider opened: trading_view
2019-01-21 10:59:47 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 10:59:47 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D7F28>
2019-01-21 10:59:50 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 10:59:50 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 10:59:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243177,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 2, 59, 50, 213907),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 2, 59, 46, 481694)}
2019-01-21 10:59:50 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:04:27 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:04:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:04:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:04:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:04:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:04:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:04:28 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:04:28 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:04:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:04:28 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:04:29 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:04:29 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000058E6F60>
2019-01-21 11:04:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:04:33 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 11:04:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243176,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 4, 33, 3082),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 4, 28, 752839)}
2019-01-21 11:04:33 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:04:48 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:04:48 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:04:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:04:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:04:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:04:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:04:49 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:04:49 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:04:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:04:49 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:04:50 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:04:50 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005818F60>
2019-01-21 11:04:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:04:53 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 11:04:53 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_tradingview.py", line 50, in close_spider
    """
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 170, in execute
    result = self._query(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 328, in _query
    conn.query(q)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 515, in query
    self._execute_command(COMMAND.COM_QUERY, sql)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 745, in _execute_command
    raise err.InterfaceError("(0, '')")
pymysql.err.InterfaceError: (0, '')
2019-01-21 11:04:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243146,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 4, 53, 516255),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 4, 49, 18998)}
2019-01-21 11:04:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:10:10 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:10:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:10:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:10:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:10:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:10:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:10:11 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:10:11 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:10:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:10:11 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:10:12 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:10:12 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D9F60>
2019-01-21 11:10:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:10:16 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 11:10:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243153,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 10, 16, 176710),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 10, 11, 335433)}
2019-01-21 11:10:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:10:43 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:10:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:10:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:10:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:10:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:10:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:10:44 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:10:44 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:10:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:10:44 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:10:45 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:10:45 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D8F28>
2019-01-21 11:10:49 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:10:49 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 11:10:49 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_tradingview.py", line 51, in close_spider
    """
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 170, in execute
    result = self._query(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 328, in _query
    conn.query(q)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 515, in query
    self._execute_command(COMMAND.COM_QUERY, sql)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 745, in _execute_command
    raise err.InterfaceError("(0, '')")
pymysql.err.InterfaceError: (0, '')
2019-01-21 11:10:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243150,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 10, 49, 308605),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 10, 44, 372323)}
2019-01-21 11:10:49 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:12:11 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:12:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:12:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:12:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:12:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:12:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:12:12 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:12:12 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:12:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:12:12 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:12:13 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:12:13 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005877F60>
2019-01-21 11:12:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:12:17 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 11:12:17 [scrapy.core.engine] ERROR: Scraper close failure
Traceback (most recent call last):
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\liuluyang\test_scrapy\test_scrapy\pipelines_tradingview.py", line 51, in close_spider
    """
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 170, in execute
    result = self._query(query)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\cursors.py", line 328, in _query
    conn.query(q)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 515, in query
    self._execute_command(COMMAND.COM_QUERY, sql)
  File "C:\Users\Administrator\Anaconda3\lib\site-packages\pymysql\connections.py", line 745, in _execute_command
    raise err.InterfaceError("(0, '')")
pymysql.err.InterfaceError: (0, '')
2019-01-21 11:12:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243151,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 12, 17, 694661),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 12, 12, 741377)}
2019-01-21 11:12:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:17:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:17:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:17:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:17:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:17:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:17:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:17:34 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:17:34 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:17:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:17:34 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:17:35 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:17:35 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DBF28>
2019-01-21 11:17:39 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:17:39 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 11:17:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243151,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 17, 39, 235651),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 17, 34, 678390)}
2019-01-21 11:17:39 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:20:18 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:20:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:20:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:20:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:20:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:20:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:20:19 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.CloseDB',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:20:19 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:20:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:20:19 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:20:20 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:20:20 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057DCF28>
2019-01-21 11:20:24 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:20:24 [trading_view] INFO: trading_view关闭mysql数据库连接
2019-01-21 11:20:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243154,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 20, 24, 508104),
 'log_count/INFO': 11,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 20, 19, 368810)}
2019-01-21 11:20:24 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:22:32 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:22:32 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:22:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:22:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:22:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:22:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:22:33 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:22:33 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:22:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:22:33 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:22:34 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:22:34 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005819E80>
2019-01-21 11:22:37 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:22:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243131,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 22, 37, 568715),
 'log_count/INFO': 10,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 22, 33, 199465)}
2019-01-21 11:22:37 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:36:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:36:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:36:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:36:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:36:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:36:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:36:29 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:36:29 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:36:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:36:29 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:36:30 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:36:30 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D8E10>
2019-01-21 11:36:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:36:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243133,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 36, 35, 647249),
 'log_count/INFO': 10,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 36, 29, 865918)}
2019-01-21 11:36:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:40:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:40:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:40:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:40:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:40:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:40:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:40:26 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:40:26 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:40:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:40:26 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:40:27 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:40:27 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005876E80>
2019-01-21 11:40:31 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:40:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243119,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 40, 31, 389733),
 'log_count/INFO': 10,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 40, 26, 948479)}
2019-01-21 11:40:31 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 11:48:52 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 11:48:52 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 11:48:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 11:48:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 11:48:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 11:48:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 11:48:52 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 11:48:52 [scrapy.core.engine] INFO: Spider opened
2019-01-21 11:48:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 11:48:52 [trading_view] INFO: Spider opened: trading_view
2019-01-21 11:48:53 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 11:48:53 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x0000000005877E80>
2019-01-21 11:48:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 11:48:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243451,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 3, 48, 57, 495279),
 'log_count/INFO': 10,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 3, 48, 52, 974021)}
2019-01-21 11:48:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 17:01:26 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 17:01:26 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 17:01:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 17:01:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 17:01:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 17:01:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 17:01:27 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 17:01:27 [scrapy.core.engine] INFO: Spider opened
2019-01-21 17:01:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 17:01:27 [trading_view] INFO: Spider opened: trading_view
2019-01-21 17:01:28 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 17:01:28 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D7E80>
2019-01-21 17:01:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 17:01:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243825,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 9, 1, 32, 555039),
 'log_count/INFO': 10,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 9, 1, 27, 766000)}
2019-01-21 17:01:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 17:21:36 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 17:21:36 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 17:21:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 17:21:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 17:21:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 17:21:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 17:21:37 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 17:21:37 [scrapy.core.engine] INFO: Spider opened
2019-01-21 17:21:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 17:21:37 [trading_view] INFO: Spider opened: trading_view
2019-01-21 17:21:38 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 17:21:38 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D7E80>
2019-01-21 17:21:41 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 17:21:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1200,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 316179,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 9, 21, 41, 846240),
 'item_scraped_count': 1,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'start_time': datetime.datetime(2019, 1, 21, 9, 21, 37, 167920)}
2019-01-21 17:21:41 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 17:23:47 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 17:23:47 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 17:23:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 17:23:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 17:23:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 17:23:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 17:23:48 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 17:23:48 [scrapy.core.engine] INFO: Spider opened
2019-01-21 17:23:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 17:23:48 [trading_view] INFO: Spider opened: trading_view
2019-01-21 17:23:49 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 17:23:49 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057D8E80>
2019-01-21 17:23:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 17:23:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243826,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 9, 23, 53, 488360),
 'log_count/INFO': 10,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 9, 23, 48, 497074)}
2019-01-21 17:23:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-01-21 17:24:38 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: test_scrapy)
2019-01-21 17:24:38 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2019-01-21 17:24:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'test_scrapy', 'COMMANDS_MODULE': 'test_scrapy.commands', 'CONCURRENT_REQUESTS': 32, 'DOWNLOAD_DELAY': 0.5, 'DOWNLOAD_TIMEOUT': 20, 'LOG_FILE': 'test_scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'test_scrapy.spiders', 'REDIRECT_ENABLED': False, 'SPIDER_MODULES': ['test_scrapy.spiders']}
2019-01-21 17:24:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-01-21 17:24:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'test_scrapy.middlewares.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-01-21 17:24:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'test_scrapy.middlewares.TestScrapySpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-01-21 17:24:40 [scrapy.middleware] INFO: Enabled item pipelines:
['test_scrapy.pipelines_common.DropTag_a',
 'test_scrapy.pipelines_attention.AttentionPipeline',
 'test_scrapy.pipelines_news.NewsPipeline',
 'test_scrapy.pipelines_newsletter.NewsletterPipeline',
 'test_scrapy.pipelines_tradingview.TradingViewPipeline',
 'test_scrapy.pipelines_currency.CurrencyPipeline',
 'test_scrapy.pipelines_media.MediaPipeline',
 'test_scrapy.pipelines_token.EthTokenPipeline',
 'test_scrapy.pipelines_address.TokenAddressPipeline',
 'test_scrapy.pipelines_common.PushMessage']
2019-01-21 17:24:40 [scrapy.core.engine] INFO: Spider opened
2019-01-21 17:24:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-01-21 17:24:40 [trading_view] INFO: Spider opened: trading_view
2019-01-21 17:24:41 [trading_view] INFO: 已连接Redis服务：Redis<ConnectionPool<Connection<host=localhost,port=6379,db=1>>>
2019-01-21 17:24:41 [trading_view] INFO: 已连接mysql服务：<pymysql.connections.Connection object at 0x00000000057EDE10>
2019-01-21 17:24:44 [scrapy.core.engine] INFO: Closing spider (finished)
2019-01-21 17:24:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 915,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 243788,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 1, 21, 9, 24, 44, 206261),
 'log_count/INFO': 10,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 1, 21, 9, 24, 40, 98026)}
2019-01-21 17:24:44 [scrapy.core.engine] INFO: Spider closed (finished)
